{
    "authorId": "3195814",
    "papers": [
        {
            "paperId": "802bedb650d65fef99cbe129d630d235b7459862",
            "title": "Mining Themes in Clinical Notes to Identify Phenotypes and to Predict Length of Stay in Patients admitted with Heart Failure",
            "abstract": "Heart failure is a syndrome which occurs when the heart is not able to pump blood and oxygen to support other organs in the body. Treatment and management of heart failure in patients include understanding the diagnostic codes and procedure reports of these patients during their hospitalization. Identifying the underlying themes in these diagnostic codes and procedure reports could reveal the clinical phenotypes associated with heart failure. These themes could also help clinicians to predict length of stay in the patients using their clinical notes. Understanding clinical phenotypes on the basis of these themes is important to group patients based on their similar characteristics which could also help in predicting patient outcomes like length of stay. These clinical phenotypes usually have a probabilistic latent structure and hence, as there has been no previous work on identifying phenotypes in clinical notes of heart failure patients using a probabilistic framework and to predict length of stay of these patients using data-driven artificial intelligence-based methods, we apply natural language processing technique, topic modeling, to identify the themes present in diagnostic codes and in procedure reports of 1,200 patients admitted for heart failure at the University of Illinois Hospital and Health Sciences System (UI Health). Topic modeling identified twelve themes each in diagnostic codes and procedure reports. These themes revealed information about different phenotypes related to various perspectives about heart failure, which could help to study patients\u2019 profiles and discover new relationships among medical concepts. Each theme had a set of keywords and each clinical note was labeled with two themes \u2014 one corresponding to its diagnostic code and the other corresponding to its procedure reports along with their percentage contribution. We used these themes and their percentage contribution to predict length of stay. We found that the themes discovered in diagnostic codes and procedure reports using topic modeling together were able to predict length of stay of the patients with an accuracy of 61.1% and an Area under the Receiver Operating Characteristic Curve (ROC AUC) value of 0.828.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149663947",
                    "name": "Ankita Agarwal"
                },
                {
                    "authorId": "1811614",
                    "name": "Tanvi Banerjee"
                },
                {
                    "authorId": "46960802",
                    "name": "W. Romine"
                },
                {
                    "authorId": "3195814",
                    "name": "K. Thirunarayan"
                },
                {
                    "authorId": "2179005638",
                    "name": "Lingwei Chen"
                },
                {
                    "authorId": "11991279",
                    "name": "Mia Cajita"
                }
            ]
        },
        {
            "paperId": "8f7d0ff10235524487ecb3d5c64425c4e4909466",
            "title": "CVII: Enhancing Interpretability in Intelligent Sensor Systems via Computer Vision Interpretability Index",
            "abstract": "In the realm of intelligent sensor systems, the dependence on Artificial Intelligence (AI) applications has heightened the importance of interpretability. This is particularly critical for opaque models such as Deep Neural Networks (DNN), as understanding their decisions is essential, not only for ethical and regulatory compliance, but also for fostering trust in AI-driven outcomes. This paper introduces the novel concept of a Computer Vision Interpretability Index (CVII). The CVII framework is designed to emulate human cognitive processes, specifically in tasks related to vision. It addresses the intricate challenge of quantifying interpretability, a task that is inherently subjective and varies across domains. The CVII is rigorously evaluated using a range of computer vision models applied to the COCO (Common Objects in Context) dataset, a widely recognized benchmark in the field. The findings established a robust correlation between image interpretability, model selection, and CVII scores. This research makes a substantial contribution to enhancing interpretability for human comprehension, as well as within intelligent sensor applications. By promoting transparency and reliability in AI-driven decision-making, the CVII framework empowers its stakeholders to effectively harness the full potential of AI technologies.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2266766170",
                    "name": "Hossein Mohammadi"
                },
                {
                    "authorId": "3195814",
                    "name": "K. Thirunarayan"
                },
                {
                    "authorId": "2266813632",
                    "name": "Lingwei Chen"
                }
            ]
        },
        {
            "paperId": "a40c1091954cd34384f57ebd59be4836ff291e73",
            "title": "Enhancing Rare Cell Type Identification in Single-Cell Data: An Innovative Gene Filtering Approach using Bipartite Cell-Gene Relation Graph",
            "abstract": "A useful tool for examining cellular diversity is single cell RNA sequencing (scRNA-seq). However, the high dimensionality and technical noise of scRNA-seq data make analysis difficult. To address this issue, gene filtering has been widely adopted to minimize single cell data noise and enhance the quality of subsequent analyses. Nonetheless, existing gene filtering techniques may inadvertently omit critical but rare genes which are necessary for identifying rare cell types that play a pivotal role in comprehending many biological processes. A novel graph-based gene selection technique is suggested in this study with the aim of preserving the informative genes to better identify rare cell types. Our findings demonstrate that this technique enhances the identification of rare cell populations, providing a refined approach for scRNA-seq data analysis and potentially enabling earlier and more reliable disease detection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266766369",
                    "name": "Maziyar Baranpouyan"
                },
                {
                    "authorId": "2266766170",
                    "name": "Hossein Mohammadi"
                },
                {
                    "authorId": "2266767332",
                    "name": "Hojjat Torabi Goudarzi"
                },
                {
                    "authorId": "3195814",
                    "name": "K. Thirunarayan"
                },
                {
                    "authorId": "2266813632",
                    "name": "Lingwei Chen"
                }
            ]
        },
        {
            "paperId": "17954960ef15e8cfc1b20df3fb92d73dcd96d20e",
            "title": "Leveraging Natural Learning Processing to Uncover Themes in Clinical Notes of Patients Admitted for Heart Failure",
            "abstract": "Heart failure occurs when the heart is not able to pump blood and oxygen to support other organs in the body as it should. Treatments include medications and sometimes hospitalization. Patients with heart failure can have both cardiovascular as well as non-cardiovascular comorbidities. Clinical notes of patients with heart failure can be analyzed to gain insight into the topics discussed in these notes and the major comorbidities in these patients. In this regard, we apply machine learning techniques, such as topic modeling, to identify the major themes found in the clinical notes specific to the procedures performed on 1,200 patients admitted for heart failure at the University of Illinois Hospital and Health Sciences System (UI Health). Topic modeling revealed five hidden themes in these clinical notes, including one related to heart disease comorbidities.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2149663947",
                    "name": "Ankita Agarwal"
                },
                {
                    "authorId": "3195814",
                    "name": "K. Thirunarayan"
                },
                {
                    "authorId": "46960802",
                    "name": "W. Romine"
                },
                {
                    "authorId": "82194910",
                    "name": "Amanuel Alambo"
                },
                {
                    "authorId": "11991279",
                    "name": "Mia Cajita"
                },
                {
                    "authorId": "1811614",
                    "name": "Tanvi Banerjee"
                }
            ]
        },
        {
            "paperId": "844c60f5f1fced8fecf43febd0147e0422553564",
            "title": "Entity-Driven Fact-Aware Abstractive Summarization of Biomedical Literature",
            "abstract": "As part of the large number of scientific articles being published every year, the publication rate of biomedical literature has been increasing. Consequently, there has been considerable effort to harness and summarize the massive amount of biomedical research articles. While transformer-based encoder-decoder models in a vanilla source document-to-summary setting have been extensively studied for abstractive summarization in different domains, their major limitations continue to be entity hallucination (a phenomenon where generated summaries constitute entities not related to or present in source article(s)) and factual inconsistency. This problem is exacerbated in a biomedical setting where named entities and their semantics (which can be captured through a knowledge base) constitute the essence of an article. The use of named entities and facts mined from background knowledge bases pertaining to the named entities to guide abstractive summarization has not been studied in biomedical article summarization literature. In this paper, we propose an entity-driven fact-aware framework for training end-to-end transformer-based encoder-decoder models for abstractive summarization of biomedical articles. We call the proposed approach, whose building block is a transformer-based model, EFAS, Entity-driven Fact-aware Abstractive Summarization. We conduct a set of experiments using five state-of-the-art transformer-based encoder-decoder models (two of which are specifically designed for long document summarization) and demonstrate that injecting knowledge into the training/inference phase of these models enables the models to achieve significantly better performance than the standard source document-to-summary setting in terms of entity-level factual accuracy, N-gram novelty, and semantic equivalence while performing comparably on ROUGE metrics. The proposed approach is evaluated on ICD-11-Summ-1000, a dataset we build for abstractive summarization of biomedical literature, and PubMed-50k, a segment of a large-scale benchmark dataset for abstractive summarization of biomedical literature.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "82194910",
                    "name": "Amanuel Alambo"
                },
                {
                    "authorId": "1811614",
                    "name": "Tanvi Banerjee"
                },
                {
                    "authorId": "3195814",
                    "name": "K. Thirunarayan"
                },
                {
                    "authorId": "2844334",
                    "name": "M. Raymer"
                }
            ]
        },
        {
            "paperId": "b15e0a98cb6321cebbdcd5b84a6877f9e489322f",
            "title": "Improving the Factual Accuracy of Abstractive Clinical Text Summarization using Multi-Objective Optimization",
            "abstract": "While there has been recent progress in abstractive summarization as applied to different domains including news articles, scientific articles, and blog posts, the application of these techniques to clinical text summarization has been limited. This is primarily due to the lack of large-scale training data and the messy/unstructured nature of clinical notes as opposed to other domains where massive training data come in structured or semi -structured form. Further, one of the least explored and critical components of clinical text summarization is factual accuracy of clinical summaries. This is specifically crucial in the healthcare domain, cardiology in particular, where an accurate summary generation that preserves the facts in the source notes is critical to the well-being of a patient. In this study, we propose a framework for improving the factual accuracy of abstractive summarization of clinical text using knowledge-guided multi-objective optimization. We propose to jointly optimize three cost functions in our proposed architecture during training: generative loss, entity loss and knowledge loss and evaluate the proposed architecture on 1) clinical notes of patients with heart failure (HF), which we collect for this study; and 2) two benchmark datasets, Indiana University Chest X-ray collection (IU X-Ray), and MIMIC-CXR, that are publicly available. We experiment with three transformer encoder-decoder architectures and demonstrate that optimizing different loss functions leads to improved performance in terms of entity-level factual accuracy.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "82194910",
                    "name": "Amanuel Alambo"
                },
                {
                    "authorId": "1811614",
                    "name": "Tanvi Banerjee"
                },
                {
                    "authorId": "3195814",
                    "name": "K. Thirunarayan"
                },
                {
                    "authorId": "11991279",
                    "name": "Mia Cajita"
                }
            ]
        },
        {
            "paperId": "f17cf3b86679073c730a6fdc5cfde4fcc5cce56b",
            "title": "Towards enhancing emotion recognition via multimodal framework",
            "abstract": "Emotional AI is the next era of AI to play a major role in various fields such as entertainment, health care, self-paced online education, etc., considering clues from multiple sources. In this work, we propose a multimodal emotion recognition system extracting information from speech, motion capture, and text data. The main aim of this research is to improve the unimodal architectures to outperform the state-of-the-arts and combine them together to build a robust multi-modal fusion architecture. We developed 1D and 2D CNN-LSTM time-distributed models for speech, a hybrid CNN-LSTM model for motion capture data, and a BERT-based model for text data to achieve state-of-the-art results, and attempted both concatenation-based decision-level fusion and Deep CCA-based feature-level fusion schemes. The proposed speech and mocap models achieve emotion recognition accuracies of 65.08% and 67.51%, respectively, and the BERT-based text model achieves an accuracy of 72.60% . The decision-level fusion approach significantly improves the accuracy of detecting emotions on the IEMOCAP and MELD datasets. This approach achieves 80.20% accuracy on IEMOCAP which is 8.61% higher than the state-of-the-art methods, and 63.52% and 61.65% in 5-class and 7-class classification on the MELD dataset which are higher than the state-of-the-arts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51204343",
                    "name": "C. devi"
                },
                {
                    "authorId": "35196491",
                    "name": "D. Renuka"
                },
                {
                    "authorId": "2084474680",
                    "name": "G. Pooventhiran"
                },
                {
                    "authorId": "2190003810",
                    "name": "D. Harish"
                },
                {
                    "authorId": "4137125",
                    "name": "S. Yadav"
                },
                {
                    "authorId": "3195814",
                    "name": "K. Thirunarayan"
                }
            ]
        },
        {
            "paperId": "0986eb096271c79faedd1106599f32c6a2881838",
            "title": "Characterization of time-variant and time-invariant assessment of suicidality on Reddit using C-SSRS",
            "abstract": "Suicide is the 10th leading cause of death in the U.S (1999-2019). However, predicting when someone will attempt suicide has been nearly impossible. In the modern world, many individuals suffering from mental illness seek emotional support and advice on well-known and easily-accessible social media platforms such as Reddit. While prior artificial intelligence research has demonstrated the ability to extract valuable information from social media on suicidal thoughts and behaviors, these efforts have not considered both severity and temporality of risk. The insights made possible by access to such data have enormous clinical potential\u2014most dramatically envisioned as a trigger to employ timely and targeted interventions (i.e., voluntary and involuntary psychiatric hospitalization) to save lives. In this work, we address this knowledge gap by developing deep learning algorithms to assess suicide risk in terms of severity and temporality from Reddit data based on the Columbia Suicide Severity Rating Scale (C-SSRS). In particular, we employ two deep learning approaches: time-variant and time-invariant modeling, for user-level suicide risk assessment, and evaluate their performance against a clinician-adjudicated gold standard Reddit corpus annotated based on the C-SSRS. Our results suggest that the time-variant approach outperforms the time-invariant method in the assessment of suicide-related ideations and supportive behaviors (AUC:0.78), while the time-invariant model performed better in predicting suicide-related behaviors and suicide attempt (AUC:0.64). The proposed approach can be integrated with clinical diagnostic interviews for improving suicide risk assessments.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491238594",
                    "name": "Manas Gaur"
                },
                {
                    "authorId": "1481714624",
                    "name": "V. Aribandi"
                },
                {
                    "authorId": "82194910",
                    "name": "Amanuel Alambo"
                },
                {
                    "authorId": "2753547",
                    "name": "Ugur Kursuncu"
                },
                {
                    "authorId": "3195814",
                    "name": "K. Thirunarayan"
                },
                {
                    "authorId": "2072253382",
                    "name": "Jonanthan Beich"
                },
                {
                    "authorId": "145236681",
                    "name": "Jyotishman Pathak"
                },
                {
                    "authorId": "144463965",
                    "name": "A. Sheth"
                }
            ]
        },
        {
            "paperId": "c4da51a7248897caadce4cdbc727ba55be30f6c9",
            "title": "The Duality of Data and Knowledge Across the Three Waves of AI",
            "abstract": "We discuss how, over the last 30\u201350 years, artificial intelligence (AI) systems that focused only on data have been handicapped and how knowledge has been critical in developing smarter, intelligent, and more effective systems. In fact, the vast progress in AI can be viewed in terms of the three waves of AI as identified by DARPA. During the first wave, handcrafted knowledge has been at the center, while during the second wave, the datadriven approaches supplanted knowledge. Now we see a strong role and resurgence of knowledge fueling major breakthroughs in the third wave of AI underpinning future intelligent systems as they attempt human-like decision making and seek to become trusted assistants and companions for humans. We find a wider availability of knowledge created from diverse sources, using manual to automated means both by repurposing as well as by extraction. Using knowledge with statistical learning is becoming increasingly indispensable to help make AI systems more transparent and auditable. We will draw a parallel with the role of knowledge and experience in human intelligence based on cognitive science, and discuss emerging neuro-symbolic or hybrid AI systems in which knowledge is the critical enabler for combining capabilities of the data-intensive statistical AI systems with those of symbolic AI systems, resulting in more capable AI systems that support more human-like intelligence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144463965",
                    "name": "A. Sheth"
                },
                {
                    "authorId": "3195814",
                    "name": "K. Thirunarayan"
                }
            ]
        },
        {
            "paperId": "d8549e0b9050b1ec4c771627fd347c22dcecf850",
            "title": "The Inescapable Duality of Data and Knowledge",
            "abstract": "ion and Analogies One of the key issues underlying the debate about the effectiveness of data-driven approaches vs knowledge-based approaches is the source and the creator of abstractions [Mitchell 2021]. Data-driven approaches derive abstractions utilizing the trends and patterns explicit or implicit in the copious amounts of data, and its utility is tied to the representativeness of the data with respect to the domain of discourse. In contrast, knowledge-based approaches typically present abstractions that are obtained directly or indirectly by end-user applications and goals. Insofar as these two orthogonal approaches yield overlapping abstractions, we can use them synergistically. Further, their hybridization presents a viable strategy to leverage their complementary strengths. Data-driven abstractions can group values into coarser categories on the basis of data usage pattern but labelling them sensibly and developing abstractions with specific purpose usually requires human-in-the-loop. For instance, abstractions of temperature ranges and temperature thresholds that are relevant to different states of matter of water (such as ice, melting point, water, boiling point, and steam) are different from those relevant to human health (such as normal temperature, fever, mild fever, high fever, and chill). In circuit design, we can describe a full adder at different levels of abstraction in terms of two half-adders, or using logic gates (either using AND, OR and NOT gates, or NAND/NOR gates), or using flip-flop circuits, or using transistor netlists, etc. In the context of image recognition and transfer learning, the later stages of neural network layers capture higher-level abstractions/features that mirror the nature/components of the images in the training data and the end-user application, and probing them can improve our confidence in its working and provide insights about the potential limitations that can be addressed and remedied. Our effort in deep knowledge-infused learning (within the shallow, semi-dep and deep infusion variety [Sheth et al 2019]) is based on this intuition. Analogical reasoning involving primitive shapes (e.g., circle, square, and triangle) and their relationships (e.g., inside, outside, adjacent, above, below, left, right, and overlapping) in an image requires non-trivial human insights and geometric reasoning [Evans 1987]. In fact, this example presents an interesting situation where we can use deep learning algorithms to recognize primitive shapes and then use geometric analogical reasoning to solve non-trivial puzzles. As discussed above, there are several different motivations and applications for combining deeplearning powered neural networks with knowledge-based systems. Developing a modular framework to combine data-driven approaches that are effective at extracting useful features from low-level sensory and linguistic data, with declarative models of normal behavior such as through the incorporation of physical laws, medical knowledge, and linguistic knowledge, as well as anticipated failure modes and exceptions, can go a long way in leveraging insights from and exploiting the duality of data and knowledge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144463965",
                    "name": "A. Sheth"
                },
                {
                    "authorId": "3195814",
                    "name": "K. Thirunarayan"
                }
            ]
        }
    ]
}