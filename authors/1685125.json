{
    "authorId": "1685125",
    "papers": [
        {
            "paperId": "7486179e3500e6246b1e0e3ab32ecbc3e61f11d7",
            "title": "SoftED: Metrics for Soft Evaluation of Time Series Event Detection",
            "abstract": "Time series event detection methods are evaluated mainly by standard classification metrics that focus solely on detection accuracy. However, inaccuracy in detecting an event can often result from its preceding or delayed effects reflected in neighboring detections. These detections are valuable to trigger necessary actions or help mitigate unwelcome consequences. In this context, current metrics are insufficient and inadequate for the context of event detection. There is a demand for metrics that incorporate both the concept of time and temporal tolerance for neighboring detections. This paper introduces SoftED metrics, a new set of metrics designed for soft evaluating event detection methods. They enable the evaluation of both detection accuracy and the degree to which their detections represent events. They improved event detection evaluation by associating events and their representative detections, incorporating temporal tolerance in over 36\\% of experiments compared to the usual classification metrics. SoftED metrics were validated by domain specialists that indicated their contribution to detection evaluation and method selection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31683603",
                    "name": "Rebecca Salles"
                },
                {
                    "authorId": "2113233053",
                    "name": "J. Lima"
                },
                {
                    "authorId": "3049239",
                    "name": "R. Coutinho"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "2028901",
                    "name": "F. Masseglia"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": null,
                    "name": "Chao Chen"
                },
                {
                    "authorId": "2142288865",
                    "name": "Jonathan Garibaldi"
                },
                {
                    "authorId": "2145364345",
                    "name": "F\u00e1bio Porto"
                },
                {
                    "authorId": "2176421587",
                    "name": "Eduardo S. Ogasawara"
                }
            ]
        },
        {
            "paperId": "d73d33b025daee1760eb0954044013ff516e5ab6",
            "title": "Forward and Backward Inertial Anomaly Detector: A Novel Time Series Event Detection Method",
            "abstract": "Time series event detection is related to studying methods for detecting observations in a series with special meaning. These observations differ from the expected behavior of the data set. In data streaming scenarios, it is possible to observe an increase in the speed of data generation in time series. Therefore, adapting to time series changes becomes crucial. Thus, identifying events associated with these changes is essential for timely and correct decision-making. Although there are many methods to detect events, it is still possible to have difficulties detecting them correctly, particularly those associated with concept drift. In order to fill the gap in the literature, this work proposes a new method, named Forward and Backward Inertial Anomaly Detector (FBIAD), for detecting events in time series. It contributes by analyzing surrounding inertia around observations. FBIAD outperformed other methods both in accuracy and elapsed time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113233053",
                    "name": "J. Lima"
                },
                {
                    "authorId": "31683603",
                    "name": "Rebecca Salles"
                },
                {
                    "authorId": "2145364345",
                    "name": "F\u00e1bio Porto"
                },
                {
                    "authorId": "3049239",
                    "name": "R. Coutinho"
                },
                {
                    "authorId": "2186558770",
                    "name": "Pedro Alpis"
                },
                {
                    "authorId": "2072567221",
                    "name": "Luciana E. G. Escobar"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "2176421587",
                    "name": "Eduardo S. Ogasawara"
                }
            ]
        },
        {
            "paperId": "b6ade06f43cd7cf8afebf119884b00078a43ceb8",
            "title": "Provenance-and machine learning-based recommendation of parameter values in scientific workflows",
            "abstract": "Scientific Workflows (SWfs) have revolutionized how scientists in various domains of science conduct their experiments. The management of SWfs is performed by complex tools that provide support for workflow composition, monitoring, execution, capturing, and storage of the data generated during execution. In some cases, they also provide components to ease the visualization and analysis of the generated data. During the workflow\u2019s composition phase, programs must be selected to perform the activities defined in the workflow specification. These programs often require additional parameters that serve to adjust the program\u2019s behavior according to the experiment\u2019s goals. Consequently, workflows commonly have many parameters to be manually configured, encompassing even more than one hundred in many cases. Wrongly parameters\u2019 values choosing can lead to crash workflows executions or provide undesired results. As the execution of data- and compute-intensive workflows is commonly performed in a high-performance computing environment e.g., (a cluster, a supercomputer, or a public cloud), an unsuccessful execution configures a waste of time and resources. In this article, we present FReeP\u2014Feature Recommender from Preferences, a parameter value recommendation method that is designed to suggest values for workflow parameters, taking into account past user preferences. FReeP is based on Machine Learning techniques, particularly in Preference Learning. FReeP is composed of three algorithms, where two of them aim at recommending the value for one parameter at a time, and the third makes recommendations for n parameters at once. The experimental results obtained with provenance data from two broadly used workflows showed FReeP usefulness in the recommendation of values for one parameter. Furthermore, the results indicate the potential of FReeP to recommend values for n parameters in scientific workflows.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2120354041",
                    "name": "Daniel Silva Junior"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144707454",
                    "name": "Aline Paes"
                },
                {
                    "authorId": "2315401825",
                    "name": "Daniel de Oliveira"
                }
            ]
        },
        {
            "paperId": "5cb263019b1700b39b87bac2a07b27971af11b36",
            "title": "Spatial-time motifs discovery",
            "abstract": "Discovering motifs in time series data has been widely explored. Various techniques have been developed to tackle this problem. However, when it comes to spatial-time series, a clear gap can be observed according to the literature review. This paper tackles such a gap by presenting an approach to discover and rank motifs in spatial-time series, denominated Combined Series Approach (CSA). CSA is based on partitioning the spatial-time series into blocks. Inside each block, subsequences of spatial-time series are combined in a way that hash-based motif discovery algorithm is applied. Motifs are validated according to both temporal and spatial constraints. Later, motifs are ranked according to their entropy, the number of occurrences, and the proximity of their occurrences. The approach was evaluated using both synthetic and seismic datasets. CSA outperforms traditional methods designed only for time series. CSA was also able to prioritize motifs that were meaningful both in the context of synthetic data and also according to seismic specialists.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145680742",
                    "name": "Heraldo Borges"
                },
                {
                    "authorId": "1992799642",
                    "name": "Murillo Dutra"
                },
                {
                    "authorId": "1992797111",
                    "name": "Amin Bazaz"
                },
                {
                    "authorId": "3049239",
                    "name": "R. Coutinho"
                },
                {
                    "authorId": "104059933",
                    "name": "F. Perosi"
                },
                {
                    "authorId": "145179575",
                    "name": "F. Porto"
                },
                {
                    "authorId": "2028901",
                    "name": "F. Masseglia"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "5628621",
                    "name": "Eduardo Ogasawara"
                }
            ]
        },
        {
            "paperId": "a6b1c6a8b4cca54d9c55fcac71d8769d646e3703",
            "title": "Data-Intensive Workflow Management: For Clouds and Data-Intensive and Scalable Computing Environments",
            "abstract": "Abstract Workflows may be defined as abstractions used to model the coherent flow of activities in the context of an in silico scientific experiment. They are employed in many domains of science su...",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33663874",
                    "name": "Daniel de Oliveira"
                },
                {
                    "authorId": "2108410330",
                    "name": "Ji Liu"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                }
            ]
        },
        {
            "paperId": "11464c5804cecebbb949ae76867a10177d713152",
            "title": "Computation of PDFs on Big Spatial Data: Problem & Architecture",
            "abstract": "Big spatial data can be produced by observation or numerical simulation programs and correspond to points that represent a 3D soil cube area. However, errors in signal processing and modeling create some uncertainty, and thus a lack of accuracy in identifying geological or seismic phenomenons. To analyze uncertainty, the main solution is to compute a Probability Density Function (PDF) of each point in the spatial cube area, which can be very time consuming. In this paper, we analyze the problem and discuss the use of Spark to efficiently compute PDFs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108410330",
                    "name": "Ji Liu"
                },
                {
                    "authorId": "8836349",
                    "name": "Noel Moreno Lemus"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "145179575",
                    "name": "F. Porto"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "2e2e68984da3c0f1f5897747fd20ad4001ead10a",
            "title": "Scientific Data Analysis Using Data-Intensive Scalable Computing: The SciDISC Project",
            "abstract": "Data-intensive science requires the integration of two fairly different paradigms: high-performance computing (HPC) and data-intensive scalable computing (DISC), as exemplified by frameworks such as Hadoop and Spark. In this context, the SciDISC project addresses the grand challenge of scientific data analysis using DISC, by developing architectures and methods to combine simulation and data analysis. SciDISC is an ongoing project between Inria, several research institutions in Rio de Janeiro and NYU. This paper introduces the motivations and objectives of the project, and reports on the first results achieved so far.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1763201",
                    "name": "M. Mattoso"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "145680742",
                    "name": "Heraldo Borges"
                },
                {
                    "authorId": "145009099",
                    "name": "J. Camata"
                },
                {
                    "authorId": "144720305",
                    "name": "A. Coutinho"
                },
                {
                    "authorId": "145614808",
                    "name": "D. Gaspar"
                },
                {
                    "authorId": "8836349",
                    "name": "Noel Moreno Lemus"
                },
                {
                    "authorId": "2108410330",
                    "name": "Ji Liu"
                },
                {
                    "authorId": "3416899",
                    "name": "Hermano Lustosa"
                },
                {
                    "authorId": "2028901",
                    "name": "F. Masseglia"
                },
                {
                    "authorId": "2623682",
                    "name": "Fabr\u00edcio Nogueira da Silva"
                },
                {
                    "authorId": "14797446",
                    "name": "V. S. Sousa"
                },
                {
                    "authorId": "144712722",
                    "name": "Renan Souza"
                },
                {
                    "authorId": "1779544",
                    "name": "Kary A. C. S. Oca\u00f1a"
                },
                {
                    "authorId": "5628621",
                    "name": "Eduardo Ogasawara"
                },
                {
                    "authorId": "33663874",
                    "name": "Daniel de Oliveira"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "145179575",
                    "name": "F. Porto"
                },
                {
                    "authorId": "1695878",
                    "name": "D. Shasha"
                }
            ]
        },
        {
            "paperId": "2fa58aa35083f1d64291dfb291e6d18c64c2beed",
            "title": "Parallel Polyglot Query Processing on Heterogeneous Cloud Data Stores with LeanXcale",
            "abstract": "The blooming of different cloud data stores has turned polystore systems to a major topic in the nowadays cloud landscape. Especially, as the amount of processed data grows rapidly each year, much attention is being paid on taking advantage of the parallel processing capabilities of the underlying data stores. To provide data federation, a typical polystore solution defines a common data model and query language with translations to API calls or queries to each data store. However, this may lead to losing important querying capabilities. The polyglot approach of the CloudMdsQL query language allows data store native queries to be expressed as inline scripts and combined with regular SQL statements in ad-hoc integration queries. Moreover, efficient optimization techniques, such as bind join, can still take place to improve the performance of selective joins. In this paper, we introduce the distributed architecture of the LeanXcale query engine that processes polyglot queries in the CloudMdsQL query language, yet allowing native scripts to be handled in parallel at data store shards, so that efficient and scalable parallel joins take place at the query engine level. The experimental evaluation of the LeanXcale parallel query engine on various join queries illustrates well the performance benefits of exploiting the parallelism of the underlying data management technologies in combination with the high expressivity provided by their scripting/querying frameworks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25017334",
                    "name": "B. Kolev"
                },
                {
                    "authorId": "145282585",
                    "name": "O. Levchenko"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "3281313",
                    "name": "R. Vila\u00e7a"
                },
                {
                    "authorId": "145783755",
                    "name": "Rui C. Gon\u00e7alves"
                },
                {
                    "authorId": "1398264223",
                    "name": "R. Jim\u00e9nez-Peris"
                },
                {
                    "authorId": "2359846",
                    "name": "Pavlos Kranas"
                }
            ]
        },
        {
            "paperId": "3025ef48d5a9b6b519166f2ce7bf86f1a14c5960",
            "title": "FReeP: towards Parameter Recommendation in Scientific Workflows using Preference Learning",
            "abstract": "Scientific workflows are a de facto standard for modeling scientific experiments. However, several workflows have too many parameters to be manually configured. Poor choices of parameter values may lead to unsuccessful executions of the workflow. In this paper, we present FReeP, a parameter recommendation algorithm that suggests a value to a parameter that agrees with the user preferences. FReeP is based on the Preference Learning technique. A preliminary experimental evaluation performed over the SciPhy workflow showed the feasibility of FReeP to recommend parameter values for scientific workflows.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115879421",
                    "name": "D. Silva"
                },
                {
                    "authorId": "144707454",
                    "name": "Aline Paes"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "33663874",
                    "name": "Daniel de Oliveira"
                }
            ]
        },
        {
            "paperId": "46a542a2816968808e25606e959b97125fb7c537",
            "title": "A Differentially Private Index for Range Query Processing in Clouds",
            "abstract": "Performing non-aggregate range queries on cloud stored data, while achieving both privacy and efficiency is a challenging problem. This paper proposes constructing a differentially private index to an outsourced encrypted dataset. Efficiency is enabled by using a cleartext index structure to perform range queries. Security relies on both differential privacy (of the index) and semantic security (of the encrypted dataset). Our solution, PINED-RQ develops algorithms for building and updating the differentially private index. Compared to state-of-the-art secure index based range query processing approaches, PINED-RQ executes queries in the order of at least one magnitude faster. The security of PINED-RQ is proved and its efficiency is assessed by an extensive experimental validation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1793101",
                    "name": "Cetin Sahin"
                },
                {
                    "authorId": "2582473",
                    "name": "T. Allard"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "1709353",
                    "name": "A. El Abbadi"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                }
            ]
        },
        {
            "paperId": "7bfa5b7748f36987fd1b5a28dc7d2d754fed7718",
            "title": "A survey of scheduling frameworks in big data systems",
            "abstract": "Cloud and big data technologies are now converging to enable organizations to outsource data in the cloud and get value from data through big data analytics. Big data systems typically exploit computer clusters to gain scalability and obtain a good cost-performance ratio. However, scheduling a workload in a computer cluster remains a well-known open problem. Scheduling methods are typically implemented in a scheduling framework and may have different objectives. In this paper, we survey scheduling methods and frameworks for big data systems, propose a taxonomy and analyze the features of the different categories of scheduling frameworks. These frameworks have been designed initially for the cloud (MapReduce) to process Web data. We examine sixteen popular scheduling frameworks and discuss their features. Our study shows that different frameworks are proposed for different big data systems, different scales of computer clusters and different objectives. We propose the main dimensions for workloads and metrics for benchmarks to evaluate these scheduling frameworks. Finally, we analyze their limitations and propose new research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108410330",
                    "name": "Ji Liu"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "aa98f4530a1de46a47fddebab12440e6337dd370",
            "title": "A Distributed Collaborative Filtering Algorithm Using Multiple Data Sources",
            "abstract": "Collaborative Filtering (CF) is one of the most commonly used recommendation methods. CF consists in predicting whether, or how much, a user will like (or dislike) an item by leveraging the knowledge of the user's preferences as well as that of other users. In practice, users interact and express their opinion on only a small subset of items, which makes the corresponding user-item rating matrix very sparse. Such data sparsity yields two main problems for recommender systems: (1) the lack of data to effectively model users' preferences, and (2) the lack of data to effectively model item characteristics. However, there are often many other data sources that are available to a recommender system provider, which can describe user interests and item characteristics (e.g., users' social network, tags associated to items, etc.). These valuable data sources may supply useful information to enhance a recommendation system in modeling users' preferences and item characteristics more accurately and thus, hopefully, to make recommenders more precise. For various reasons, these data sources may be managed by clusters of different data centers, thus requiring the development of distributed solutions. In this paper, we propose a new distributed collaborative filtering algorithm, which exploits and combines multiple and diverse data sources to improve recommendation quality. Our experimental evaluation using real datasets shows the effectiveness of our algorithm compared to state-of-the-art recommendation algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3080469",
                    "name": "Mohamed Reda Bouadjenek"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "3348870",
                    "name": "Maximilien Servajean"
                },
                {
                    "authorId": "2028901",
                    "name": "F. Masseglia"
                },
                {
                    "authorId": "1709353",
                    "name": "A. El Abbadi"
                }
            ]
        },
        {
            "paperId": "ff2487a4977eb14359fe61a28e5a7dfe1a2e9ef4",
            "title": "Distributed Management of Scientific Workflows for High-Throughput Plant Phenotyping",
            "abstract": "High-throughput phenotyping platforms allow acquisition of quantitative data on thousands of plants required for genetic analyses in well-controlled environmental conditions. However, analysing these massive datasets and reproducing computational experiments require the use of new computational infrastructure and algorithms to scale.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2224947",
                    "name": "C. Pradal"
                },
                {
                    "authorId": "1783625",
                    "name": "Sarah Cohen Boulakia"
                },
                {
                    "authorId": "153289553",
                    "name": "Ga\u00ebtan Heidsieck"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "5837924",
                    "name": "F. Tardieu"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "0f1026a2696ff258de679200fe213627bae44652",
            "title": "Efficient Scheduling of Scientific Workflows Using Hot Metadata in a Multisite Cloud",
            "abstract": "Large-scale, data-intensive scientific applications are often expressed as scientific workflows (SWfs). In this paper, we consider the problem of efficient scheduling of a large SWf in a multisite cloud, i.e., a cloud with geo-distributed cloud data centers (sites). The reasons for using multiple cloud sites to run a SWf are that data is already distributed, the necessary resources exceed the limits at a single site, or the monetary cost is lower. In a multisite cloud, metadata management has a critical impact on the efficiency of SWf scheduling as it provides a global view of data location and enables task tracking during execution. Thus, it should be readily available to the system at any given time. While it has been shown that efficient metadata handling plays a key role in performance, little research has targeted this issue in multisite cloud. In this paper, we propose to identify and exploit hot metadata (frequently accessed metadata) for efficient SWf scheduling in a multisite cloud, using a distributed approach. We implemented our approach within a scientific workflow management system, which shows that our approach reduces the execution time of highly parallel jobs up to 64 percent and that of the whole SWfs up to 55 percent.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108410330",
                    "name": "Ji Liu"
                },
                {
                    "authorId": "2066236904",
                    "name": "Luis Pineda"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "1726606",
                    "name": "Alexandru Costan"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1779139",
                    "name": "Gabriel Antoniu"
                },
                {
                    "authorId": "1763201",
                    "name": "M. Mattoso"
                }
            ]
        },
        {
            "paperId": "5059697ef987abbb6c1aa7982af4dcc57cd687c6",
            "title": "Crowdsourcing Thousands of Specialized Labels: A Bayesian Active Training Approach",
            "abstract": "Large-scale annotated corpora have yielded impressive performance improvements in computer vision and multimedia content analysis. However, such datasets depend on an enormous amount of human labeling effort. When the labels correspond to well-known concepts, it is straightforward to train the annotators by giving a few examples with known answers. It is also straightforward to judge the quality of their labels. Neither is true when there are thousands of complex domain-specific labels. Training on all labels is infeasible and the quality of an annotator's judgements may be vastly different for some subsets of labels than for others. This paper proposes a set of data-driven algorithms to 1) train image annotators on how to disambiguate among automatically generated candidate labels, 2) evaluate the quality of annotators\u2019 label suggestions, and 3) weigh predictions. The algorithms adapt to the skills of each annotator both in the questions asked and the weights given to their answers. The underlying judgements are Bayesian, based on adaptive priors. We measure the benefits of these algorithms on a live user experiment related to image-based plant identification involving around 1000 people. The proposed methods are shown to enable huge gains in annotation accuracy. A standard user can correctly label around <inline-formula><tex-math notation=\"LaTeX\">$2\\%$ </tex-math></inline-formula> of our data. This goes up to <inline-formula><tex-math notation=\"LaTeX\">$80\\%$</tex-math> </inline-formula> with machine learning assisted training and assignment and up to almost <inline-formula> <tex-math notation=\"LaTeX\">$90\\%$</tex-math></inline-formula> when doing a weighted combination of several annotators\u2019 labels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3348870",
                    "name": "Maximilien Servajean"
                },
                {
                    "authorId": "144406930",
                    "name": "A. Joly"
                },
                {
                    "authorId": "1695878",
                    "name": "D. Shasha"
                },
                {
                    "authorId": "2152860",
                    "name": "Julien Champ"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                }
            ]
        },
        {
            "paperId": "efe423ee9b9d8aa58cbbdd052c5ac0d9cf963bba",
            "title": "Highly Scalable Real-Time Analytics with CloudDBAppliance",
            "abstract": "The current cloud landscape is getting populated with many applications that are being migrated to the cloud due to its convenience and ease of use. However, there are still a subset of applications that are infrequently seen in the cloud. These are data-intensive and time critical applications. Data intensive applications, in the best case, experience bad performance in the cloud, as the current database infrastructure in the cloud fails to satisfy high loads and does not provide predictable performance. Many critical applications today still run on mainframes due to their high resilience and high performance. Unfortunately, there is no equivalent in the cloud till today. The CloudDBAppliance project addresses this problem by introducing a cloud appliance for providing a database-as-a-service able to match the predictable performance, robustness and trustworthiness of on-premise architectures such as those based on mainframes. The project introduces innovations across the following areas: New Efficient Hardware Enabling In-Memory Databases: The new Bullion hardware will provide a high modular server technology with blade and density-optimized servers with a many-core architecture that will provide over 1000 cores and reach 32 TB of memory. In-Memory Data Management Solutions that Scale up efficiently on many-core architectures: (1) An ultra-scalable database with fast analytical queries for data-intensive cloud applications both on the operational and the analytical side, an offering non-existing today. (2) A real-time analytics framework for providing data mining and machine learning functions, implemented in an efficient scale-up streaming solution over the operational data. (3) An operational data lake for enterprises relying on Hadoop data lake technologies. CloudDBAppliance provides an innovative solution for providing real-time analytics combining the ultra-scalable operational database with an online analytics solution that accesses directly the operational data. The online analytics is based on a highly efficient streaming solution able to scale vertically and linearly data streaming in many-core architectures to 1000+ CPU cores. The data streaming provides algebraic operators and also allows for custom-operators to incorporate data mining and machine learning algorithms. To provide robust big data analytics on data streams for CloudDBAppliance, data analytics algorithms will be exploited with the focus on boosting their efficiency in the context of massive parallelism on many-core architectures. For this purpose, the algorithms are being implemented on top of the scalable streaming engine. A typical example analytical technique we focus further on is time series correlation mining. The time series correlation discovery algorithm is based on a recent work on fast window correlations over time series of numerical data, and concentrates on adapting the approach for the context of a big number of parallel data streams, thus making it highly scalable. The analysis is done incrementally on sliding windows of time series data, so that recent correlations are being continuously discovered in nearly real-time. The algorithm takes full advantage of the new efficient hardware architecture that enables a large in-memory storage model with low access times, which allows intermediate data to be efficiently shared across different streaming operators and across different instances of the same operator.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25017334",
                    "name": "B. Kolev"
                },
                {
                    "authorId": "145282585",
                    "name": "O. Levchenko"
                },
                {
                    "authorId": "2028901",
                    "name": "F. Masseglia"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "1575616a412c77cc413e757796d92f60a7fe4ad5",
            "title": "Scientific Workflow Execution with Multiple Objectives in Multisite Clouds",
            "abstract": "In this short paper (see [4] for the extended version), we propose a general solution based on multi-objective scheduling to execute SWfs in a multisite cloud with the following main contributions: the design of a multi-objective cost model, SSVP VM provisioning approach, ActGreedy scheduling algorithm and an extensive experimental evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108410330",
                    "name": "Ji Liu"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "33663874",
                    "name": "Daniel de Oliveira"
                },
                {
                    "authorId": "1763201",
                    "name": "M. Mattoso"
                }
            ]
        },
        {
            "paperId": "62443600f8d524523c0e0d08ed09e8d90aef95d5",
            "title": "A new privacy-preserving solution for clustering massively distributed personal times-series",
            "abstract": "New personal data fields are currently emerging due to the proliferation of on-body/at-home sensors connected to personal devices. However, strong privacy concerns prevent individuals to benefit from large-scale analytics that could be performed on this fine-grain highly sensitive wealth of data. We propose a demonstration of Chiaroscuro, a complete solution for clustering massively-distributed sensitive personal data while guaranteeing their privacy. The demonstration scenario highlights the affordability of the privacy vs. quality and privacy vs. performance tradeoffs by dissecting the inner working of Chiaroscuro - launched over energy consumption times-series -, by exposing the results obtained by the individuals participating in the clustering process, and by illustrating possible uses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2582473",
                    "name": "T. Allard"
                },
                {
                    "authorId": "2569397",
                    "name": "G. H\u00e9brail"
                },
                {
                    "authorId": "2028901",
                    "name": "F. Masseglia"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                }
            ]
        },
        {
            "paperId": "796786cc75871fae586899a6073bad019837f170",
            "title": "ThePlantGame: Actively Training Human Annotators for Domain-specific Crowdsourcing",
            "abstract": "In a typical citizen science/crowdsourcing environment, the contributors label items. When there are few labels, it is straightforward to train contributors and judge the quality of their labels by giving a few examples with known answers. Neither is true when there are thousands of domain-specific labels and annotators with heterogeneous skills. This demo paper presents an Active User Training framework implemented as a serious game called ThePlantGame. It is based on a set of data-driven algorithms allowing to (i) actively train annotators, and (ii) evaluate the quality of contributors' answers on new test items to optimize predictions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3348870",
                    "name": "Maximilien Servajean"
                },
                {
                    "authorId": "144406930",
                    "name": "A. Joly"
                },
                {
                    "authorId": "1695878",
                    "name": "D. Shasha"
                },
                {
                    "authorId": "2152860",
                    "name": "Julien Champ"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                }
            ]
        },
        {
            "paperId": "7ca1621f12e1c79056fc08467440930d24e9a14f",
            "title": "Spatial Sequential Pattern Mining for Seismic Data",
            "abstract": "A myriad of applications from different domains collects time series data for further analysis. In many of them, such as seismic datasets, the observed data is also associated to a space dimension, which corresponds, in fact, to spatial-time series. The analysis of these datasets is difficult due to both the continuous nature of the observed data and the relationship between spatial and time dimensions. Meanwhile, sequential patterns mining techniques have been successfully used in large volume of transactional databases to obtain insights from data. In this work, we start exploring the discovery of frequent sequential patterns in seismic datasets. For that, we discretize continuous values into symbols and adapt well known sequential algorithm to mine spatial-time dataset. To better understand the quality of the identified patterns, we visualize them over the original seismic traces images. Our preliminary results indicate that the study of sequence mining in seismic datasets is promising.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8543233",
                    "name": "R. Campisano"
                },
                {
                    "authorId": "145179575",
                    "name": "F. Porto"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "2028901",
                    "name": "F. Masseglia"
                },
                {
                    "authorId": "5628621",
                    "name": "Eduardo Ogasawara"
                }
            ]
        },
        {
            "paperId": "b3de61ce1665d7f8b5ddc344289b7a4e18f015f6",
            "title": "Chiaroscuro: Transparency and Privacy for Massive Personal Time-Series Clustering",
            "abstract": "The advent of on-body/at-home sensors connected to personal devices leads to the generation of fine grain highly sensitive personal data at an unprecendent rate. However, despite the promises of large scale analytics there are obvious privacy concerns that prevent individuals to share their personnal data. In this paper, we propose Chiaroscuro, a complete solution for clustering personal data with strong privacy guarantees. The execution sequence produced by Chiaroscuro is massively distributed on personal devices, coping with arbitrary connections and disconnections. Chiaroscuro builds on our novel data structure, called Diptych, which allows the participating devices to collaborate privately by combining encryption with differential privacy. Our solution yields a high clustering quality while minimizing the impact of the differentially private perturbation. Chiaroscuro is both correct and secure. Finally, we provide an experimental validation of our approach on both real and synthetic sets of time-series.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2582473",
                    "name": "T. Allard"
                },
                {
                    "authorId": "2569397",
                    "name": "G. H\u00e9brail"
                },
                {
                    "authorId": "2028901",
                    "name": "F. Masseglia"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                }
            ]
        },
        {
            "paperId": "48d5fca8a423bbb3a24c7766aa30d3fcbd4465e2",
            "title": "Parallelization of Scientific Workflows in the Cloud",
            "abstract": "Nowadays, more and more scientific experiments need to handle massive amounts of data. Their data processing consists of multiple computational steps and dependencies within them. A data-intensive scientific workflow is an appropriate tool for modeling such process. Since the execution of data-intensive scientific workflows requires large-scale computing and storage resources, a cloud environment, which provides virtually infinite resources is appealing. However, because of the general geographical distribution of scientific groups collaborating in the experiments, multisite management of data-intensive scientific workflows in the cloud is becoming an important problem. This paper presents a general study of the current state of the art of data-intensive scientific workflow execution in the cloud and corresponding multisite management techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108410330",
                    "name": "Ji Liu"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1763201",
                    "name": "M. Mattoso"
                }
            ]
        },
        {
            "paperId": "22e156d03107e62074fdb6888d05d53dd96a6eae",
            "title": "Data Replication for the Distributed Database Using Decision Support Systems",
            "abstract": "Replication is a topic of interest in the distributed computing, distributed systems, and database communities. Decision support systems became practical with the development of minicomputer, timeshare operating systems and distributed computing. Replicated data may get insufficient due to system failure, fault tolerance, and reliability. A partial Replication is quantized in the replication system will increase the non replicated system. Fault tolerance is the property that enables a system (often computer-based) to continue operating properly. Transaction Processing Replication (TP-R) and Decision-support replication schema (DDS-R) will clear the non replica and it is used to clear the server problems and system error. This process is well executed in distributed systems and it doesn\u2019t fail to detect the system errors when multiple access are multiplexed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "1771611",
                    "name": "Mounir Tlili"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "2285089822",
                    "name": "Prasanna Padmanabhan"
                },
                {
                    "authorId": "144099017",
                    "name": "L. Gruenwald"
                },
                {
                    "authorId": "35117525",
                    "name": "A. Vallur"
                },
                {
                    "authorId": "2285083902",
                    "name": "Hannes Muhleisen"
                },
                {
                    "authorId": "2285076826",
                    "name": "Tilman Walther"
                },
                {
                    "authorId": "2286359662",
                    "name": "Yi Lin"
                },
                {
                    "authorId": "2257203599",
                    "name": "Bettina Kemme"
                },
                {
                    "authorId": "2286571140",
                    "name": "Marta Patino Martinez"
                },
                {
                    "authorId": "2285078731",
                    "name": "Ricardo Jimenez"
                },
                {
                    "authorId": "1738067",
                    "name": "Z. Guessoum"
                },
                {
                    "authorId": "2257346085",
                    "name": "Jean-Pierre Briot"
                },
                {
                    "authorId": "51931512",
                    "name": "Nora Faci"
                },
                {
                    "authorId": "2285089702",
                    "name": "Byung-Gon Chun"
                },
                {
                    "authorId": "1720563",
                    "name": "F. Dabek"
                },
                {
                    "authorId": "2263453298",
                    "name": "Andreas Haeberlen"
                },
                {
                    "authorId": "1733770",
                    "name": "Emil Sit"
                },
                {
                    "authorId": "1712689",
                    "name": "Hakim Weatherspoon"
                },
                {
                    "authorId": "2298051451",
                    "name": "Frans Kaashoek"
                },
                {
                    "authorId": "1717872",
                    "name": "J. Kubiatowicz"
                }
            ]
        },
        {
            "paperId": "c29215e6c7cc5a9935e81008e58405c50c9f4256",
            "title": "MR-Part : Minimizing Data Transfers Between Mappers and Reducers in MapReduce",
            "abstract": "La reduction du transfert des donnees dans la phase \"Shuf?e\" de MapReduce est tres importante, car elle augmente la localite des donnees, et diminue le cout total des executions des jobs MapReduce. Dans la litterature, plusieurs optimisations ont ete proposees pour reduire le transfert de donnees entre les mappers et les reducers. Neanmoins, toutes ces approches sont limitees par la facon dont les cle-valeurs intermediaires sont reparties sur les mappers. Dans cet article, nous proposons une technique qui repartitionne les tuples dans le ?chier d'entree, avec l'objectif d'optimiser la distribution des cles-valeurs sur les mappers. Notre approche detecte les relations entre les tuples d'entree et les cle-valeurs intermediaires en surveillant l'execution d'un ensemble de t\u00e2ches MapReduce qui est representatif du workload. Puis, a partir des relations detectees, il affecte les tuples d'entree aux mappers, et augmente la localite des donnees lors des futures executions. Nous avons implemente notre approche dans Hadoop, et l'avons evaluee par experimentation dans Grid5000. Les resultats montrent une grande reduction dans le transfert de donnees pendant la phase \"Shuf?e\" par rapport a Hadoop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403731017",
                    "name": "M. Liroz-Gistau"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "143970078",
                    "name": "D. Agrawal"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "d884f3bfdfde9ea685993c0af1498646d1196658",
            "title": "Profile diversity in search and recommendation",
            "abstract": "We investigate profile diversity, a novel idea in searching scientic documents. Combining keyword relevance with popularity in a scoring function has been the subject of different forms of social relevance [2, 6, 9]. Content diversity has been thoroughly studied in search and advertising [4, 11], database queries [16, 5, 8], and recommendations [17, 10, 18]. We believe our work is the first to investigate profile diversity to address the problem of returning highly popular but too-focused documents. We show how to adapt Fagin's threshold-based algorithms to return the most relevant and most popular documents that satisfy content and profile diversities and run preliminary experiments on two benchmarks to validate our scoring function.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3348870",
                    "name": "Maximilien Servajean"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "32360795",
                    "name": "P. Neveu"
                }
            ]
        },
        {
            "paperId": "06fde8fd0a3314634704c12005a818027264fce3",
            "title": "P2P Techniques for Decentralized Applications",
            "abstract": "As an alternative to traditional client-server systems, Peer-to-Peer (P2P) systems provide major advantages in terms of scalability, autonomy and dynamic behavior of peers, and decentralization of control. Thus, they are well suited for large-scale data sharing in distributed environments. Most of the existing P2P approaches for data sharing rely on either structured networks (e.g., DHTs) for efficient indexing, or unstructured networks for ease of deployment, or some combination. However, these approaches have some limitations, such as lack of freedom for data placement in DHTs, and high latency and high network traffic in unstructured networks. To address these limitations, gossip protocols which are easy to deploy and scale well, can be exploited. In this book, we will give an overview of these different P2P techniques and architectures, discuss their trade-offs, and illustrate their use for decentralizing several large-scale data sharing applications. Table of Contents: P2P Overlays, Query Routing, and Gossiping / Content Distribution in P2P Systems / Recommendation Systems / Top-k Query Processing in P2P Systems",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "2204990",
                    "name": "Manal El Dick"
                }
            ]
        },
        {
            "paperId": "8680d3a2c00d1180f94259b3fc614b61bd01bc25",
            "title": "P2PShare: a Social-based P2P Data Sharing System",
            "abstract": "P2PShare is a P2P system for large-scale probabilistic data sharing that leverages content-based and expert-based recommendation. It is designed to manage probabilistic and deterministic data in P2P environments. It provides a flexible environment for integration of heterogeneous sources, and takes into account the social based aspects to discover high quality results for queries by privileging the data of friends (or friends of friends), who are expert on the topics related to the query. We have implemented a prototype of P2PShare using the Shared-Data Overlay Network (SON), an open source development platform for P2P networks using web services, JXTA and OSGi. In this paper, we describe the demo of P2PShare's main services, e.g., gossiping topics of interest among friends, key- word querying for contents, and probabilistic queries over datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2452546",
                    "name": "Fady Draidi"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "1732401",
                    "name": "D. Parigot"
                },
                {
                    "authorId": "2084779965",
                    "name": "Guillaume Verger"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1775801",
                    "name": "R\u00e9mi Coletta"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "3089466",
                    "name": "E. Castanier"
                }
            ]
        },
        {
            "paperId": "bddedff4fc2d707657ae753bdc0a263e302a9939",
            "title": "Shared-data Overlay Network (SON)",
            "abstract": "SON is a development platform for P2P networks using web services, JXTA \nand OSGi. The development of a SON application is done through the design and implementation of a set \nof components. Each component includes a technical code that provides the component services and a code \ncomponent that provides the component logic (in Java). The complex aspects of asynchronous distributed \nprogramming are separated from code components and automatically generated from an abstract description \nof services for each component by the component generator.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1732401",
                    "name": "D. Parigot"
                },
                {
                    "authorId": "1954569",
                    "name": "A. A. Lahcen"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                }
            ]
        },
        {
            "paperId": "e72e7c451fd24640794f566887123202deda5a70",
            "title": "Zenith: Scientific Data Management on a Large Scale",
            "abstract": "Modern science disciplines such as environmental science and astronomy must deal with overwhelming amounts of experimental data. Such data must be processed (cleaned, transformed, analyzed) in all kinds of ways in order to draw new conclusions and test scientific theories. Despite their differences, certain features are common to scientific data of all disciplines: massive scale; manipulated through large, distributed workflows; complexity with uncertainty in the data values, eg, to reflect data capture or observation; important metadata about experiments and their provenance; and mostly append-only (with rare updates). Furthermore, modern scientific research is highly collaborative, involving scientists from different disciplines (eg biologists, soil scientists, and geologists working on an environmental project), in some cases from different organizations in different countries. Since each discipline or organization tends to produce and manage its own data in specific formats, with its own processes, integrating distributed data and processes gets difficult as the amounts of heterogeneous data grow. In 2011, to address these challenges, we started Zenith (http://www-sop.inria.fr/teams/zenith/), a joint team between INRIA and University Montpellier 2. Zenith is located at LIRMM in Montpellier, a city that enjoys a very strong position in environmental science with major labs and groups working on related topics such as agronomy, biodiversity, water hazard, land dynamics and biology. We are developing our solutions by working closely with scientific application partners such as CIRAD and INRA in agronomy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "1bec3ff0b8c0ba902b8b9844c3c0ea2731a0d16c",
            "title": "P2Prec: a social-based P2P recommendation system",
            "abstract": "P2Prec is a social-based P2P recommendation system for large-scale content sharing that leverages content-based and social-based recommendation. The main idea is to recommend high quality documents related to query topics and contents held by useful friends (of friends) of the users, by exploiting friendship networks. We have implemented a prototype of P2Prec using the Shared-Data Overlay Network (SON), an open source development platform for P2P networks using web services, JXTA and OSGi. In this paper, we describe the demo of P2Prec's main services (installing P2Prec peers, initializing peers, gossiping topics of interest among friends, key-word querying for contents) using our prototype implemented as an application of SON.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2452546",
                    "name": "Fady Draidi"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "1732401",
                    "name": "D. Parigot"
                },
                {
                    "authorId": "2084779965",
                    "name": "Guillaume Verger"
                }
            ]
        },
        {
            "paperId": "6194f409cac7eb9ee24109417a1273828e8b9436",
            "title": "Semantic Query Reformulation in Social PDMS",
            "abstract": "We consider social peer-to-peer data management systems (PDMS), where each peer maintains both semantic mappings between its schema and some acquaintances, and social links with peer friends. In this context, reformulating a query from a peer's schema into other peer's schemas is a hard problem, as it may generate as many rewritings as the set of mappings from that peer to the outside and transitively on, by eventually traversing the entire network. However, not all the obtained rewritings are relevant to a given query. In this paper, we address this problem by inspecting semantic mappings and social links to find only relevant rewritings. We propose a new notion of 'relevance' of a query with respect to a mapping, and, based on this notion, a new se-mantic query reformulation approach for social PDMS, which achieves great accuracy and flexibility. To find rapidly the most interesting mappings, we combine several techniques: (i) social links are expressed as FOAF links to characterize peer's friendship and compact mapping summaries are used to obtain mapping descriptions; (ii) local semantic views are special views that contain information about external mappings; and (iii) gossiping techniques improve the search of relevant mappings. Our experimental evaluation, based on a prototype on top of PeerSim and a simulated network demonstrate that our solution yields greater recall, compared to traditional query translation approaches proposed in the literature.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1699192",
                    "name": "A. Bonifati"
                },
                {
                    "authorId": "1772316",
                    "name": "G. Summa"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "2452546",
                    "name": "Fady Draidi"
                }
            ]
        },
        {
            "paperId": "6b5fdec45131d087987b566ccfdb029a1b04e9f2",
            "title": "Isolation Levels for Data Sharing in Large-Scale Scientific Workflows",
            "abstract": "Scientists can benefit from Grid and Cloud infrastructures to face the increasing need to share scientific data and execute data-intensive workflows at a large scale. However, these workflows are creating more and more challenging problems in the automation of data management during execution. Existing workflow management systems focus on how data is stored, transfered and on data provenance. However they lack in managing isolation during the execution of tasks of the same or different workflows that read/update shared data. In this scope, we propose three isolation levels taking into account data provenance and multiversioning. In the best of our knowledge this is the first proposal in such context.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403731017",
                    "name": "M. Liroz-Gistau"
                },
                {
                    "authorId": "39516613",
                    "name": "Hinde-Lilia Bouziane"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                }
            ]
        },
        {
            "paperId": "da27ba9179ee26df520cd39fbf201dbd5b664f34",
            "title": "Leveraging Social and Content-based Recommendation in P2P Systems",
            "abstract": "We focus on peer-to-peer (P2P) content recommendation for on-line communities, where social relationships between users can be exploited as a parameter to increase the trust of recommendation. Most of the existing solutions establish friendship relationships based on users behavior or declared trust. In this paper, we propose a novel P2P recommendation approach (called F2Frec) that leverages content and social-based recommendation by maintaining a P2P and friend-to-friend network. This network is used as a basis to provide useful and high quality recommendations. Based on F2Frec, we propose new metrics, such as usefulness and similarity (among users and their respective friend network), necessary to enable friendship establishment and to select recommendations. We define our proposed metrics based on users' topic of interest and relevant topics that are automatically extracted from the contents stored by each user. Our experimental evaluation, using the TREC09 dataset and Wiki vote social network, shows the benefits of our approach compared to anonymous recommendation. In addition, we show that F2Frec increases recall by a factor of 8.8 compared with centralized collaborative filtering.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2452546",
                    "name": "Fady Draidi"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "2491017",
                    "name": "M. Cart"
                },
                {
                    "authorId": "39516613",
                    "name": "Hinde-Lilia Bouziane"
                }
            ]
        },
        {
            "paperId": "01b256bdd8bd639b51c5f92670f889290d140cca",
            "title": "Scalable P2P Reconciliation Infrastructure for Collaborative Text Editing",
            "abstract": "We address the problem of optimistic replication forcollaborative text editing in Peer-to-Peer (P2P) systems. This problem is challenging because of concurrent updating at multiple peers and dynamic behavior of peers. Operationaltransformation (OT) is a typical approach used for handlingoptimistic replication in the context of distributed text editing. However, most of OT solutions are neither scalable nor suited for P2P networks due to the dynamic behavior of peers. In this paper, we propose a scalable P2P reconciliation infrastructure for OT that assures eventual consistency and liveness despite dynamicity and failures. We propose a P2P logging and timestamping service called P2P-LTR (P2P Logging and Timestamping for Reconciliation) which exploits a distributed hash table (DHT) for reconciliation. While updating replica copies at collaborating peer editors, updates are stored in ahighly available P2P log. To enforce eventual consistency, these updates must be retrieved in a specific total order to be reconciled at the peer editors. P2P-LTR provides an efficient mechanism for determining the total order of updates. It also deals with the case of peers that may join and leave the system during the update operation. We evaluated the performance of P2P-LTR through simulation; the results show the efficiency and the scalability of our solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1771611",
                    "name": "Mounir Tlili"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "85d55dd6d71a15ac1eca2ebbf07b204f689fbe96",
            "title": "P2Prec: a Social-based P2P Recommendation System for Large-scale Data Sharing",
            "abstract": "We propose P2Prec, a P2P recommendation system for large-scale data sharing, which exploits friendship links. The main idea is to recommend high quality contents related to query topics and contents of friends (or friends of friends), who are expert on the topics related to the query. Expertise is implicitly deduced based on the contents stored by a user. To exploit friendship links, we rely on Friend-Of-A-Friend (FOAF) descriptions. To disseminate information about experts, we propose new semantic-based gossip algorithms that provide scalability, robustness, simplicity and load balancing. By using information retrieval techniques, we propose an efficient query routing algorithm that recommends the best peers to serve a query. In our experimental evaluation, using the TREC09 dataset and Wiki vote social network, we show that using semantic gossiping increases recall by a factor of 2.5 compared with well known random gossiping. Furthermore, P2Prec has the ability to get reasonable recall with acceptable query processing load and network traffic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2452546",
                    "name": "Fady Draidi"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "2257203599",
                    "name": "Bettina Kemme"
                }
            ]
        },
        {
            "paperId": "a3a1e79d22b59558f4c889fbefa8a2ae150f96bb",
            "title": "Improving Many-Task computing in scientific workflows using P2P techniques",
            "abstract": "Large-scale scientific experiments are usually supported by scientific workflows that may demand high performance computing infrastructure. Within a given experiment, the same workflow may be explored with different sets of parameters. However, the parallelization of the workflow instances is hard to be accomplished mainly due to the heterogeneity of its activities. Many-Task computing paradigm seems to be a candidate approach to support workflow activity parallelism. However, scheduling a huge amount of workflow activities on large clusters may be susceptible to resource failures and overloading. In this paper, we propose Heracles, an approach to apply consolidated P2P techniques to improve Many-Task computing of workflow activities on large clusters. We present a fault tolerance mechanism, a dynamic resource management and a hierarchical organization of computing nodes to handle workflow instances execution properly. We have evaluated Heracles by executing experimental analysis regarding the benefits of P2P techniques on the workflow execution time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145021511",
                    "name": "Jonas Dias"
                },
                {
                    "authorId": "5628621",
                    "name": "Eduardo Ogasawara"
                },
                {
                    "authorId": "33663874",
                    "name": "Daniel de Oliveira"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "1763201",
                    "name": "M. Mattoso"
                }
            ]
        },
        {
            "paperId": "d15f156378ec71c648665e7b7329a4980d6e20f5",
            "title": "P2Prec: a Recommendation Service for P2P Content Sharing Systems",
            "abstract": "In this paper, we propose P2Prec, a recommendation service for P2P content sharing systems that exploits users' social data. The key idea is to recommend to a user high quality documents in a specific topic using ratings of friends (or friends of friends) who are expert in that topic. To manage users' social data, we rely on Friend-Of-A-Friend (FOAF) descriptions. P2Prec has a hybrid P2P architecture to work on top of any P2P content sharing system. It combines efficient DHT indexing to manage the users' FOAF files with gossip robustness to disseminate the topics of expertise between friends. In our experimental evaluation, using the CiteSeer dataset, we show that P2Prec has the ability to get the maximum recall with very good performance. Furthermore, it increases recall and precision by a factor of 2 compared with centralized solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2452546",
                    "name": "Fady Draidi"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "2257203599",
                    "name": "Bettina Kemme"
                }
            ]
        },
        {
            "paperId": "1e65b6b5643904786da93774db85d23c3c005e6a",
            "title": "Leveraging P2P overlays for Large-scale and Highly Robust Content Distribution and Search",
            "abstract": "In the last decade, there has been a tendency of shifting content distribution towards peer-to-peer (P2P) technology. The reason behind this is the self-scalability of P2P systems provided by the principles of communal collaboration and resource sharing in P2P systems. By building a P2P Content Distribution Network (CDN), peers collaborate to distribute the content of under-provisioned websites and to serve queries for large audiences on behalf of the websites. When designing a P2P CDN, the main challenge is to actually maintain an acceptable level of performance in terms of client-perceived latency and hit ratio while minimizing the incurred overhead. This is not a straightforward endeavor given that the P2P CDN relies on autonomous and dynamic peers rather than a dedicated infrastructure. Indeed, the distribution of duties and content over peers should take into account their interests in order to give them proper incentives to cooperate. Moreover, the P2P-CDN should adapt to increasing numbers of participants and provide robust algorithms under high levels of churn because these issues have a key impact on performance. Finally, the routing of queries should aim peers close in locality and serve content from close-by providers to achieve short latencies. This paper gives an overview of our contributions in designing and maintaining a P2P CDN that tackles the issues identied above. First, we present Flower-CDN [3], a P2P content distribution network (CDN) that tackles some of these issues. Peers store only content of websites they are interested in and serve them to others. Furthermore, peers can nd close-by content providers by a locality aware P2P directory structure. Secondly, we present a highly scalable approach of Flower-CDN called PetalUp-CDN which dynamically adjusts the directory structure in order to avoid overload situations and to keep the index information any peer must maintain at an acceptable level. Thirdly, we discuss maintenance protocols for Flower-CDN and PetalUp-CDN to cope with the worst scenarios of churn. The performance evaluation wrt. scalability and churn management shows that",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2204990",
                    "name": "Manal El Dick"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                }
            ]
        },
        {
            "paperId": "40f9ea1605e3557454b6fc525e8dd58091501a4c",
            "title": "Content Distribution in P2P Systems",
            "abstract": "The report provides a literature review of the state-of-the-art for content distribution. The report's contributions are of threefold. First, it gives more insight into traditional Content Distribution Networks (CDN), their requirements and open issues. Second, it discusses Peer-to-Peer (P2P) systems as a cheap and scalable alternative for CDN and extracts their design challenges. Finally, it evaluates the existing P2P systems dedicated for content distribution according to the identied requirements and challenges.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2204990",
                    "name": "Manal El Dick"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                }
            ]
        },
        {
            "paperId": "4a660839fcc81a962d22a78fc7d4fa0c35a3271d",
            "title": "Deployment of Flower-CDN",
            "abstract": "Flower-CDN is a Peer-to-Peer Content Distribution Network (P2P CDN) that enables any under-provisionned website to distribute its content by relying on the community interested in its content. Therefore the deployment of Flower-CDN is supported by the clients that are willing to contribute in behalf of the website of their interest. This report provides the first guidelines to make Flower-CDN available for public use. We propose to implement Flower-CDN functionality as an extension for the user's web browser. As such, the user enjoys a transparent, flexible and highly configurable experience with Flower-CDN. Furthermore, we design an implementation architecture that covers security and privacy issues in a simple and practical manner.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2204990",
                    "name": "Manal El Dick"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                }
            ]
        },
        {
            "paperId": "4bb0fd19414e3f2e68708de175d1bf5fccd707ab",
            "title": "P2P Join Query Processing over Data Streams",
            "abstract": "Recent years have witnessed the growth of a new class of data-intensive applications that do not fit the DBMS data model and querying paradigm. Instead, the data arrive at high speeds taking the form of an unbounded sequence of values (data streams) and queries run continuously returning new results as new data arrive. In these applications, data streams from external sources flow into a Data Stream Management System (DSMS) where they are processed by different operators. Many applications share the same need for processing data streams in a continuous fashion. For most distributed streaming applications, the centralized processing of continuous queries over distributed data is simply not viable. This paper addresses the problem of computing continuous join queries over distributed data streams. We present a new method, called DHTJoin that exploits the power of a Distributed Hash Table (DHT) combining hash-based placement of tuples and dissemination of queries by exploiting the embedded trees in the underlying DHT, thereby incuring little overhead. Unlike state of the art solutions that index all data, DHTJoin identi\ufb01es, using query predicates, a subset of tuples in order to index the data required by the user's queries, thus reducing network traffic. DHTJoin tackles the dynamic behavior of DHT networks during query execution and dissemination of queries. We provide a performance evaluation of DHTJoin which shows that it can achieve significant performance gains in terms of network traffic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067136841",
                    "name": "Wenceslao Palma"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "8ff08f6089c397d3c1c246959f3498b601d5a5d1",
            "title": "Flower-CDN: a hybrid P2P overlay for efficient query processing in CDN",
            "abstract": "Many websites with a large user base, e.g., websites of nonprofit organizations, do not have the financial means to install large web-servers or use specialized content distribution networks such as Akamai. For those websites, we have developed Flower-CDN, a locality-aware P2P based content-distribution network (CDN) in which the users that are interested in a website support the distribution of its content. The idea is that peers keep the content they retrieve and later serve it to other peers that are close to them in locality. Our architecture is a hybrid between structured and unstructured networks. When a new client requests some content from a website, a locality-aware DHT quickly finds a peer in its neighborhood that has the content available. Additionally, all peers in a given locality that maintain content of a particular website build an unstructured content overlay. Within this overlay, peers gossip information about their content allowing the system to maintain accurate information despite churn. In our performance evaluation, we compare Flower-CDN with an existing P2P-CDN strictly based on DHT and not locality aware. Flower-CDN reduces lookup latency by a factor of 9 and transfer distance by a factor of 2. We also show that Flower-CDN's gossip has low overhead and can be adjusted according to hit ratio requirements and bandwidth availability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2204990",
                    "name": "Manal El Dick"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "2257203599",
                    "name": "Bettina Kemme"
                }
            ]
        },
        {
            "paperId": "ab9398d1385119948d392d860ab77a5a5b002719",
            "title": "Distributed processing of continuous join queries using DHT networks",
            "abstract": "This paper addresses the problem of computing approximate answers to continuous join queries. We present a new method, called DHTJoin, which combines hash-based placement of tuples in a Distributed Hash Table (DHT) and dissemination of queries exploiting the trees formed by the underlying DHT links. DHTJoin distributes the query workload across multiple DHT nodes and provides a mechanism that avoids indexing tuples that cannot contribute to join results. We provide a performance evaluation which shows that DHTJoin can achieve significant performance gains in terms of network traffic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067136841",
                    "name": "Wenceslao Palma"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "bec7daca8c234b8f1cc0e8f92da8172087864f17",
            "title": "A Highly Robust P2P-CDN under Large-Scale and Dynamic Participation",
            "abstract": "By building a P2P Content Distribution Network (CDN), peers collaborate to distribute the content of underprovisionned websites and to serve queries for larger audiences on behalf of the websites. This can reveal very challenging, given the highly dynamic and autonomous participation of peers. Indeed, the P2P-CDN should adapt to increasing numbers of participants and provide robust algorithms under churn because these issues have a key impact on performance. Also, the distribution of tasks and content over peers should take into account their interests in order to give them proper incentives to cooperate. Finally, the routing of queries should aim peers close in locality and serve content from close-by providers to reduce network overload and achieve scalability. We have previously proposed a locality and interest-aware P2P-CDN, Flower-CDN, that lacks efficient management of robustness and scalability. In this paper, we focus on these crucial shortcomings and propose PetalUp-CDN. The performance evaluation wrt. scalability and churn shows highly significant gains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2204990",
                    "name": "Manal El Dick"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "2257203599",
                    "name": "Bettina Kemme"
                }
            ]
        },
        {
            "paperId": "fc9963c5f42b05b94407c32d710343dde4f784f4",
            "title": "SARAV\u00c1: data sharing for online communities in P2P",
            "abstract": "This paper describes SARAVA, a research project that aims at investigating new challenges in P2P data sharing for online communities. The major advantage of P2P is a completely decentralized approach to data sharing which does not require centralized administration. Users may be in high numbers and interested in different kinds of collaboration and sharing their knowledge, ideas, experiences, etc. Data sources can be in high numbers, fairly autonomous, i.e. locally owned and controlled, and highly heterogeneous with different semantics and structures. Our project deals with new, decentralized data management techniques that scale up while addressing the autonomy, dynamic behavior and heterogeneity of both users and data sources. In this context, we focus on two major problems: query processing with uncertain data and management of scientific workflows.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1763201",
                    "name": "M. Mattoso"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "1740182",
                    "name": "V. Braganholo"
                },
                {
                    "authorId": "144674433",
                    "name": "Alexandre A. B. Lima"
                }
            ]
        },
        {
            "paperId": "02aa120c7bd21dee56097ec3f9ec5bafcc059cfc",
            "title": "Adaptive hybrid partitioning for OLAP query processing in a database cluster",
            "abstract": "We consider the use of a database cluster for high-performance support of Online Analytical Processing (OLAP) applications. OLAP intra-query parallelism can be obtained by partitioning the database tables across cluster nodes. We propose to combine physical and virtual partitioning into a partitioning scheme called Adaptive Hybrid Partitioning (AHP). AHP requires less disk space while allowing for load balancing. We developed a prototype for OLAP parallel query processing in database clusters using AHP. Our experiments on a 32-node database cluster using the TPC-H benchmark demonstrate linear and super-linear speedup. Thus, AHP can reduce significantly the execution time of typical OLAP queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31776208",
                    "name": "Camille Furtado"
                },
                {
                    "authorId": "144674433",
                    "name": "Alexandre A. B. Lima"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1763201",
                    "name": "M. Mattoso"
                }
            ]
        },
        {
            "paperId": "5214b8448dc23ec2660189e84906913d889370d9",
            "title": "XWiki concerto: a P2P wiki for nomadic workers",
            "abstract": "XWiki Concerto is a research project aiming at evolving the open-source XWiki engine toward a P2P architecture supporting mobility, offline work and replication of content across a large number of peers. This research brings XWiki the capacity to (i) run on a fault tolerant P2P architecture allowing to scale to millions of users using a secure solution supporting replication and synchronization of contents, (ii) support mobile workers in their daily collaborative activities. This paper presents the research challenges of the project and the algorithms that have been designed for collaborative P2P editing.\n XWiki Concerto brings together XWiki, INRIA, ENST, EISTI, and Mandriva and is sponsored by the French Research Agency. The XWiki Concerto components will be released as LGPL software.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1771611",
                    "name": "Mounir Tlili"
                },
                {
                    "authorId": "2487785",
                    "name": "W. Dedzo\u00e9"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "3025899",
                    "name": "P. Molli"
                },
                {
                    "authorId": "3105410",
                    "name": "G. Canals"
                },
                {
                    "authorId": "2075856247",
                    "name": "Julien Maire"
                },
                {
                    "authorId": "31259576",
                    "name": "G. Oster"
                },
                {
                    "authorId": "2252393",
                    "name": "L. Dubost"
                },
                {
                    "authorId": "2192027",
                    "name": "Sergiu Dumitriu"
                },
                {
                    "authorId": "2578347",
                    "name": "St\u00e9phane Lauri\u00e8re"
                },
                {
                    "authorId": "2317605",
                    "name": "F. Mancinelli"
                }
            ]
        },
        {
            "paperId": "5e3d75f744c9259fef01c04eeb948012c32f30dc",
            "title": "P2P logging and timestamping for reconciliation",
            "abstract": "In this paper, we address data reconciliation in peer-to-peer (P2P) collaborative applications. We propose P2P-LTR (Logging and Timestamping for Reconciliation) which provides P2P logging and timestamping services for P2P reconciliation over a distributed hash table (DHT). While updating at collaborating peers, updates are timestamped and stored in a highly available P2P log. During reconciliation, these updates are retrieved in total order to enforce eventual consistency. In this paper, we first give an overview of P2P-LTR with its model and its main procedures. We then present our prototype used to validate P2P-LTR. To demonstrate P2P-LTR, we propose several scenarios that test our solutions and measure performance. In particular, we demonstrate how P2P-LTR handles the dynamic behavior of peers with respect to the DHT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1771611",
                    "name": "Mounir Tlili"
                },
                {
                    "authorId": "2487785",
                    "name": "W. Dedzo\u00e9"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "3025899",
                    "name": "P. Molli"
                },
                {
                    "authorId": "3105410",
                    "name": "G. Canals"
                },
                {
                    "authorId": "2578347",
                    "name": "St\u00e9phane Lauri\u00e8re"
                }
            ]
        },
        {
            "paperId": "618b21a1abf742629567ebb36b3b8b283e37d0b6",
            "title": "Acceleration of a procedure to generate fractal curves of a given dimension through the probabilistic analysis of execution time",
            "abstract": "In a previous work [Ortega et al. 03], the authors have described the use of grammatical evolution to automatically generate L Systems (LS) representing fractal curves with a pre-determined fractal dimension. The experiments presented in this paper prove that the efficiency of this procedure is variable, with very different execution times for different executions with the same fractal dimension target. The paper shows that the probabilistic distribution of execution times belongs to a well-known family of random variables: heavytailed distributions. This analysis explains the erratic performance of the algorithm and suggests the use of a technique that corrects this variability and improves the efficiency about one order of magnitude. Acknowledgements: This paper has been sponsored by the Spanish Ministry of Science and Technology, project numbers TIC2001-0685-C02-01 and TIC 200201948. INTRODUCTION Our procedure to generate fractal curves with a required dimension consists of three parts: a) representation of fractals by means of L Systems (LS); b) computation of the fractal dimension from the grammar; c) application of a grammar-evolution based genetic algorithm to get a grammar representing a fractal with the required dimension. LS provide a powerful tool to represent fractals in the class of recursive transformations. The iterator may be represented by means of production rules, while the initiator corresponds to the axiom. The fractal curve is generated by the sequence of words derived from the axiom, by means of a representation scheme: vector graphics or turtle graphics. In a previous work [Alfonseca and Ortega 01] we have described an algorithm that estimates the fractal dimension of a non-trivial set of these fractals from their equivalent LS by means of symbolic manipulation, without the need of graphical procedures. In [Ortega et al. 03] we applied Grammar Evolution (GE) [O\u2019Neill and Ryan 2001] to obtain the LS equivalent to a fractal with the required dimension. The proposed procedure has a clear biological inspiration acting on three different levels: a genotype (a vector of integers) an intermediate level, equivalent to proteins (LS) and a phenotype (the fractal curve). See Figure 1.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112893280",
                    "name": "P. Gray"
                },
                {
                    "authorId": "1796237",
                    "name": "Todd Eavis"
                },
                {
                    "authorId": "1733199",
                    "name": "A. Inselberg"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1799376",
                    "name": "G. Graefe"
                },
                {
                    "authorId": "2061778718",
                    "name": "Hansj\u00f6rg Zeller"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "145624228",
                    "name": "Christoph E. Koch"
                },
                {
                    "authorId": "143758471",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "1733268",
                    "name": "B. Kemme"
                },
                {
                    "authorId": "2115529060",
                    "name": "Yuqing Wu"
                },
                {
                    "authorId": "2117996586",
                    "name": "Hong Cheng"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "1393644275",
                    "name": "P. Cudr\u00e9-Mauroux"
                },
                {
                    "authorId": "1740364",
                    "name": "Wojciech Galuba"
                },
                {
                    "authorId": "69947993",
                    "name": "Sarunas Girdzijauskas"
                },
                {
                    "authorId": "1743906",
                    "name": "P. Felber"
                },
                {
                    "authorId": "9169892",
                    "name": "E. Biersack"
                },
                {
                    "authorId": "1687892",
                    "name": "Anastasios Kementsietsidis"
                },
                {
                    "authorId": "1732298",
                    "name": "P. Triantafillou"
                },
                {
                    "authorId": "1757596",
                    "name": "Ioannis Aekaterinidis"
                },
                {
                    "authorId": "145688317",
                    "name": "Anwitaman Datta"
                },
                {
                    "authorId": "1751591",
                    "name": "G. Weikum"
                },
                {
                    "authorId": "1714278",
                    "name": "Alexander Thomasian"
                },
                {
                    "authorId": "48843863",
                    "name": "P. Bonnet"
                },
                {
                    "authorId": "1695878",
                    "name": "D. Shasha"
                },
                {
                    "authorId": "1809784",
                    "name": "N. Lorentzos"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                },
                {
                    "authorId": "1897235",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "35119829",
                    "name": "Ruihua Song"
                },
                {
                    "authorId": "145898058",
                    "name": "Wil M.P. van der Aalst"
                },
                {
                    "authorId": "1710939",
                    "name": "C. Dyreson"
                },
                {
                    "authorId": "2958795",
                    "name": "S. Lightstone"
                },
                {
                    "authorId": "2108568943",
                    "name": "Ryan Johnson"
                },
                {
                    "authorId": "1781993",
                    "name": "E. Pitoura"
                },
                {
                    "authorId": "1783767",
                    "name": "K. Wada"
                },
                {
                    "authorId": "1725663",
                    "name": "David Toman"
                },
                {
                    "authorId": "1702232",
                    "name": "George Karabatis"
                },
                {
                    "authorId": "1896793",
                    "name": "Cristina Sirangelo"
                },
                {
                    "authorId": "1788571",
                    "name": "G. Grahne"
                },
                {
                    "authorId": "1393591007",
                    "name": "J. Domingo-Ferrer"
                },
                {
                    "authorId": "3270238",
                    "name": "Ethan Zhang"
                },
                {
                    "authorId": "11859917",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "1750995",
                    "name": "Ben Carterette"
                },
                {
                    "authorId": "1703980",
                    "name": "Nick Craswell"
                },
                {
                    "authorId": "1753628",
                    "name": "J. Kamps"
                },
                {
                    "authorId": "1796253",
                    "name": "Y. Manolopoulos"
                },
                {
                    "authorId": "1714996",
                    "name": "Y. Theodoridis"
                },
                {
                    "authorId": "1761528",
                    "name": "V. Tsotras"
                },
                {
                    "authorId": "152555512",
                    "name": "H. Shen"
                },
                {
                    "authorId": "1707003",
                    "name": "P. Hung"
                },
                {
                    "authorId": "1758225",
                    "name": "Vivying S. Y. Cheng"
                },
                {
                    "authorId": "2935513",
                    "name": "Chris Clifton"
                },
                {
                    "authorId": "153697517",
                    "name": "Y. Zheng"
                },
                {
                    "authorId": "48922886",
                    "name": "S. Chow"
                },
                {
                    "authorId": "1398881851",
                    "name": "S. Fischer-H\u00fcbner"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                },
                {
                    "authorId": "1785829",
                    "name": "T. Roelleke"
                },
                {
                    "authorId": "2152808927",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "2072942186",
                    "name": "S. Robertson"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2108304551",
                    "name": "Jinchuan Chen"
                },
                {
                    "authorId": "1728462",
                    "name": "V. S. Subrahmanian"
                },
                {
                    "authorId": "48183556",
                    "name": "Ben He"
                },
                {
                    "authorId": "1691929",
                    "name": "D. Hiemstra"
                },
                {
                    "authorId": "1739132",
                    "name": "N. Palmer"
                },
                {
                    "authorId": "1739500",
                    "name": "D. Ardagna"
                },
                {
                    "authorId": "2549968",
                    "name": "Pat Helland"
                },
                {
                    "authorId": "143619288",
                    "name": "G. Ram\u00edrez"
                },
                {
                    "authorId": "145980720",
                    "name": "A. Trotman"
                },
                {
                    "authorId": "1687211",
                    "name": "P. Boncz"
                },
                {
                    "authorId": "1402263442",
                    "name": "K. Pinel-Sauvagnat"
                },
                {
                    "authorId": "34582619",
                    "name": "W. Tan"
                },
                {
                    "authorId": "1399712184",
                    "name": "Sarah Cohen-Boulakia"
                },
                {
                    "authorId": "145641161",
                    "name": "H. Jacobsen"
                },
                {
                    "authorId": "1891854",
                    "name": "Y. Diao"
                },
                {
                    "authorId": "143666627",
                    "name": "M. Franklin"
                },
                {
                    "authorId": "145249669",
                    "name": "D. Maier"
                },
                {
                    "authorId": "33811160",
                    "name": "Peter A. Tucker"
                }
            ]
        },
        {
            "paperId": "8dd3a71201450daff396389bb08f3cf49d0ee48b",
            "title": "Parallel query processing for OLAP in grids",
            "abstract": "OLAP query processing is critical for enterprise grids. Capitalizing on our experience with the ParGRES database cluster, we propose a middleware solution, GParGRES, which exploits database replication and inter\u2010 and intra\u2010query parallelism to efficiently support OLAP queries in a grid. GParGRES is designed as a wrapper that enables the use of ParGRES in PC clusters of a grid (in our case, Grid5000). Our approach has two levels of query splitting: grid\u2010level splitting, implemented by GParGRES, and node\u2010level splitting, implemented by ParGRES. GParGRES has been partially implemented as database grid services compatible with existing grid solutions such as the open grid service architecture and the Web services resource framework. We give preliminary experimental results obtained with two clusters of Grid5000 using queries of the TPC\u2010H Benchmark. The results show linear or almost linear speedup in query execution, as more nodes are added in all tested configurations. Copyright \u00a9 2008 John Wiley & Sons, Ltd.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053263",
                    "name": "Nelson Kotowski"
                },
                {
                    "authorId": "144674433",
                    "name": "Alexandre A. B. Lima"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1763201",
                    "name": "M. Mattoso"
                }
            ]
        },
        {
            "paperId": "952a1a3e6e602d71f268925c1ef2cf4ca45cb55d",
            "title": "P2P logging and timestamping for XWiki",
            "abstract": "Collaborative applications are getting an increasing interest as a result of the rapid progress in distributed technologies such as grid computing. P2P applications, and mobile computing. Constructing these applications on top of P2P networks has many advantages which stem from P2P properties: decentralization, self-organization, scalability and fault-tolerance\n In this paper, we address data replication and reconciliation in XWiki peer-to-peer (P2P) collaborative application. We propose a new mechanism of optimistic replication, which consists of an algorithm for data reconciliation based on operational transformation approach (OT) and an extension of KTS service that performs distributed timestamp generation running over a distributed hash table (DHT). While updating at collaborating XWiki peers, updates are timestamped and stored in a highly available P2P log. During reconciliation, these updates are retrieved in total order to enforce eventual consistencies.\n The work presented here is partially supported by the National Agency of Research within RNTL XWiki Concerto project (2007--2009).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1771611",
                    "name": "Mounir Tlili"
                },
                {
                    "authorId": "2487785",
                    "name": "W. Dedzo\u00e9"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "2252393",
                    "name": "L. Dubost"
                },
                {
                    "authorId": "2192027",
                    "name": "Sergiu Dumitriu"
                },
                {
                    "authorId": "2578347",
                    "name": "St\u00e9phane Lauri\u00e8re"
                },
                {
                    "authorId": "3105410",
                    "name": "G. Canals"
                },
                {
                    "authorId": "3025899",
                    "name": "P. Molli"
                },
                {
                    "authorId": "2075856247",
                    "name": "Julien Maire"
                }
            ]
        },
        {
            "paperId": "e47c6d9481b9e31f16306f819049d97ad7b3bb89",
            "title": "Proceedings of the 2008 International Workshop on Data Management in Peer-to-Peer Systems, DaMaP 2008, Nantes, France, March 25, 2008",
            "abstract": "This volume contains papers selected for presentation at the International Workshop on Data Management in Peer-to-Peer Systems (DAMAP 2008), held on March 25th 2008, in Nantes (France), in conjunction with the 11th International Conference on Extending Database Technology (EDBT 08). \n \nThe purpose of this first edition of the Damap workshop is to bring together researchers who are involved in designing, managing and implementing peer-to-peer systems for large scale data management applications. The goal is to provide a forum for the exchange of ideas, knowledge and experiences in this domain, and to debate new issues and directions for research and development work in the future. \n \nAfter thorough review process for each of the 14 submissions, we have selected 9 papers. The program also included a keynote speech by Wolfgang Neijdl, researcher at the University of Hannover, Germany.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1706235",
                    "name": "Anne Doucet"
                },
                {
                    "authorId": "1721361",
                    "name": "S. Gan\u00e7arski"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                }
            ]
        },
        {
            "paperId": "fa50abadba56e06083480c45a88507e4d6da0abc",
            "title": "Data Reconciliation in P2P Collaborative Applications",
            "abstract": "In this paper, we address data reconciliation in peer-to-peer (P2P) collaborative applications. We propose P2P-LTR (Logging and Timestamping for Reconciliation) which provides P2P logging and timestamping services for P2P reconciliation over a distributed hash table (DHT). While updating at collaborating peers, updates are timestamped and stored in a highly available P2P log. During reconciliation, these updates are retrieved in total order to enforce eventual consistency. In this paper, we first give an overview of P2P-LTR with its model and its main procedures. We then present our prototype used to validate P2P-LTR. To demonstrate P2P-LTR, we propose several scenarios that test our solutions and measure performance. In particular, we demonstrate how P2P-LTR handles the dynamic behavior of peers with respect to the DHT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1771611",
                    "name": "Mounir Tlili"
                },
                {
                    "authorId": "2487785",
                    "name": "W. Dedzo\u00e9"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "3025899",
                    "name": "P. Molli"
                },
                {
                    "authorId": "3105410",
                    "name": "G. Canals"
                },
                {
                    "authorId": "2075856247",
                    "name": "Julien Maire"
                },
                {
                    "authorId": "2578347",
                    "name": "St\u00e9phane Lauri\u00e8re"
                }
            ]
        },
        {
            "paperId": "3636413a8989a90e2ff80ff24f34a6c5874a5c0b",
            "title": "Supporting Data Currency in Replicated DHTs using a Key-based Timestamping Service",
            "abstract": "Distributed Hash Tables (DHTs) provide a scalable solution for data sharing in P2P systems. To ensure high data availability, DHTs typically rely on data replication, yet without data currency guarantees. Supporting data currency in replicated DHTs is difficult as it requires the ability to return a current replica despite peers leaving the network or concurrent updates. In this paper, we give a complete solution to this problem. We propose an Update Management Service (UMS) to deal with data availability and efficient retrieval of current replicas based on timestamping. For generating timestamps, we propose a Key-based Timestamping Service (KTS) which performs distributed timestamp generation using local counters. Through probabilistic analysis, we compute the expected number of replicas which UMS must retrieve for finding a current replica. Except for the cases where the availability of current replicas is very low, the expected number of retrieved replicas is typically small, e.g. if at least 35% of available replicas are current then the expected number of retrieved replicas is less than 3. We validated our solution through implementation and experimentation over a 64-node cluster and evaluated its scalability through simulation up to 10,000 peers using SimJava. The results show the effectiveness of our solution. They also show that our algorithm used in UMS achieves major performance gains, in terms of response time and communication cost, compared with a baseline algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "72073f487c95a08d1e430660e62c5f244b7d1a28",
            "title": "Best Position Algorithms for Top-k Queries",
            "abstract": "The general problem of answering top-k queries can be modeled using lists of data items sorted by their local scores. The most efficient algorithm proposed so far for answering top-k queries over sorted lists is the Threshold Algorithm (TA). However, TA may still incur a lot of useless accesses to the lists. In this paper, we propose two new algorithms which stop much sooner. First, we propose the best position algorithm (BPA) which executes top-k queries more efficiently than TA. For any database instance (i.e. set of sorted lists), we prove that BPA stops as early as TA, and that its execution cost is never higher than TA. We show that the position at which BPA stops can be (m-1) times lower than that of TA, where m is the number of lists. We also show that the execution cost of our algorithm can be (m-1) times lower than that of TA. Second, we propose the BPA2 algorithm which is much more efficient than BPA. We show that the number of accesses to the lists done by BPA2 can be about (m-1) times lower than that of BPA. Our performance evaluation shows that over our test databases, BPA and BPA2 achieve significant performance gains in comparison with TA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "7b09cbcf2201b4ca81855d6af047b02245df6bd5",
            "title": "Query processing in P2P systems",
            "abstract": "Peer-to-peer (P2P) computing offers new opportunities for building highly distributed data systems. Unlike client-server computing, P2P is a very dynamic environment where peers can join and leave the network at any time. This yields important advantages such as operation without central coordination, peers autonomy, and scale up to large number of peers. However, providing high-level data management services is difficult. Most techniques designed in distributed database systems which statically exploit schema and network information no longer apply. New techniques are needed which should be decentralized, dynamic and self-adaptive. In this paper, we survey the techniques which have been developed for query processing in P2P systems. We first give an overview of the existing P2P networks, and com-pare their properties from the perspective of data management. Then, we discuss the ap-proaches which are used for schema mapping. Then, we describe the algorithms which have been proposed for query routing. In particular, we focus on query routing in unstructured net-works and DHTs. Finally, we present the techniques which have been proposed for processing complex queries, e.g. top-k queries, in P2P systems, in particular in DHTs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "b8d9c723b9d8a165fdd8539dc14051f81754f3c6",
            "title": "OLAP Query Processing in Grids",
            "abstract": "OLAP query processing is critical for enterprise grids. Capitalizing on our experience with the ParGRES database cluster, we propose a middleware solution, GParGRES, which exploits database replication and interand intra-query parallelism to efficiently support OLAP queries in a grid. GParGRES has been partially implemented as database grid services on Grid5000. We give preliminary experimental results obtained with two clusters of Grid5000 using queries of the TPC-H Benchmark. The results show linear or almost linear speedup in query execution, as more nodes are added in all tested configurations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053263",
                    "name": "Nelson Kotowski"
                },
                {
                    "authorId": "144674433",
                    "name": "Alexandre A. B. Lima"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "1763201",
                    "name": "M. Mattoso"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "5d8b55ecad043bdc54d66819dff8dacd837000e5",
            "title": "Reconciliation in the APPA P2P system",
            "abstract": "In peer-to-peer (P2P) systems, there has been little work on managing data replication in the presence of updates. However, important P2P applications that involve collaboration require multi-master replication support. In this paper, we adopt optimistic replication and propose a novel distributed algorithm to reconcile conflicting updates in the context of APPA (Atlas peer-to-peer architecture) data management system. APPA has a network-independent architecture that can be implemented over various P2P networks. Our algorithm for distributed semantic reconciliation (DSR) enables multi-master replication and assures eventual consistency among replicas. We validated the DSR algorithm through implementation and simulation using Java and SimJava. The experimental results show that DSR has good performance and scale up",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34609996",
                    "name": "Vidal Martins"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "cb471bc9419a7a7200a2cdb1dea1d99ecaa1d466",
            "title": "An Efficient Mechanism for Processing Top-k Queries in DHTs",
            "abstract": "We consider the problem of top-k query processing in Distributed Hash Tables (DHTs). The most efficient approaches for top-k query processing in centralized and distributed systems are based on the Threshold Algorithm (TA) which is applicable for queries where the scoring function is monotone. However, the specific interface of DHTs, i.e. data storage and retrieval based on keys, makes it hard to develop TA-style top-k query processing algorithms. In this paper, we propose an efficient mechanism for top-k query processing in DHTs. It is widely applicable to many different DHT implementations. Although our algorithm is TA-style, it is much more general since it supports a large set of non monotone scoring functions including linear functions. In fact, it is the first TA-style algorithm that supports linear scoring functions. We prove analytically the correctness of our algorithm. We have validated our algorithm through a combination of implementation and simulation. The results show very good performance, in terms of communication cost and response time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "e8512c444f65c3a29f94bfe1ee7384fbe2cdac4d",
            "title": "Design and Implementation of Atlas P2P Architecture",
            "abstract": "Peer-to-peer (P2P) computing offers new opportunities for building highly distributed data systems. Unlike client-server computing, P2P is a very dynamic environment where peers can join and leave the network at any time and offers important advantages such as operation without central coordination, peers autonomy, and scale up to large number of peers. However, providing high-level data management services (schema, queries, replication, availability, etc.) in a P2P system implies revisiting distributed database technology in major ways. In this chapter, we discuss the design and implementation of high-level data management services in APPA (Atlas Peer-to-Peer Architecture). APPA has a network-independent architecture that can be implemented over various structured and super-peer P2P networks. It uses novel solutions for persistent data management with updates, data replication with semantic-based reconciliation and query processing. APPA's services are implemented using the JXTA framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "34609996",
                    "name": "Vidal Martins"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "fed0e56561d3f2607e537b3681d10eac6b42d939",
            "title": "Survey of data replication in P2P systems",
            "abstract": "Large-scale distributed collaborative applications are getting common as a result of rapid progress in distributed technologies (grid, peer-to-peer, and mobile computing). Peer-to-peer (P2P) systems are particularly interesting for collaborative applications as they can scale without the need for powerful servers. In P2P systems, data storage and processing are distributed across autonomous peers, which can join and leave the network at any time. To provide high data availability in spite of such dynamic behavior, P2P systems rely on data replication. Some replication approaches assume static, read-only data (e.g. music files). Other solutions deal with updates, but they simplify replica management by assuming no update conflicts or single-master replication (i.e. only one copy of the replicated data accepts write operations). P2P advanced applications, which must deal with semantically rich data (e.g. XML documents, relational tables, etc.) using a high-level SQL-like query language, are likely to need more sophisticated capabilities such as multi-master replication (i.e. all replicas accept write operations) and update conflict resolution. These issues are addressed by optimistic replication. Optimistic replication allows asynchronous updating of replicas so that applications can progress even though some nodes are disconnected or have failed. As a result, users can collaborate asynchronously. However, concurrent updates may cause replica divergence and conflicts, which should be reconciled. In this survey, we present an overview of data replication, focusing on the optimistic approach that provides good properties for dynamic environments. We also introduce P2P systems and the replication solutions they implement. In particular, we show that current P2P systems do not provide eventual consistency among replicas in the presence of updates, apart from APPA system, a P2P data management system that we are building.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34609996",
                    "name": "Vidal Martins"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "1301b1c04422f185f8509dca5a67166b4fe6837a",
            "title": "Physical and virtual partitioning in OLAP database clusters",
            "abstract": "On-line analytical processing (OLAP) applications require high performance database support to achieve good response time (crucial for decision making). Database clusters provide a cost-effective alternative to parallel database systems. For OLAP applications, that typically use heavy weight queries, intra-query parallelism yields better performance as it reduces the execution time of individual queries. Intra-query parallelism is based on processing the same query on different subsets of the query table. Combining physical and virtual partitioning to define table subsets provides flexibility in intra-query parallelism while optimizing disk space usage and data availability. Experiments with our partitioning technique using TPC-H benchmark queries on a 32-dual node cluster gave linear and super-linear speedup, thereby reducing significantly the time of typical OLAP heavy weight queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31776208",
                    "name": "Camille Furtado"
                },
                {
                    "authorId": "144674433",
                    "name": "Alexandre A. B. Lima"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1763201",
                    "name": "M. Mattoso"
                }
            ]
        },
        {
            "paperId": "2de97af49b144f097fc62d5505c251dada2e42ad",
            "title": "Optimistic-Preventive Replication in a Database Cluster",
            "abstract": "In a database cluster, preventive replication can provide strong consistency without the limitations of synchronous replication. However, the original proposal [11] and its extension to partial replication [3] have performance limitations: to prevent conflicts, transactions are forced to wait a delay time before executing. In this paper, we address this problem in order to scale up to large cluster configurations. Our first contribution is an Optimistic Preventive refreshment algorithm (OptPrev) that reduces delays optimistically and prevents inconsistencies for partially-replicated databases. Our second contribution is an optimization that improves transaction throughput. We describe the implementation of OptPrev in our RepDB* prototype over a cluster of 64 nodes running PostgreSQL. Our experimental results using the TPC-C Benchmark show that our algorithm has excellent scale up and speed up.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2074823821",
                    "name": "C\u00e9dric Coulon"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "ef41676ab38bf4f27a10744a65a23e966ddc6916",
            "title": "Consistency management for partial replication in a high performance database cluster",
            "abstract": "In a database cluster, preventive replication can provide strong consistency without the limitations of synchronous replication. However, the original proposal (E. Pacitti et al., 2003) assumes full replication and has performance limitations. In this paper, we address these two limitations in order to scale up to large cluster configurations. Our first contribution is a refreshment algorithm that reduces the delay introduced by the algorithm and prevents inconsistencies for partially replicated databases. Our second contribution is an optimization that improves transaction throughput. We describe the implementation of our algorithm in our RepDB* prototype over a cluster of 64 nodes running PostgreSQL. Our experimental results using the TPC-C benchmark show that our algorithm has excellent scale up and speed up.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2074823821",
                    "name": "C\u00e9dric Coulon"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "d94e511a956d0242a2dc7a6fb2a6b3b22346b9a2",
            "title": "Replication and query processing in the appa data management system",
            "abstract": "Advanced P2P applications are likely to need general replication capabilities such as variable granularity and multi-master mode. However, existing replication solutions do not address important properties of P2P systems such as self-organization. In this paper, we address replication and query processing in the context of the APPA (Atlas Peer-to-Peer Architecture) data management system. APPA has a network-independent architecture that can be implemented over various P2P networks. It supports decentralized schema management and uses XML for the shared data. We propose an optimistic replication solution which provides eventual consistency for multi-master mode. We also propose a schema-based query processing strategy that deals with replication.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "34609996",
                    "name": "Vidal Martins"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "9e115782dd2455ef556c11317c5e08c0c3a83c67",
            "title": "Fast Algorithms for Maintaining Replica Consistency in Lazy Master Replicated Databases",
            "abstract": "In a lazy master replicated database, a transaction can commit after updating one replica copy (primary copy) at some master node. After the transaction commits, the updates are propagated towards the other replicas (secondary copies), which are updated in separate refresh transactions. A central problem is the design of algorithms that maintain replica's consistency while at the same time minimizing the performance degradation due to the synchronization of refresh transactions. In this paper, we propose a simple and general refreshment algorithm that solves this problem and we prove its correctness. The principle of the algorithm is to let refresh transactions wait for a certain \u00abdeliver time\u00bb before being executed at a node having secondary copies. We then present two main optimizations to this algorithm. One is based on specific properties of the topology of replica distribution across nodes. In particular, we characterize the nodes for which the deliver time can be null. The other improves the refreshment algorithm by using an immediate update propagation strategy. Our performance evaluation demonstrate the effectiveness of this optimization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "1684475",
                    "name": "P. Minet"
                },
                {
                    "authorId": "144463763",
                    "name": "E. Simon"
                }
            ]
        },
        {
            "paperId": "87f495521cd9a50b2d96f2f46ce36e862c6be4d3",
            "title": "Replicated Databases: concepts, Architectures and Techniques",
            "abstract": "Parce qu'elles ameliorent la disponibilite des donnees et les performances des requetes, les bases de donnees repliquees sont devenues la solution majeure a la gestion des donnees distribuees. Dans cet article, nous presentons les concepts, les architectures et les techniques de la replication. Nous les definissons dans le contexte des bases de donnees et des transactions reparties. Nous insistons sur les strategies de propagation des mises a jour qui doivent parvenir a un bon compromis entre la fraicheur des donnees et le cout de la replication. Nous discutons aussi des protocoles de diffusion mis en oeuvre pour fournir la tolerance aux pannes. Nous illustrons les techniques de replication en comparant les diverses solutions utilisees dans les systemes existants. Finalement, nous presentons les problemes ouverts qui concernent les entrepots de donnees, la mobilite et les systemes distribues a grande echelle.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "a24f608d4f7a96dc0d7dbbb5d6a506210ee3d589",
            "title": "Improving Data Freshness in Replicated Databases",
            "abstract": "Data replication is often used in distributed database applications to improve data availability and performance. Replicated data must be periodicall- y refreshed using update propagation strategies. Most of the current strategie- s guarantee mutual consistency of replicated data but are inefficient. Lazy (or asynchronous) replication is an alternative, more efficient solution where mutual consistency is relaxed. It is needed in applications such as on-line financial transactions and telecommunication systems which require high freshness of replicated data. In this case, the concept of \\it freshness",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                }
            ]
        },
        {
            "paperId": "a6d3b401dff701e88680bd0118812c116b693440",
            "title": "Improving data freshness in lazy master schemes",
            "abstract": "Many distributed database applications need to replicate data to improve data availability and query response time. The two phase commit protocol guarantees mutual consistency of replicated data but does not provide good performance. Lazy replication has been used as an alternative solution in several types of applications such as online financial transactions and telecommunication systems. In this case, mutual consistency is relaxed and the concept of freshness is used to measure the deviation between replica copies. We propose two update propagation strategies that improve freshness. Both of them use immediate propagation: updates to a primary copy are propagated towards a slave node as soon as they are detected at the master node without waiting for the commitment of the update transaction. We study the behavior of our strategies and show that they improve data freshness with respect to the deferred approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144463763",
                    "name": "E. Simon"
                },
                {
                    "authorId": "32072453",
                    "name": "R. Melo"
                }
            ]
        },
        {
            "paperId": "3914f65d89326bd81a42f0f4ea395c88ffe54cbc",
            "title": "Update Propagation Strategies to Improve Freshness of Data in Lazy Master Schemes",
            "abstract": "Many distributed database applications need to replicate data to improve data availability and query response time. The two-phase-commit protocol guarantees mutual consistency of replicated data but does not provide good performance. Lazy replication has been used as an alternative solution. In this case, mutual consistency is relaxed and the concept of freshness is used to measure the deviation between replica copies. In this paper we present a framework for lazy replication and focus on a special replication scheme called lazy master. In this scheme the common update propagation strategy used is deferred update propagation and works as follows: changes on a primary copy are first commited at the master node, afterwards the secondary copy is updated in a separate transaction at the slave node. We propose strategies based on what we call immediate update propagation. With immediate update propagation, updates to a primary copy are propagated towards a secondary copy as soon as they occur at the master node without waiting for the commitment of the update transaction. We study the behavior of these strategies and show that immediate update propagation may improve freshness with respect to the deferred approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "144463763",
                    "name": "E. Simon"
                }
            ]
        }
    ]
}