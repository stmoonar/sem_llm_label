{
    "authorId": "40466858",
    "papers": [
        {
            "paperId": "0413a92b91e2dbcbfaa042d0519f3d919bfe57e7",
            "title": "Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology",
            "abstract": "Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially in structuring patient information from longitudinal medical records.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2112677245",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "7016395",
                    "name": "C. Moung"
                },
                {
                    "authorId": "2228760409",
                    "name": "Jacob Abel"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "6150927",
                    "name": "R. Weerasinghe"
                },
                {
                    "authorId": "35695043",
                    "name": "B. Piening"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "46392172",
                    "name": "C. Bifulco"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "34e1a8a75bf6f35084ac6d714a136f39d02c649e",
            "title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
            "abstract": "Extracting patient information from unstructured text is a critical task in health decision-support and clinical research. Large language models (LLMs) have shown the potential to accelerate clinical curation via few-shot in-context learning, in contrast to supervised learning which requires much more costly human annotations. However, despite drastic advances in modern LLMs such as GPT-4, they still struggle with issues regarding accuracy and interpretability, especially in mission-critical domains such as health. Here, we explore a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs. This is made possible by the asymmetry between verification and generation, where the latter is often much easier than the former. Experimental results show that our method consistently improves accuracy for various LLMs in standard clinical information extraction tasks. Additionally, self-verification yields interpretations in the form of a short text span corresponding to each output, which makes it very efficient for human experts to audit the results, paving the way towards trustworthy extraction of clinical information in resource-constrained scenarios. To facilitate future research in this direction, we release our code and prompts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1395101702",
                    "name": "Zelalem Gero"
                },
                {
                    "authorId": "145229121",
                    "name": "Chandan Singh"
                },
                {
                    "authorId": "47413820",
                    "name": "Hao Cheng"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1947267",
                    "name": "Michel Galley"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "44caf26949b4799e21f7b0754fe61315e8b71542",
            "title": "Compositional Zero-Shot Domain Transfer with Text-to-Text Models",
            "abstract": "Abstract Label scarcity is a bottleneck for improving task performance in specialized domains. We propose a novel compositional transfer learning framework (DoT51) for zero-shot domain transfer. Without access to in-domain labels, DoT5 jointly learns domain knowledge (from masked language modelling of unlabelled in-domain free text) and task knowledge (from task training on more readily available general-domain data) in a multi-task manner. To improve the transferability of task training, we design a strategy named NLGU: We simultaneously train natural language generation (NLG) for in-domain label-to-data generation, which enables data augmentation for self-finetuning and natural language understanding (NLU) for label prediction. We evaluate DoT5 on the biomedical domain and the resource-lean subdomain of radiology, focusing on natural language inference, text summarization, and embedding learning. DoT5 demonstrates the effectiveness of compositional transfer learning through multi-task learning. In particular, DoT5 outperforms the current state-of-the-art in zero-shot transfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with ablations and a case study demonstrating its ability to solve challenging NLI examples requiring in-domain expertise.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144097210",
                    "name": "Fangyu Liu"
                },
                {
                    "authorId": "50383712",
                    "name": "Qianchu Liu"
                },
                {
                    "authorId": "82303325",
                    "name": "Shruthi Bannur"
                },
                {
                    "authorId": "1398728014",
                    "name": "Fernando P\u00e9rez-Garc\u00eda"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "2064953709",
                    "name": "A. Nori"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                },
                {
                    "authorId": "2029689505",
                    "name": "Javier Alvarez-Valle"
                },
                {
                    "authorId": "2941969",
                    "name": "O. Oktay"
                },
                {
                    "authorId": "34918424",
                    "name": "Stephanie L. Hyland"
                }
            ]
        },
        {
            "paperId": "5814bd146b37e13115af4330caf3a751159a156f",
            "title": "BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs",
            "abstract": "Biomedical data is inherently multimodal, comprising physical measurements and natural language narratives. A generalist biomedical AI model needs to simultaneously process different modalities of data, including text and images. Therefore, training an effective generalist biomedical model requires high-quality multimodal data, such as parallel image-text pairs. Here, we present PMC-15M, a novel dataset that is two orders of magnitude larger than existing biomedical multimodal datasets such as MIMIC-CXR, and spans a diverse range of biomedical image types. PMC-15M contains 15 million biomedical image-text pairs collected from 4.4 million scientific articles. Based on PMC-15M, we have pretrained BiomedCLIP, a multimodal foundation model, with domain-specific adaptations tailored to biomedical vision-language processing. We conducted extensive experiments and ablation studies on standard biomedical imaging tasks from retrieval to classification to visual question-answering (VQA). BiomedCLIP achieved new state-of-the-art results in a wide range of standard datasets, substantially outperforming prior approaches. Intriguingly, by large-scale pretraining on diverse biomedical image types, BiomedCLIP even outperforms state-of-the-art radiology-specific models such as BioViL in radiology-specific tasks such as RSNA pneumonia detection. In summary, BiomedCLIP is a fully open-access foundation model that achieves state-of-the-art performance on various biomedical tasks, paving the way for transformative multimodal biomedical discovery and applications. We release our models at https://aka.ms/biomedclip to facilitate future research in multimodal biomedical AI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2143558044",
                    "name": "Yanbo Xu"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "6157638",
                    "name": "J. Bagga"
                },
                {
                    "authorId": "1846722967",
                    "name": "Robert Tinn"
                },
                {
                    "authorId": "2159543147",
                    "name": "Sam Preston"
                },
                {
                    "authorId": "35794791",
                    "name": "Rajesh N. Rao"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "48269128",
                    "name": "Naveen Valluri"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "4204731",
                    "name": "M. Lungren"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "5a104380f0b671e956e3f202ff592917b92eabf5",
            "title": "What are the Desired Characteristics of Calibration Sets? Identifying Correlates on Long Form Scientific Summarization",
            "abstract": "Summarization models often generate text that is poorly calibrated to quality metrics because they are trained to maximize the likelihood of a single reference (MLE). To address this, recent work has added a calibration step, which exposes a model to its own ranked outputs to improve relevance or, in a separate line of work, contrasts positive and negative sets to improve faithfulness. While effective, much of this work has focused on how to generate and optimize these sets. Less is known about why one setup is more effective than another. In this work, we uncover the underlying characteristics of effective sets. For each training instance, we form a large, diverse pool of candidates and systematically vary the subsets used for calibration fine-tuning. Each selection strategy targets distinct aspects of the sets, such as lexical diversity or the size of the gap between positive and negatives. On three diverse scientific long-form summarization datasets (spanning biomedical, clinical, and chemical domains), we find, among others, that faithfulness calibration is optimal when the negative sets are extractive and more likely to be generated, whereas for relevance calibration, the metric margin between candidates should be maximized and surprise\u2013the disagreement between model and metric defined candidate rankings\u2013minimized.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49496422",
                    "name": "Griffin Adams"
                },
                {
                    "authorId": "38697713",
                    "name": "Bichlien H. Nguyen"
                },
                {
                    "authorId": "2119127072",
                    "name": "Jake Smith"
                },
                {
                    "authorId": "2111056280",
                    "name": "Yingce Xia"
                },
                {
                    "authorId": "1889683",
                    "name": "Shufang Xie"
                },
                {
                    "authorId": "2259881446",
                    "name": "Anna Ostropolets"
                },
                {
                    "authorId": "48717082",
                    "name": "Budhaditya Deb"
                },
                {
                    "authorId": "2144037981",
                    "name": "Yuan Chen"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "2763493",
                    "name": "No\u00e9mie Elhadad"
                }
            ]
        },
        {
            "paperId": "690b43567b5b60d5edc708458dd142603d438b23",
            "title": "Prompt Discriminative Language Models for Domain Adaptation",
            "abstract": "Prompt tuning offers an efficient approach to domain adaptation for pretrained language models, which predominantly focus on masked language modeling or generative objectives. However, the potential of discriminative language models in biomedical tasks remains underexplored.To bridge this gap, we develop BioDLM, a method tailored for biomedical domain adaptation of discriminative language models that incorporates prompt-based continual pretraining and prompt tuning for downstream tasks. BioDLM aims to maximize the potential of discriminative language models in low-resource scenarios by reformulating these tasks as span-level corruption detection, thereby enhancing performance on domain-specific tasks and improving the efficiency of continual pertaining.In this way, BioDLM provides a data-efficient domain adaptation method for discriminative language models, effectively enhancing performance on discriminative tasks within the biomedical domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1515662094",
                    "name": "K. Lu"
                },
                {
                    "authorId": "2039635",
                    "name": "P. Potash"
                },
                {
                    "authorId": "29457904",
                    "name": "Xihui Lin"
                },
                {
                    "authorId": "2109026528",
                    "name": "Yuwen Sun"
                },
                {
                    "authorId": "2210751970",
                    "name": "Zihan Qian"
                },
                {
                    "authorId": "2112340945",
                    "name": "Zheng Yuan"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "2114620790",
                    "name": "Tianxi Cai"
                },
                {
                    "authorId": "2218113443",
                    "name": "Junwei Lu"
                }
            ]
        },
        {
            "paperId": "6c12769939dd75bd681d37ea17cce7e6a57f5c6e",
            "title": "Distilling Large Language Models for Biomedical Knowledge Extraction: A Case Study on Adverse Drug Events",
            "abstract": "Large language models (LLMs), such as GPT-4, have demonstrated remarkable capabilities across a wide range of tasks, including health applications. In this paper, we study how LLMs can be used to scale biomedical knowledge curation. We find that while LLMs already possess decent competency in structuring biomedical text, by distillation into a task-specific student model through self-supervised learning, substantial gains can be attained over out-of-box LLMs, with additional advantages such as cost, efficiency, and white-box model access. We conduct a case study on adverse drug event (ADE) extraction, which is an important area for improving care. On standard ADE extraction evaluation, a GPT-3.5 distilled PubMedBERT model attained comparable accuracy as supervised state-of-the-art models without using any labeled data. Despite being over 1,000 times smaller, the distilled model outperformed its teacher GPT-3.5 by over 6 absolute points in F1 and GPT-4 by over 5 absolute points. Ablation studies on distillation model choice (e.g., PubMedBERT vs BioGPT) and ADE extraction architecture shed light on best practice for biomedical knowledge extraction. Similar gains were attained by distillation for other standard biomedical knowledge extraction tasks such as gene-disease associations and protected health information, further illustrating the promise of this approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112677245",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "3310870",
                    "name": "Yonas G. Woldesenbet"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "2223542076",
                    "name": "Praneeth Sanapathi"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "48269128",
                    "name": "Naveen Valluri"
                },
                {
                    "authorId": "35009788",
                    "name": "Erika Strandberg"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "a893a42e32651a0d6d1fee327c285765216809ae",
            "title": "Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making",
            "abstract": "Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability. We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1) while PubMedBERT, the domain-specific model, contains valuable information for fine-tuning, it can overfit to minority classes when class imbalances exist. In contrast, mixed-domain models exhibit greater resistance to overfitting, suggesting potential improvements in domain-specific model robustness; (2) in-domain pre-training accelerates feature disambiguation during fine-tuning; and (3) feature spaces undergo significant sparsification during this process, enabling clinicians to identify common outlier modes among fine-tuned models as demonstrated in this paper. These findings showcase the utility of SUFO in enhancing trust and safety when using transformers in medicine, and we believe SUFO can aid practitioners in evaluating fine-tuned language models for other applications in medicine and in more critical domains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "121374896",
                    "name": "Aliyah R. Hsu"
                },
                {
                    "authorId": "3422842",
                    "name": "Yeshwanth Cherapanamjeri"
                },
                {
                    "authorId": "7811734",
                    "name": "Briton Park"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "4351517",
                    "name": "A. Odisho"
                },
                {
                    "authorId": "2116415778",
                    "name": "Bin Yu"
                }
            ]
        },
        {
            "paperId": "ed6669cb421ab7a753a2f22974cec1e92ae3e10c",
            "title": "Precision Health in the Age of Large Language Models",
            "abstract": "Medicine today is imprecise. Among the top 20 drugs in the U.S., up to 80% of patients are non-responders. The goal of precision health is to provide the right intervention for the right people at the right time. The key to realize this dream is to develop a data-driven, learning system that can instantly incorporate new health information to optimize care delivery and accelerate biomedical discovery. In reality, however, the health ecosystem is mired in overwhelming unstructured data and excruciating manual processing. For example, in cancer, standard of care often fails, and clinical trials are the last hope. Yet less than 3% of patients could find a matching trial, whereas 40% of trial failures simply stem from insufficient recruitment. Discovery is painfully slow as a new drug may take billions of dollars and over a decade to develop. In this tutorial, we will explore how large language models (LLMs) can serve as a universal structuring tool to democratize biomedical knowledge work and usher in an intelligence revolution in precision health. We first review background for precision health and give a broad overview of the AI revolution that culminated in the development of large language models, highlighting key technical innovations and prominent trends such as consolidation of AI methods across modalities. We then give an in-depth review of biomedical LLMs and precision health applications, with a particular focus on scaling real-world evidence generation and drug discovery. To conclude, we discuss key technical challenges (e.g., bias, hallucination, cost), societal ramifications (e.g., privacy, regulation), as well as exciting research frontiers such as prompt programming, knowledge distillation, multi-modal learning, causal discovery.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2227530999",
                    "name": "Javier Gonz\u00e1lez Hern\u00e1ndez"
                }
            ]
        },
        {
            "paperId": "f22d71c7ce9720ba1f717a4f1181488200e78198",
            "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day",
            "abstract": "Conversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text. Multimodal conversational AI has seen rapid progress by leveraging billions of image-text pairs from the public web, but such general-domain vision-language models still lack sophistication in understanding and conversing about biomedical images. In this paper, we propose a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to train a Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal conversational capability and can follow open-ended instruction to assist with inquiries about a biomedical image. On three standard biomedical visual question answering datasets, LLaVA-Med outperforms previous supervised state-of-the-art on certain metrics. To facilitate biomedical multimodal research, we will release our instruction-following data and the LLaVA-Med model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109737569",
                    "name": "Chunyuan Li"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "2143856368",
                    "name": "Haotian Liu"
                },
                {
                    "authorId": "120157163",
                    "name": "Jianwei Yang"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ]
        }
    ]
}