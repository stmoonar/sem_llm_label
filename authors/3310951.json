{
    "authorId": "3310951",
    "papers": [
        {
            "paperId": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
            "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
            "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114891202",
                    "name": "Marah Abdin"
                },
                {
                    "authorId": "2297768912",
                    "name": "Sam Ade Jacobs"
                },
                {
                    "authorId": "2942686",
                    "name": "A. A. Awan"
                },
                {
                    "authorId": "29956361",
                    "name": "J. Aneja"
                },
                {
                    "authorId": "113916198",
                    "name": "Ahmed Awadallah"
                },
                {
                    "authorId": "3032929",
                    "name": "H. Awadalla"
                },
                {
                    "authorId": "2297768888",
                    "name": "Nguyen Bach"
                },
                {
                    "authorId": "2297768420",
                    "name": "Amit Bahree"
                },
                {
                    "authorId": "2274106850",
                    "name": "Arash Bakhtiari"
                },
                {
                    "authorId": "145560551",
                    "name": "Harkirat Singh Behl"
                },
                {
                    "authorId": "102222453",
                    "name": "Alon Benhaim"
                },
                {
                    "authorId": "2297766346",
                    "name": "Misha Bilenko"
                },
                {
                    "authorId": "46278353",
                    "name": "Johan Bjorck"
                },
                {
                    "authorId": "121645690",
                    "name": "S\u00e9bastien Bubeck"
                },
                {
                    "authorId": "2297768425",
                    "name": "Martin Cai"
                },
                {
                    "authorId": "2157424631",
                    "name": "C. C. T. Mendes"
                },
                {
                    "authorId": "2264439430",
                    "name": "Weizhu Chen"
                },
                {
                    "authorId": "113810201",
                    "name": "Vishrav Chaudhary"
                },
                {
                    "authorId": "2297780966",
                    "name": "Parul Chopra"
                },
                {
                    "authorId": "50672277",
                    "name": "Allison Del Giorno"
                },
                {
                    "authorId": "2297768528",
                    "name": "Gustavo de Rosa"
                },
                {
                    "authorId": "2297768894",
                    "name": "Matthew Dixon"
                },
                {
                    "authorId": "2315830",
                    "name": "Ronen Eldan"
                },
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "2054713176",
                    "name": "Abhishek Goswami"
                },
                {
                    "authorId": "2281352409",
                    "name": "S. Gunasekar"
                },
                {
                    "authorId": "2297768377",
                    "name": "Emman Haider"
                },
                {
                    "authorId": "2266241191",
                    "name": "Junheng Hao"
                },
                {
                    "authorId": "2945519",
                    "name": "Russell J. Hewett"
                },
                {
                    "authorId": "2297778093",
                    "name": "Jamie Huynh"
                },
                {
                    "authorId": "51900416",
                    "name": "Mojan Javaheripi"
                },
                {
                    "authorId": "2268726222",
                    "name": "Xin Jin"
                },
                {
                    "authorId": "2160340819",
                    "name": "Piero Kauffmann"
                },
                {
                    "authorId": "1830939",
                    "name": "Nikos Karampatziakis"
                },
                {
                    "authorId": "2297803872",
                    "name": "Dongwoo Kim"
                },
                {
                    "authorId": "2152658577",
                    "name": "Young Jin Kim"
                },
                {
                    "authorId": "2297767306",
                    "name": "Mahoud Khademi"
                },
                {
                    "authorId": "2279749803",
                    "name": "Lev Kurilenko"
                },
                {
                    "authorId": "2297805471",
                    "name": "James R. Lee"
                },
                {
                    "authorId": "2239163839",
                    "name": "Yin Tat Lee"
                },
                {
                    "authorId": "2268435346",
                    "name": "Yuanzhi Li"
                },
                {
                    "authorId": "2269852520",
                    "name": "Chen Liang"
                },
                {
                    "authorId": "2268632175",
                    "name": "Weishung Liu"
                },
                {
                    "authorId": "2297769037",
                    "name": "Eric Lin"
                },
                {
                    "authorId": "2297828605",
                    "name": "Zeqi Lin"
                },
                {
                    "authorId": "46781068",
                    "name": "Piyush Madan"
                },
                {
                    "authorId": "2256988075",
                    "name": "Arindam Mitra"
                },
                {
                    "authorId": "2297767663",
                    "name": "Hardik Modi"
                },
                {
                    "authorId": "2274742737",
                    "name": "Anh Nguyen"
                },
                {
                    "authorId": "2172095",
                    "name": "Brandon Norick"
                },
                {
                    "authorId": "27419446",
                    "name": "Barun Patra"
                },
                {
                    "authorId": "1416388980",
                    "name": "D. Perez-Becker"
                },
                {
                    "authorId": "6625302",
                    "name": "Thomas Portet"
                },
                {
                    "authorId": "4099006",
                    "name": "Reid Pryzant"
                },
                {
                    "authorId": "2279740366",
                    "name": "Heyang Qin"
                },
                {
                    "authorId": "2297777494",
                    "name": "Marko Radmilac"
                },
                {
                    "authorId": "41016119",
                    "name": "Corby Rosset"
                },
                {
                    "authorId": "2298400461",
                    "name": "Sambudha Roy"
                },
                {
                    "authorId": "2347792",
                    "name": "Olli Saarikivi"
                },
                {
                    "authorId": "2282542305",
                    "name": "Amin Saied"
                },
                {
                    "authorId": "2297768516",
                    "name": "Adil Salim"
                },
                {
                    "authorId": "1413038175",
                    "name": "Michael Santacroce"
                },
                {
                    "authorId": "2297814416",
                    "name": "Shital Shah"
                },
                {
                    "authorId": "2284868563",
                    "name": "Ning Shang"
                },
                {
                    "authorId": "2266307341",
                    "name": "Hiteshi Sharma"
                },
                {
                    "authorId": "2291873212",
                    "name": "Xianmin Song"
                },
                {
                    "authorId": "2537545",
                    "name": "Olatunji Ruwase"
                },
                {
                    "authorId": "2153689937",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2274157852",
                    "name": "Rachel Ward"
                },
                {
                    "authorId": "2298020153",
                    "name": "Guanhua Wang"
                },
                {
                    "authorId": "2297768553",
                    "name": "Philipp Witte"
                },
                {
                    "authorId": "2226773110",
                    "name": "Michael Wyatt"
                },
                {
                    "authorId": "46747953",
                    "name": "Can Xu"
                },
                {
                    "authorId": "2257094139",
                    "name": "Jiahang Xu"
                },
                {
                    "authorId": "2297814868",
                    "name": "Sonali Yadav"
                },
                {
                    "authorId": "145338263",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "2291073936",
                    "name": "Ziyi Yang"
                },
                {
                    "authorId": "2287794511",
                    "name": "Donghan Yu"
                },
                {
                    "authorId": "2287118838",
                    "name": "Cheng-Yuan Zhang"
                },
                {
                    "authorId": "2297810599",
                    "name": "Cyril Zhang"
                },
                {
                    "authorId": "2297818019",
                    "name": "Jianwen Zhang"
                },
                {
                    "authorId": "2274195530",
                    "name": "L. Zhang"
                },
                {
                    "authorId": "2271712604",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "2298130748",
                    "name": "Yunan Zhang"
                },
                {
                    "authorId": "2297801063",
                    "name": "Xiren Zhou"
                }
            ]
        },
        {
            "paperId": "05405e73f1afcf9564a5f81de0db7570cfa3d792",
            "title": "LMGQS: A Large-scale Dataset for Query-focused Summarization",
            "abstract": "Query-focused summarization (QFS) aims to extract or generate a summary of an input document that directly answers or is relevant to a given query. The lack of large-scale datasets in the form of documents, queries, and summaries has hindered model development in this area. In contrast, multiple large-scale high-quality datasets for generic summarization exist. We hypothesize that there is a hidden query for each summary sentence in a generic summarization annotation, and we utilize a large-scale pretrained language model to recover it. In this way, we convert four generic summarization benchmarks into a new QFS benchmark dataset, LMGQS, which consists of over 1 million document-query-summary samples. We thoroughly investigate the properties of our proposed dataset and establish baselines with state-of-the-art summarization models. By fine-tuning a language model on LMGQS, we achieve state-of-the-art zero-shot and supervised performance on multiple existing QFS benchmarks, demonstrating the high quality and diversity of LMGQS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8233965",
                    "name": "Ruochen Xu"
                },
                {
                    "authorId": "2117074851",
                    "name": "Song Wang"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2146294891",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2110197273",
                    "name": "Yichong Xu"
                },
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                },
                {
                    "authorId": "48262024",
                    "name": "Michael Zeng"
                }
            ]
        },
        {
            "paperId": "381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55",
            "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
            "abstract": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "2110197273",
                    "name": "Yichong Xu"
                },
                {
                    "authorId": "2146294891",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "8233965",
                    "name": "Ruochen Xu"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                }
            ]
        },
        {
            "paperId": "62454a3694e2e52b8698458440612505a3f7404b",
            "title": "The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions",
            "abstract": "Recent progress in Large Language Models (LLMs) has produced models that exhibit remarkable performance across a variety of NLP tasks. However, it remains unclear whether the existing focus of NLP research accurately captures the genuine requirements of human users. This paper provides a comprehensive analysis of the divergence between current NLP research and the needs of real-world NLP applications via a large-scale collection of user-GPT conversations. We analyze a large-scale collection of real user queries to GPT. We compare these queries against existing NLP benchmark tasks and identify a significant gap between the tasks that users frequently request from LLMs and the tasks that are commonly studied in academic research. For example, we find that tasks such as ``design'' and ``planning'' are prevalent in user interactions but are largely neglected or different from traditional NLP benchmarks. We investigate these overlooked tasks, dissect the practical challenges they pose, and provide insights toward a roadmap to make LLMs better aligned with user needs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260339714",
                    "name": "Siru Ouyang"
                },
                {
                    "authorId": "2146294891",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2260822008",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1606040932",
                    "name": "Ming Zhong"
                },
                {
                    "authorId": "1381900594",
                    "name": "Yizhu Jiao"
                },
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "4099006",
                    "name": "Reid Pryzant"
                },
                {
                    "authorId": "2256797847",
                    "name": "Chenguang Zhu"
                },
                {
                    "authorId": "2181650518",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2259869648",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "8c75f0f2393ac08f1749e6177f31a0f8842dae0f",
            "title": "How Does In-Context Learning Help Prompt Tuning?",
            "abstract": "Fine-tuning large language models is becoming ever more impractical due to their rapidly-growing scale. This motivates the use of parameter-efficient adaptation methods such as prompt tuning (PT), which adds a small number of tunable embeddings to an otherwise frozen model, and in-context learning (ICL), in which demonstrations of the task are provided to the model in natural language without any additional training. Recently, (CITATION) propose \u201cinstruction prompt tuning\u201d (IPT), which combines PT with ICL by concatenating a natural language demonstration with learned prompt embeddings. While all of these methods have proven effective on different tasks, how they interact with each other remains unexplored. In this paper, we empirically study when and how in-context examples improve prompt tuning by measuring the effectiveness of ICL, PT, and IPT on five text generation tasks with multiple base language models. We observe that (1) IPT does not always outperform PT, and in fact requires the in-context demonstration to be semantically similar to the test input to yield improvements; (2) PT is unstable and exhibits high variance, but combining PT and ICL (into IPT) consistently reduces variance across all five tasks; and(3) prompts learned for a specific source task via PT exhibit positive transfer when paired with in-context examples of a different target task. Our results offer actionable insights on choosing a suitable parameter-efficient adaptation method for a given task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "23134878",
                    "name": "Simeng Sun"
                },
                {
                    "authorId": "2152802138",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                },
                {
                    "authorId": "2136562",
                    "name": "Mohit Iyyer"
                }
            ]
        },
        {
            "paperId": "9591e76f296afe9589a42e899266e1c6a355d9ba",
            "title": "InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT",
            "abstract": "While large models such as GPT-3 demonstrate exceptional performance in zeroshot and fewshot summarization tasks, their extensive serving and fine-tuning costs hinder their utilization in various applications. Conversely, previous studies have found that although automatic metrics tend to favor smaller fine-tuned models, the quality of the summaries they generate is inferior to that of larger models like GPT-3 when assessed by human evaluators. To address this issue, we propose InheritSumm, a versatile and compact summarization model derived from GPT-3.5 through distillation. InheritSumm not only exhibits comparable zeroshot and fewshot summarization capabilities to GPT-3.5 but is also sufficiently compact for fine-tuning purposes. Experimental results demonstrate that InheritSumm achieves similar or superior performance to GPT-3.5 in zeroshot and fewshot settings. Furthermore, it outperforms the previously established best small models in both prefix-tuning and full-data fine-tuning scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110197273",
                    "name": "Yichong Xu"
                },
                {
                    "authorId": "8233965",
                    "name": "Ruochen Xu"
                },
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2146294891",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                },
                {
                    "authorId": "48262024",
                    "name": "Michael Zeng"
                }
            ]
        },
        {
            "paperId": "bfef29e0b88a45b3b6d8018b37d6272d0f32efe2",
            "title": "In-Context Demonstration Selection with Cross Entropy Difference",
            "abstract": "Large language models (LLMs) can use in-context demonstrations to improve performance on zero-shot tasks. However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples. We present a cross-entropy difference (CED) method for selecting in-context demonstrations. Our method is based on the observation that the effectiveness of in-context demonstrations negatively correlates with the perplexity of the test example by a language model that was finetuned on that demonstration. We utilize parameter efficient finetuning to train small models on training data that are used for computing the cross-entropy difference between a test example and every candidate in-context demonstration. This metric is used to rank and select in-context demonstrations independently for each test input. We evaluate our method on a mix-domain dataset that combines 8 benchmarks, representing 4 text generation tasks, showing that CED for in-context demonstration selection can improve performance for a variety of LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "4099006",
                    "name": "Reid Pryzant"
                },
                {
                    "authorId": "8233965",
                    "name": "Ruochen Xu"
                },
                {
                    "authorId": "2146294891",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2110197273",
                    "name": "Yichong Xu"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                }
            ]
        },
        {
            "paperId": "c76dd4a70361c3afd2e19d046343e2dedd16ecc3",
            "title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
            "abstract": "Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language\"gradients\"that criticize the current prompt. The gradients are then\"propagated\"into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4099006",
                    "name": "Reid Pryzant"
                },
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "2800851",
                    "name": "Jerry Li"
                },
                {
                    "authorId": "2109308930",
                    "name": "Y. Lee"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                },
                {
                    "authorId": "48262024",
                    "name": "Michael Zeng"
                }
            ]
        },
        {
            "paperId": "e1414fc1e1a6752524a1807a29ee406e8d808849",
            "title": "Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models",
            "abstract": "Large language models (LLMs) can perform a wide range of tasks by following natural language instructions, without the necessity of task-specific fine-tuning. Unfortunately, the performance of LLMs is greatly influenced by the quality of these instructions, and manually writing effective instructions for each task is a laborious and subjective process. In this paper, we introduce Auto-Instruct, a novel method to automatically improve the quality of instructions provided to LLMs. Our method leverages the inherent generative ability of LLMs to produce diverse candidate instructions for a given task, and then ranks them using a scoring model trained on a variety of 575 existing NLP tasks. In experiments on 118 out-of-domain tasks, Auto-Instruct surpasses both human-written instructions and existing baselines of LLM-generated instructions. Furthermore, our method exhibits notable generalizability even with other LLMs that are not incorporated into its training process.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72871419",
                    "name": "Zhihan Zhang"
                },
                {
                    "authorId": "2146294891",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "38767143",
                    "name": "W. Yu"
                },
                {
                    "authorId": "2110197273",
                    "name": "Yichong Xu"
                },
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "2261382250",
                    "name": "Qingkai Zeng"
                },
                {
                    "authorId": "2260822008",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2256797847",
                    "name": "Chenguang Zhu"
                },
                {
                    "authorId": "2152153656",
                    "name": "Meng Jiang"
                }
            ]
        },
        {
            "paperId": "b2542a738b75ee9b7ce1a13d8b78f9095d212412",
            "title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators",
            "abstract": "Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GenRead), which first prompts a large language model to generate contextutal documents based on a given question, and then reads the generated documents to produce the final answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, resulting in the generated documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GenRead achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation. Our code and generated documents can be found at https://github.com/wyu97/GenRead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38767143",
                    "name": "W. Yu"
                },
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "2992833",
                    "name": "Shuohang Wang"
                },
                {
                    "authorId": "2110197273",
                    "name": "Yichong Xu"
                },
                {
                    "authorId": "67171225",
                    "name": "Mingxuan Ju"
                },
                {
                    "authorId": "3313909",
                    "name": "Soumya Sanyal"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                },
                {
                    "authorId": "48262024",
                    "name": "Michael Zeng"
                },
                {
                    "authorId": "2152153656",
                    "name": "Meng Jiang"
                }
            ]
        }
    ]
}