{
    "authorId": "2128158461",
    "papers": [
        {
            "paperId": "215e620a0f1f621ccc5ab1e507c185cb7a5f173b",
            "title": "Steering Prototypes with Prompt-tuning for Rehearsal-free Continual Learning",
            "abstract": "In the context of continual learning, prototypes\u2014as representative class embeddings\u2014offer advantages in memory conservation and the mitigation of catastrophic forgetting. However, challenges related to semantic drift and prototype interference persist. In this study, we introduce the Contrastive Prototypical Prompt (CPP) approach. Through task-specific prompt-tuning, underpinned by a contrastive learning objective, we effectively address both aforementioned challenges. Our evaluations on four challenging class-incremental benchmarks reveal that CPP achieves a significant 4% to 6% improvement over state-of-the-art methods. Importantly, CPP operates without a rehearsal buffer and narrows the performance divergence between continual and offline joint-learning, suggesting an innovative scheme for Transformer-based continual learning systems1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118394123",
                    "name": "Zhuowei Li"
                },
                {
                    "authorId": "48096253",
                    "name": "Long Zhao"
                },
                {
                    "authorId": "2128158461",
                    "name": "Zizhao Zhang"
                },
                {
                    "authorId": null,
                    "name": "Han Zhang"
                },
                {
                    "authorId": "96765020",
                    "name": "Diya Liu"
                },
                {
                    "authorId": "2115431213",
                    "name": "Ting Liu"
                },
                {
                    "authorId": "1711560",
                    "name": "Dimitris N. Metaxas"
                }
            ]
        },
        {
            "paperId": "5d00e8f305d20ad937938fa4db054a33186626f7",
            "title": "AutoGT: Automated Graph Transformer Architecture Search",
            "abstract": "Although Transformer architectures have been successfully applied to graph data with the advent of Graph Transformer, the current design of Graph Transformers still heavily relies on human labor and expertise knowledge to decide on proper neural architectures and suitable graph encoding strategies at each Transformer layer. In literature, there have been some works on the automated design of Transformers focusing on non-graph data such as texts and images without considering graph encoding strategies, which fail to handle the non-euclidean graph data. In this paper, we study the problem of automated graph Transformers, for the first time. However, solving these problems poses the following challenges: i) how can we design a unified search space for graph Transformer, and ii) how to deal with the coupling relations between Transformer architectures and the graph encodings of each Transformer layer. To address these challenges, we propose Automated Graph Transformer (AutoGT), a neural architecture search framework that can automatically discover the optimal graph Transformer architectures by joint optimization of Transformer architecture and graph encoding strategies. Specifically, we first propose a unified graph Transformer formulation that can represent most state-ofthe-art graph Transformer architectures. Based upon the unified formulation, we further design the graph Transformer search space that includes both candidate architectures and various graph encodings. To handle the coupling relations, we propose a novel encoding-aware performance estimation strategy by gradually training and splitting the supernets according to the correlations between graph encodings and architectures. The proposed strategy can provide a more consistent and fine-grained performance prediction when evaluating the jointly optimized graph encodings and architectures. Extensive experiments and ablation studies show that our proposed AutoGT gains sufficient improvement over state-of-the-art hand-crafted baselines on all datasets, demonstrating its effectiveness and wide applicability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2128158461",
                    "name": "Zizhao Zhang"
                },
                {
                    "authorId": "153316152",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "133761917",
                    "name": "Chaoyu Guan"
                },
                {
                    "authorId": "2116460208",
                    "name": "Ziwei Zhang"
                },
                {
                    "authorId": "144911687",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "8a6bb6e7647c24a1a68205ebbfa3556838a27c1d",
            "title": "Unifying Distribution Alignment as a Loss for Imbalanced Semi-supervised Learning",
            "abstract": "While remarkable progress has been made in imbalanced supervised learning, less attention has been given to the setting of imbalanced semi-supervised learning (SSL) where not only are few labeled data provided, but the underlying data distribution can be severely imbalanced. Recent work requires both complicated sampling strategies of pseudo-labeled unlabeled data and distribution alignment of the pseudo-label distribution to accommodate this imbalance. We present a novel approach that relies only on a form of a distribution alignment but no sampling strategy where rather than aligning the pseudo-labels during inference, we move the distribution alignment component into the respective cross entropy loss computations for both the supervised and unsupervised losses. This alignment compensates for both imbalance in the data and the eventual distributional shift present during evaluation. Altogether, this provides a unified strategy that offers both significantly reduced training requirements and improved performance across both low and richly labeled regimes and over varying degrees of imbalance. In experiments, we validate the efficacy of our method on SSL variants of CIFAR10-LT, CIFAR100-LT, and ImageNet-127. On ImageNet-127, our method shows 1.6% accuracy improvement over CReST with an 80% training time reduction and is competitive with other SOTA methods. Code is available at https://github.com/google-research/crest",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2212690",
                    "name": "Justin Lazarow"
                },
                {
                    "authorId": "1729571",
                    "name": "Kihyuk Sohn"
                },
                {
                    "authorId": "50521003",
                    "name": "Chen-Yu Lee"
                },
                {
                    "authorId": "2136342754",
                    "name": "Chun-Liang Li"
                },
                {
                    "authorId": "2128158461",
                    "name": "Zizhao Zhang"
                },
                {
                    "authorId": "1945962",
                    "name": "Tomas Pfister"
                }
            ]
        },
        {
            "paperId": "c1fd5f40fd8c2fa96ae81aded3846e7d59473edf",
            "title": "StraIT: Non-autoregressive Generation with Stratified Image Transformer",
            "abstract": "We propose Stratified Image Transformer(StraIT), a pure non-autoregressive(NAR) generative model that demonstrates superiority in high-quality image synthesis over existing autoregressive(AR) and diffusion models(DMs). In contrast to the under-exploitation of visual characteristics in existing vision tokenizer, we leverage the hierarchical nature of images to encode visual tokens into stratified levels with emergent properties. Through the proposed image stratification that obtains an interlinked token pair, we alleviate the modeling difficulty and lift the generative power of NAR models. Our experiments demonstrate that StraIT significantly improves NAR generation and out-performs existing DMs and AR methods while being order-of-magnitude faster, achieving FID scores of 3.96 at 256*256 resolution on ImageNet without leveraging any guidance in sampling or auxiliary image classifiers. When equipped with classifier-free guidance, our method achieves an FID of 3.36 and IS of 259.3. In addition, we illustrate the decoupled modeling process of StraIT generation, showing its compelling properties on applications including domain transfer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152230789",
                    "name": "Shengju Qian"
                },
                {
                    "authorId": "2914394",
                    "name": "Huiwen Chang"
                },
                {
                    "authorId": "2167749913",
                    "name": "Yuanzhen Li"
                },
                {
                    "authorId": "2128158461",
                    "name": "Zizhao Zhang"
                },
                {
                    "authorId": "1729056",
                    "name": "Jiaya Jia"
                },
                {
                    "authorId": null,
                    "name": "Han Zhang"
                }
            ]
        },
        {
            "paperId": "22274d28d90df56167d59f04d6f1106fe2f1c97a",
            "title": "DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning",
            "abstract": "Continual learning aims to enable a single model to learn a sequence of tasks without catastrophic forgetting. Top-performing methods usually require a rehearsal buffer to store past pristine examples for experience replay, which, however, limits their practical value due to privacy and memory constraints. In this work, we present a simple yet effective framework, DualPrompt, which learns a tiny set of parameters, called prompts, to properly instruct a pre-trained model to learn tasks arriving sequentially without buffering past examples. DualPrompt presents a novel approach to attach complementary prompts to the pre-trained backbone, and then formulates the objective as learning task-invariant and task-specific\"instructions\". With extensive experimental validation, DualPrompt consistently sets state-of-the-art performance under the challenging class-incremental setting. In particular, DualPrompt outperforms recent advanced continual learning methods with relatively large buffer sizes. We also introduce a more challenging benchmark, Split ImageNet-R, to help generalize rehearsal-free continual learning research. Source code is available at https://github.com/google-research/l2p.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135785111",
                    "name": "Zifeng Wang"
                },
                {
                    "authorId": "2128158461",
                    "name": "Zizhao Zhang"
                },
                {
                    "authorId": "27556211",
                    "name": "Sayna Ebrahimi"
                },
                {
                    "authorId": "2015991",
                    "name": "Ruoxi Sun"
                },
                {
                    "authorId": null,
                    "name": "Han Zhang"
                },
                {
                    "authorId": "50521003",
                    "name": "Chen-Yu Lee"
                },
                {
                    "authorId": "49448661",
                    "name": "Xiaoqi Ren"
                },
                {
                    "authorId": "3164760",
                    "name": "Guolong Su"
                },
                {
                    "authorId": "2066252171",
                    "name": "Vincent Perot"
                },
                {
                    "authorId": "153140737",
                    "name": "Jennifer G. Dy"
                },
                {
                    "authorId": "1945962",
                    "name": "Tomas Pfister"
                }
            ]
        },
        {
            "paperId": "36d3093ea753b2e5da2e5a5d18ffe37c25d24953",
            "title": "QueryForm: A Simple Zero-shot Form Entity Query Framework",
            "abstract": "Zero-shot transfer learning for document understanding is a crucial yet under-investigated scenario to help reduce the high cost involved in annotating document entities. We present a novel query-based framework, QueryForm, that extracts entity values from form-like documents in a zero-shot fashion. QueryForm contains a dual prompting mechanism that composes both the document schema and a specific entity type into a query, which is used to prompt a Transformer model to perform a single entity extraction task. Furthermore, we propose to leverage large-scale query-entity pairs generated from form-like webpages with weak HTML annotations to pre-train QueryForm. By unifying pre-training and fine-tuning into the same query-based framework, QueryForm enables models to learn from structured documents containing various entities and layouts, leading to better generalization to target document types without the need for target-specific training data. QueryForm sets new state-of-the-art average F1 score on both the XFUND (+4.6%~10.1%) and the Payment (+3.2%~9.5%) zero-shot benchmark, with a smaller model size and no additional image input.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135785111",
                    "name": "Zifeng Wang"
                },
                {
                    "authorId": "2128158461",
                    "name": "Zizhao Zhang"
                },
                {
                    "authorId": "39172707",
                    "name": "Jacob Devlin"
                },
                {
                    "authorId": "2115293180",
                    "name": "Chen-Yu Lee"
                },
                {
                    "authorId": "3164760",
                    "name": "Guolong Su"
                },
                {
                    "authorId": "40479003",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "153140737",
                    "name": "Jennifer G. Dy"
                },
                {
                    "authorId": "2066252171",
                    "name": "Vincent Perot"
                },
                {
                    "authorId": "1945962",
                    "name": "Tomas Pfister"
                }
            ]
        },
        {
            "paperId": "7ef3fce558f5972877dc818214cbc9852a4c1fe8",
            "title": "Learning Instance-Specific Adaptation for Cross-Domain Segmentation",
            "abstract": "We propose a test-time adaptation method for cross-domain image segmentation. Our method is simple: Given a new unseen instance at test time, we adapt a pre-trained model by conducting instance-specific BatchNorm (statistics) calibration. Our approach has two core components. First, we replace the manually designed BatchNorm calibration rule with a learnable module. Second, we leverage strong data augmentation to simulate random domain shifts for learning the calibration rule. In contrast to existing domain adaptation methods, our method does not require accessing the target domain data at training time or conducting computationally expensive test-time model training/optimization. Equipping our method with models trained by standard recipes achieves significant improvement, comparing favorably with several state-of-the-art domain generalization and one-shot unsupervised domain adaptation approaches. Combining our method with the domain generalization methods further improves performance, reaching a new state of the art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8299168",
                    "name": "Yuliang Zou"
                },
                {
                    "authorId": "2128158461",
                    "name": "Zizhao Zhang"
                },
                {
                    "authorId": "2136342754",
                    "name": "Chun-Liang Li"
                },
                {
                    "authorId": null,
                    "name": "Han Zhang"
                },
                {
                    "authorId": "1945962",
                    "name": "Tomas Pfister"
                },
                {
                    "authorId": "2238908904",
                    "name": "Jia-Bin Huang"
                }
            ]
        },
        {
            "paperId": "e3a1e3a961954cdbd075b8b686d2dc1e281f7a03",
            "title": "Exploit Customer Life-time Value with Memoryless Experiments",
            "abstract": "As a measure of the long-term contribution produced by customers in a service or product relationship, life-time value, or LTV, can more comprehensively find the optimal strategy for service delivery. However, it is challenging to accurately abstract the LTV scene, model it reasonably, and find the optimal solution. The current theories either cannot precisely express LTV because of the single modeling structure, or there is no efficient solution. We propose a general LTV modeling method, which solves the problem that customers' long-term contribution is difficult to quantify while existing methods, such as modeling the click-through rate, only pursue the short-term contribution. At the same time, we also propose a fast dynamic programming solution based on a mutated bisection method and the memoryless repeated experiments assumption. The model and method can be applied to different service scenarios, such as the recommendation system. Experiments on real-world datasets confirm the effectiveness of the proposed model and optimization method. In addition, this whole LTV structure was deployed at a large E-commerce mobile phone application, where it managed to select optimal push message sending time and achieved a 10\\% LTV improvement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2128158461",
                    "name": "Zizhao Zhang"
                },
                {
                    "authorId": "2109816684",
                    "name": "Yifei Zhao"
                },
                {
                    "authorId": "2013942",
                    "name": "Guangda Huzhang"
                }
            ]
        },
        {
            "paperId": "083ca4bd4d5b231a1d7a0715ec55cc57a0f44b13",
            "title": "Aggregating Nested Transformers",
            "abstract": "Although hierarchical structures are popular in recent vision transformers, they require sophisticated designs and massive datasets to work well. In this work, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical manner. We \ufb01nd that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simpli\ufb01ed architecture with minor code changes upon the original vision transformer and obtains improved performance compared to existing methods. Our empirical results show that the proposed method NesT converges faster and requires much less training data to achieve good generalization. For example, a NesT with 68M parameters trained on ImageNet for 100/300 epochs achieves 82 . 3% / 83 . 8% accuracy evaluated on 224 \u00d7 224 image size, outperforming previous methods with up to 57% parameter reduction. Training a NesT with 6M parameters from scratch on CIFAR10 achieves 96% accuracy using a single GPU, setting a new state of the art for vision transformers. Beyond image classi\ufb01cation, we extend the key idea to image generation and show NesT leads to a strong decoder that is 8 \u00d7 faster than previous transformer based generators. Furthermore, we also propose a novel method for visually interpreting the learned model. Source code is available https://github.com/google-research/nested-transformer .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2128158461",
                    "name": "Zizhao Zhang"
                },
                {
                    "authorId": "2119079641",
                    "name": "Han Zhang"
                },
                {
                    "authorId": "48096253",
                    "name": "Long Zhao"
                },
                {
                    "authorId": "2117180415",
                    "name": "Ting Chen"
                },
                {
                    "authorId": "1945962",
                    "name": "Tomas Pfister"
                }
            ]
        },
        {
            "paperId": "28b8b1166d99cc14c00e0cfc961c23c4f862dbc6",
            "title": "[[EQUATION]]-optimal Designs for Hierarchical Linear Models: an Equivalence Theorem and a Nature-inspired Meta-heuristic Algorithm",
            "abstract": "\n Hierarchical linear models are widely used in many research disciplines and estimation issues for such models are generally well addressed. Design issues are relatively much less discussed for hierarchical linear models but there is an increasing interest as these models grow in popularity. This paper discusses [[EQUATION]] -optimality for predicting individual parameters in such models and establishes an equivalence theorem for confirming the [[EQUATION]] -optimality of an approximate design. Because the criterion is non-differentiable and requires solving multiple nested optimization problems, it is much harder to find and study [[EQUATION]] -optimal designs analytically. We propose a nature-inspired meta-heuristic algorithm called competitive swarm optimizer (CSO) to generate [[EQUATION]] -optimal designs for linear mixed models with different means and covariance structures. We further demonstrate that CSO is flexible and generally effective for finding the widely used locally [[EQUATION]] -optimal designs for nonlinear models with multiple interacting factors and some of the random effects are correlated. Our numerical results for a few examples suggest that [[EQUATION]] and [[EQUATION]] -optimal designs may be equivalent and we establish that [[EQUATION]] and [[EQUATION]] -optimal designs for hierarchical linear models are equivalent when the models have only a random intercept only. The challenging mathematical question whether their equivalence applies more generally to other hierarchical models remains elusive.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2120099536",
                    "name": "Xin Liu"
                },
                {
                    "authorId": "1967848",
                    "name": "R. Yue"
                },
                {
                    "authorId": "2128158461",
                    "name": "Zizhao Zhang"
                },
                {
                    "authorId": "1750425",
                    "name": "W. Wong"
                }
            ]
        }
    ]
}