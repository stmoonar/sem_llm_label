{
    "authorId": "46867002",
    "papers": [
        {
            "paperId": "e7a4e7b349b70b1681978cd57a59a6e1684b219f",
            "title": "ScienceBenchmark: A Complex Real-World Benchmark for Evaluating Natural Language to SQL Systems",
            "abstract": "Natural Language to SQL systems (NL-to-SQL) have recently shown improved accuracy (exceeding 80%) for natural language to SQL query translation due to the emergence of transformer-based language models, and the popularity of the Spider benchmark. However, Spider mainly contains simple databases with few tables, columns, and entries, which do not reflect a realistic setting. Moreover, complex real-world databases with domain-specific content have little to no training data available in the form of NL/SQL-pairs leading to poor performance of existing NL-to-SQL systems.\n \n In this paper, we introduce\n ScienceBenchmark\n , a new complex NL-to-SQL benchmark for three real-world, highly domain-specific databases. For this new benchmark, SQL experts and domain experts created high-quality NL/SQL-pairs for each domain. To garner more data, we extended the small amount of human-generated data with synthetic data generated using GPT-3. We show that our benchmark is highly challenging, as the top performing systems on Spider achieve a very low performance on our benchmark. Thus, the challenge is many-fold: creating NL-to-SQL systems for highly complex domains with a small amount of hand-made training data augmented with synthetic data. To our knowledge,\n ScienceBenchmark\n is the first NL-to-SQL benchmark designed with complex real-world scientific databases, containing challenging training and test data carefully validated by domain experts.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46867002",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "145116511",
                    "name": "Jan Deriu"
                },
                {
                    "authorId": "2055401797",
                    "name": "George Katsogiannis-Meimarakis"
                },
                {
                    "authorId": "2072251887",
                    "name": "Catherine Kosten"
                },
                {
                    "authorId": "1680709",
                    "name": "Georgia Koutrika"
                },
                {
                    "authorId": "2113917675",
                    "name": "Kurt Stockinger"
                }
            ]
        },
        {
            "paperId": "b28aa2960c6caa10d9141d4e8cf5222bfbede764",
            "title": "A transfer learning CNN-LSTM network-based production progress prediction approach in IIoT-enabled manufacturing",
            "abstract": "In make-to-order manufacturing workshops, accurate prediction value of production progress (PP) is a significant reference index for dynamic optimisation of production process and on-time delivery of production orders. The implementation of big data and Industrial Internet of Things (IIoT) in manufacturing workshops makes it possible to obtain large amounts of production data which can affect PP. However, the particularities of massive historical order data are not fully excavated and the amount of target order data is insufficient to support the training of high-precision prediction model, which will result in bad training approximation and generalisation. To overcome these shortcomings, a PP prediction approach consisting of two models with transfer learning (TL) is proposed. TL can avoid the training of PP prediction model from scratch every time. Consequently, computational efficiency can be greatly improved. A convolutional neural network (CNN) model with TL is devised to excavate the comprehensive features from historical and current orders. Additionally, a long short-term memory network (LSTM) model with TL is constructed to \ufb01t the nonlinear relation of the features provided by CNN-TL model for PP prediction. In order to validate the performance of the proposed PP prediction approach, comparative experiments of eight algorithms are conducted in an IIoT-enabled manufacturing workshop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2162000290",
                    "name": "Changchun Liu"
                },
                {
                    "authorId": "48808850",
                    "name": "Haihua Zhu"
                },
                {
                    "authorId": "1881922",
                    "name": "D. Tang"
                },
                {
                    "authorId": "2067805055",
                    "name": "Qingwei Nie"
                },
                {
                    "authorId": "48830603",
                    "name": "Shipei Li"
                },
                {
                    "authorId": "46867002",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "2161964509",
                    "name": "Xuan Liu"
                }
            ]
        },
        {
            "paperId": "aebdd7aab992437ba66cd81eab9306cc38bb8357",
            "title": "What is Your Article Based On? Inferring Fine-grained Provenance",
            "abstract": "When evaluating an article and the claims it makes, a critical reader must be able to assess where the information presented comes from, and whether the various claims are mutually consistent and support the conclusion. This motivates the study of claim provenance, which seeks to trace and explain the origins of claims. In this paper, we introduce new techniques to model and reason about the provenance of multiple interacting claims, including how to capture fine-grained information about the context. Our solution hinges on first identifying the sentences that potentially contain important external information. We then develop a query generator with our novel rank-aware cross attention mechanism, which aims at generating metadata for the source article, based on the context and the signals collected from a search engine. This establishes relevant search queries, and it allows us to obtain source article candidates for each identified sentence and propose an ILP based algorithm to infer the best sources. We experiment with a newly created evaluation dataset, Politi-Prov, based on fact-checking articles from www.politifact.com; our experimental results show that our solution leads to a significant improvement over baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46867002",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "1804315",
                    "name": "Z. Ives"
                },
                {
                    "authorId": "144590225",
                    "name": "D. Roth"
                }
            ]
        },
        {
            "paperId": "c58269be7bb0b7c896e5d74bfdcfb023c6787cf6",
            "title": "Learning to Decompose and Organize Complex Tasks",
            "abstract": "People rely on digital task management tools, such as email or to-do apps, to manage their tasks. Some of these tasks are large and complex, leading to action paralysis and feelings of being overwhelmed on the part of the user. The micro-productivity literature has shown that such tasks could benefit from being decomposed and organized, in order to reduce user cognitive load. Thus in this paper, we propose a novel end-to-end pipeline that consumes a complex task and induces a dependency graph from unstructured text to represent sub-tasks and their relationships. Our solution first finds nodes for sub-tasks from multiple \u2018how-to\u2019 articles on the web by injecting a neural text generator with three key desiderata \u2013 relevance, abstraction, and consensus. Then we resolve and infer edges between these subtask nodes by learning task dependency relations. We collect a new dataset of complex tasks with their sub-task graph to develop and evaluate our solutions. Both components of our graph induction solution are evaluated in experiments, demonstrating that our models outperform a state-of-the-art text generator significantly. Our generalizable and scalable end-to-end solution has important implications for boosting user productivity and assisting with digital task management.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46867002",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "3001990",
                    "name": "S. Jauhar"
                },
                {
                    "authorId": "1755651",
                    "name": "Julia Kiseleva"
                },
                {
                    "authorId": "34286525",
                    "name": "Ryen W. White"
                },
                {
                    "authorId": "144590225",
                    "name": "D. Roth"
                }
            ]
        },
        {
            "paperId": "85b49a68f2471022c2e17c7c57637c1b34b8f163",
            "title": "\u201cWho said it, and Why?\u201d Provenance for Natural Language Claims",
            "abstract": "In an era where generating content and publishing it is so easy, we are bombarded with information and are exposed to all kinds of claims, some of which do not always rank high on the truth scale. This paper suggests that the key to a longer-term, holistic, and systematic approach to navigating this information pollution is capturing the provenance of claims. To do that, we develop a formal definition of provenance graph for a given natural language claim, aiming to understand where the claim may come from and how it has evolved. To construct the graph, we model provenance inference, formulated mainly as an information extraction task and addressed via a textual entailment model. We evaluate our approach using two benchmark datasets, showing initial success in capturing the notion of provenance and its effectiveness on the application of claim verification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46867002",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "1804315",
                    "name": "Z. Ives"
                },
                {
                    "authorId": "144590225",
                    "name": "D. Roth"
                }
            ]
        },
        {
            "paperId": "ed573b7e477f7e31c36b36a429e02e55ebbff671",
            "title": "Research on Analyse for Shifting Process of Automatic Transmission",
            "abstract": "The author establish the dynamic model of vehicle power transmission, taking three gear to four gear as an example, Simulate the vehicle acceleration, Research the change of clutch control oil pressure and torque transmission. In order to improve the power performance and comfort performance of the whole vehicle in the shifting process, the control parameters of the shifting process are optimized to minimize the acceleration fluctuation of the whole vehicle in the shifting process",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46867002",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "31860927",
                    "name": "Tian Wang"
                },
                {
                    "authorId": "2883970",
                    "name": "Q. Ren"
                }
            ]
        },
        {
            "paperId": "f42c69dbd792155fee6f4d2c525971f8d43f138b",
            "title": "Finding Related Tables in Data Lakes for Interactive Data Science",
            "abstract": "Many modern data science applications build on data lakes, schema-agnostic repositories of data files and data products that offer limited organization and management capabilities. There is a need to build data lake search capabilities into data science environments, so scientists and analysts can find tables, schemas, workflows, and datasets useful to their task at hand. We develop search and management solutions for the Jupyter Notebook data science platform, to enable scientists to augment training data, find potential features to extract, clean data, and find joinable or linkable tables. Our core methods also generalize to other settings where computational tasks involve execution of programs or scripts.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46867002",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "1804315",
                    "name": "Z. Ives"
                }
            ]
        },
        {
            "paperId": "02acc874d8d6b45d7b697b7a33289bc28569344d",
            "title": "Dataset Relationship Management",
            "abstract": "The database community has largely focused on providing improved transaction management and query capabilities over records (and generalizations thereof). Yet such capabilities address only a small part of today\u2019s data science tasks, which are often much more focused on discovery, linking, comparative analysis, and collaboration across holistic datasets and data products. Data scientists frequently point to a strong need for data management \u2014 with respect to their many datasets and data products. We propose the development of the dataset relationship management system to support five main classes of operations on datasets: reuse of schema, data, curation, and work across many datasets; revelation of provenance, context, and assumptions; rapid revision of data and processing steps; system-assisted retargeting of computation to alternative execution environments; and metrics to reward individuals\u2019 contributions to the broader data ecosystem. We argue that the recent adoption of computational notebooks (particularly JupyterLab and Jupyter Notebook), as a unified interface over data tools, provides an ideal way of gathering detailed information about how data is being used, i.e., of transparently capturing dataset provenance and relationships, and thus such notebooks provide an attractive mechanism for integrating dataset relationship management into the data science ecosystem. We briefly outline our experiences in building towards JuNEAU, the first prototype DRMS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1804315",
                    "name": "Z. Ives"
                },
                {
                    "authorId": "46867002",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "67146495",
                    "name": "Soonbo Han"
                },
                {
                    "authorId": "2065869027",
                    "name": "Nan Zheng"
                }
            ]
        },
        {
            "paperId": "0e1b8f5f84f72c060a797d4ba59016a5e263df90",
            "title": "Evidence-based Trustworthiness",
            "abstract": "The information revolution brought with it information pollution. Information retrieval and extraction help us cope with abundant information from diverse sources. But some sources are of anonymous authorship, and some are of uncertain accuracy, so how can we determine what we should actually believe? Not all information sources are equally trustworthy, and simply accepting the majority view is often wrong. This paper develops a general framework for estimating the trustworthiness of information sources in an environment where multiple sources provide claims and supporting evidence, and each claim can potentially be produced by multiple sources. We consider two settings: one in which information sources directly assert claims, and a more realistic and challenging one, in which claims are inferred from evidence provided by sources, via (possibly noisy) NLP techniques. Our key contribution is to develop a family of probabilistic models that jointly estimate the trustworthiness of sources, and the credibility of claims they assert. This is done while accounting for the (possibly noisy) NLP needed to infer claims from evidence supplied by sources. We evaluate our framework on several datasets, showing strong results and significant improvement over baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46867002",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "1804315",
                    "name": "Z. Ives"
                },
                {
                    "authorId": "144590225",
                    "name": "D. Roth"
                }
            ]
        },
        {
            "paperId": "5c264da0a132ac08a043792c4f1d91c022c99767",
            "title": "Juneau: Data Lake Management for Jupyter",
            "abstract": "In collaborative settings such as multi-investigator laboratories, data scientists need improved tools to manage not their data records but rather their data sets and data products , to facilitate both provenance tracking and data (and code) reuse within their data lakes and \ufb01le systems. We demonstrate the Juneau System, which extends computational notebook software (Jupyter Notebook) as an instrumentation and data management point for overseeing and facilitating improved dataset usage, through capabilities for indexing, searching, and recommending \u201ccomplementary\u201d data sources, previously extracted machine learning features, and additional training data. This demonstration focuses on how we help the user \ufb01nd re-lated datasets via search",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46867002",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "1804315",
                    "name": "Z. Ives"
                }
            ]
        }
    ]
}