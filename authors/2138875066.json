{
    "authorId": "2138875066",
    "papers": [
        {
            "paperId": "52fcda1bfd02b92d946ef22f4dd001c61f7da039",
            "title": "A New Paradigm for Counterfactual Reasoning in Fairness and Recourse",
            "abstract": "Counterfactuals underpin numerous techniques for auditing and understanding artificial intelligence (AI) systems. The traditional paradigm for counterfactual reasoning in this literature is the interventional counterfactual, where hypothetical interventions are imagined and simulated. For this reason, the starting point for causal reasoning about legal protections and demographic data in AI is an imagined intervention on a legally-protected characteristic, such as ethnicity, race, gender, disability, age, etc. We ask, for example, what would have happened had your race been different? An inherent limitation of this paradigm is that some demographic interventions \u2014 like interventions on race \u2014 may not be well-defined or translate into the formalisms of interventional counterfactuals. In this work, we explore a new paradigm based instead on the backtracking counterfactual, where rather than imagine hypothetical interventions on legally-protected characteristics, we imagine alternate initial conditions while holding these characteristics fixed. We ask instead, what would explain a counterfactual outcome for you as you actually are or could be? This alternate framework allows us to address many of the same social concerns, but to do so while asking fundamentally different questions that do not rely on demographic interventions.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "692d0b221057485908c2524d6ebc96763daa7fd6",
            "title": "The Possibility of Fairness: Revisiting the Impossibility Theorem in Practice",
            "abstract": "The \u201cimpossibility theorem\u201d \u2014 which is considered foundational in algorithmic fairness literature \u2014 asserts that there must be trade-offs between common notions of fairness and performance when fitting statistical models, except in two special cases: when the prevalence of the outcome being predicted is equal across groups, or when a perfectly accurate predictor is used. However, theory does not always translate to practice. In this work, we challenge the implications of the impossibility theorem in practical settings. First, we show analytically that, by slightly relaxing the impossibility theorem (to accommodate a practitioner\u2019s perspective of fairness), it becomes possible to identify abundant sets of models that satisfy seemingly incompatible fairness constraints. Second, we demonstrate the existence of these models through extensive experiments on five real-world datasets. We conclude by offering tools and guidance for practitioners to understand when \u2014 and to what degree \u2014 fairness along multiple criteria can be achieved. This work has an important implication for the community: achieving fairness along multiple metrics for multiple groups (and their intersections) is much more possible than was previously believed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067721671",
                    "name": "A. Bell"
                },
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "2205540262",
                    "name": "Nazarii Drushchak"
                },
                {
                    "authorId": "2205547383",
                    "name": "Tetiana Herasymova"
                },
                {
                    "authorId": "2066297941",
                    "name": "Lucas Rosenblatt"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "6ae352e19261ec177dd8aed3e7fc432708402757",
            "title": "Causal Dependence Plots",
            "abstract": "Explaining artificial intelligence or machine learning models is increasingly important. To use such data-driven systems wisely we must understand how they interact with the world, including how they depend causally on data inputs. In this work we develop Causal Dependence Plots (CDPs) to visualize how one variable--an outcome--depends on changes in another variable--a predictor--$\\textit{along with any consequent causal changes in other predictor variables}$. Crucially, CDPs differ from standard methods based on holding other predictors constant or assuming they are independent. CDPs make use of an auxiliary causal model because causal conclusions require causal assumptions. With simulations and real data experiments, we show CDPs can be combined in a modular way with methods for causal learning or sensitivity analysis. Since people often think causally about input-output dependence, CDPs can be powerful tools in the xAI or interpretable machine learning toolkit and contribute to applications like scientific machine learning and algorithmic fairness.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "2210982171",
                    "name": "Sakina Hansen"
                }
            ]
        },
        {
            "paperId": "c4cea044af3ad99024290345e0287383e5708606",
            "title": "Causal Dependence Plots for Interpretable Machine Learning",
            "abstract": "Explaining arti\ufb01cial intelligence or machine learning models is an increasingly important problem. For humans to stay in the loop and control such systems, we must be able to understand how they interact with the world. This work proposes us-ing known or assumed causal structure in the input variables to produce simple and practical explanations of supervised learning models. Our explanations\u2014which we name Causal Dependence Plots or CDP\u2014visualize how the model output depends on changes in a given predictor along with any consequent causal changes in other predictors . Since this causal dependence captures how humans often think about input-output dependence, CDPs can be powerful tools in the explainable AI or interpretable ML toolkit and contribute to applications including scienti\ufb01c machine learning and algorithmic fairness. CDP can also be used for model-agnostic or black-box explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "2210982171",
                    "name": "Sakina Hansen"
                }
            ]
        },
        {
            "paperId": "7657eb913f77e3d95f2ebcd5f8e55d77f856064a",
            "title": "Counterfactuals for the Future",
            "abstract": "Counterfactuals are often described as 'retrospective,' focusing on hypothetical alternatives to a realized past. This description relates to an often implicit assumption about the structure and stability of exogenous variables in the system being modeled --- an assumption that is reasonable in many settings where counterfactuals are used. In this work, we consider cases where we might reasonably make a different assumption about exogenous variables; namely, that the exogenous noise terms of each unit do exhibit some unit-specific structure and/or stability. This leads us to a different use of counterfactuals --- a forward-looking rather than retrospective counterfactual. We introduce \"counterfactual treatment choice,\" a type of treatment choice problem that motivates using forward-looking counterfactuals. We then explore how mismatches between interventional versus forward-looking counterfactual approaches to treatment choice, consistent with different assumptions about exogenous noise, can lead to counterintuitive results.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "13b48e8acdfa98aedc83cb23832145e9b5352bbe",
            "title": "Disaggregated Interventions to Reduce Inequality",
            "abstract": "A significant body of research in the data sciences considers unfair discrimination against social categories such as race or gender that could occur or be amplified as a result of algorithmic decisions. Simultaneously, real-world disparities continue to exist, even before algorithmic decisions are made. In this work, we draw on insights from the social sciences brought into the realm of causal modeling and constrained optimization, and develop a novel algorithmic framework for tackling pre-existing real-world disparities. The purpose of our framework, which we call the \u201cimpact remediation framework,\u201d is to measure real-world disparities and discover the optimal intervention policies that could help improve equity or access to opportunity for those who are underserved with respect to an outcome of interest. We develop a disaggregated approach to tackling pre-existing disparities that relaxes the typical set of assumptions required for the use of social categories in structural causal models. Our approach flexibly incorporates counterfactuals and is compatible with various ontological assumptions about the nature of social categories. We demonstrate impact remediation with a hypothetical case study and compare our disaggregated approach to an existing state-of-the-art approach, comparing its structure and resulting policy recommendations. In contrast to most work on optimal policy learning, we explore disparity reduction itself as an objective, explicitly focusing the power of algorithms on reducing inequality.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "85517e6e34857cd923b6875090f7be9b26d17473",
            "title": "Impact Remediation: Optimal Interventions to Reduce Inequality",
            "abstract": "A significant body of research in the data sciences considers un-fair discrimination against social categories such as race or gender that could occur or be amplified as a result of algorithmic decisions. Simultaneously, real-world disparities continue to exist, even before algorithmic decisions are made. In this work, we draw on insights from the social sciences and humanistic studies brought into the realm of causal modeling and constrained optimization, and develop a novel algorithmic framework for tackling pre-existing real-world disparities. The purpose of our framework, which we call the \u201cimpact remediation framework,\u201d is to measure real-world disparities and discover the optimal intervention policies that could help improve equity or access to opportunity for those who are underserved with respect to an outcome of interest. We develop a disaggregated approach to tackling pre-existing disparities that relaxes the typical set of assumptions required for the use of social categories in structural causal models. Our approach flexibly incorporates counterfactuals and is compatible with various ontological assumptions about the nature of social categories. We demonstrate impact remediation with a real-world case study and compare our disaggregated approach to an existing state-of-the-art approach, comparing its structure and resulting policy recommendations. In contrast to most work on optimal policy learning, we explore disparity reduction itself as an objective, explicitly focusing the power of algorithms on reducing inequality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "26ba1497d908c90766c808a0521017313ea2a019",
            "title": "Rotational Equivariance for Object Classification Using xView",
            "abstract": "With the recent addition of large, curated and labeled data sets to the remote sensing discipline, deep learning models have largely surpassed the performance of classical techniques. These deep models, typically Convolutional Neural Networks, are invariant to translation through the use of successive convolution layers which are themselves equivariant to translation. Further, the combination of multiple convolution and pooling layers means that in practice, the model is also approximately invariant to translation. However, until recently these models could only approach rotational invariance through data augmentation. Here we propose using a new model formulation which achieves rotational equaivariance without data augmentation for overhead imagery classification. We utilize the popular xView data set to compare the rotational equivariance formalization against a regular CNN and CNN with rotational data augmentation for the task of image classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "2886830",
                    "name": "T. Doster"
                },
                {
                    "authorId": "47341214",
                    "name": "T. Emerson"
                },
                {
                    "authorId": "2051293392",
                    "name": "Henry Kvinze"
                }
            ]
        },
        {
            "paperId": "ad47269a06f505e42255b41a3edf227bdf678cda",
            "title": "Argumentative Topology: Finding Loop(holes) in Logic",
            "abstract": "Advances in natural language processing have resulted in increased capabilities with respect to multiple tasks. One of the possible causes of the observed performance gains is the introduction of increasingly sophisticated text representations. While many of the new word embedding techniques can be shown to capture particular notions of sentiment or associative structures, we explore the ability of two different word embeddings to uncover or capture the notion of logical shape in text. To this end we present a novel framework that we call Topological Word Embeddings which leverages mathematical techniques in dynamical system analysis and data driven shape extraction (i.e. topological data analysis). In this preliminary work we show that using a topological delay embedding we are able to capture and extract a different, shape-based notion of logic aimed at answering the question \"Can we find a circle in a circular argument?\"",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72542266",
                    "name": "Sarah Tymochko"
                },
                {
                    "authorId": "80048668",
                    "name": "Zach New"
                },
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "3632646",
                    "name": "Emilie Purvine"
                },
                {
                    "authorId": "2886830",
                    "name": "T. Doster"
                },
                {
                    "authorId": "2066496756",
                    "name": "Julien Chaput"
                },
                {
                    "authorId": "47341214",
                    "name": "T. Emerson"
                }
            ]
        }
    ]
}