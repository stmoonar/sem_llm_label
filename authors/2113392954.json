{
    "authorId": "2113392954",
    "papers": [
        {
            "paperId": "cb6bc6ae117c27d7cebd171fe32054d7e435f906",
            "title": "Federated Learning With Heterogeneous Quantization Bit Allocation and Aggregation for Internet of Things",
            "abstract": "Model quantization has drawn much attention for federated learning (FL) over the Internet of Things (IoT) since it is an effective way to address the critical bottleneck of communication efficiency. State-of-the-art studies have generally assumed homogeneous model quantization, where all clients\u2019 updates are quantized using the same number of bits and aggregated with the same weight at the server. However, in practical IoT scenarios, various IoT devices may apply heterogeneous quantization bits due to their different hardware capabilities, which leads to heterogeneous model quantization accuracy. This article addresses the problem of heterogeneous quantization bit allocation and aggregation for FL. The clients may be allocated with a different number of quantization bits, subject to a total quantization bit constraint. The server may employ different aggregation weights to each IoT device. In particular, we propose FedHBAA\u2014FL with Heterogeneous Bit Allocation and Aggregation, a novel joint quantization bit allocation and server aggregation algorithm. In FedHBAA, we first develop an optimal server aggregation scheme under any given bit allocation among clients. By minimizing the drift term in the convergence rate analysis, a closed-form aggregation weight solution as a function of the allocated bits is obtained for both the strongly convex and nonconvex loss functions. Then, by solving the derived optimal aggregation weights, the optimal bit allocation scheme is obtained. Numerical experiments demonstrate that FedHBAA outperforms the traditional FedAVG algorithm with equal quantization bits.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118528391",
                    "name": "Shengbo Chen"
                },
                {
                    "authorId": "2187085929",
                    "name": "Le Li"
                },
                {
                    "authorId": "34953270",
                    "name": "Guanghui Wang"
                },
                {
                    "authorId": "2189757940",
                    "name": "Meng Pang"
                },
                {
                    "authorId": "2113392954",
                    "name": "Cong Shen"
                }
            ]
        },
        {
            "paperId": "2667471283b408e63ac0544d17192e32ac3d5292",
            "title": "Performance Optimization for Noise Interference Privacy Protection in Federated Learning",
            "abstract": "The data security issue in federated learning is critical. While federated learning allows clients to collaboratively participate in the global model training without sharing private data, external eavesdroppers may intercept the model uploaded by the client to the server, revealing some sensitive information. Noise interference, i.e., adding noise to the client model before transmission, is an effective and efficient privacy-preserving method, but it degrades the learning performance of the system at the same time. In this paper, to address the challenge of system performance degradation caused by noise interference, we propose the FedNoise algorithm, which adopts two separate learning rates at the client and server respectively. By carefully tuning these learning rates, the global model can converge to the optimum. We provide theoretical proofs of the convergence of FedNoise for both strongly convex and non-convex loss functions and conduct simulations on real tasks. Numerical experimental results demonstrate that, under the same privacy protection level, FedNoise significantly outperforms the state-of-art scheme on datasets MNIST, Fashion-MNIST, and CIFAR10.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256369484",
                    "name": "Zihao Peng"
                },
                {
                    "authorId": "2256127185",
                    "name": "Boyuan Li"
                },
                {
                    "authorId": "2256977383",
                    "name": "Le Li"
                },
                {
                    "authorId": "2249341093",
                    "name": "Shengbo Chen"
                },
                {
                    "authorId": "2152580834",
                    "name": "Guanghui Wang"
                },
                {
                    "authorId": "2249033900",
                    "name": "Hong Rao"
                },
                {
                    "authorId": "2113392954",
                    "name": "Cong Shen"
                }
            ]
        },
        {
            "paperId": "2b2eb42ae21193d0e88c6e4d3982747154d89b51",
            "title": "Joint Client Assignment and UAV Route Planning for Indirect-Communication Federated Learning",
            "abstract": "Federated Learning (FL) is a machine learning approach that enables the creation of shared models for powerful applications while allowing data to remain on devices. This approach provides benefits such as improved data privacy, security, and reduced latency. However, in some systems, direct communication between clients and servers may not be possible, such as remote areas without proper communication infrastructure. To overcome this challenge, a new framework called FedEx (Federated Learning via Model Express Delivery) is proposed. This framework employs mobile transporters, such as UAVs, to establish indirect communication channels between the server and clients. These transporters act as intermediaries and allow for model information exchange. The use of indirect communication presents new challenges for convergence analysis and optimization, as the delay introduced by the transporters' movement creates issues for both global model dissemination and local model collection. To address this, two algorithms, FedEx-Sync and FedEx-Async, are proposed for synchronized and asynchronized learning at the transporter level. Additionally, a bi-level optimization algorithm is proposed to solve the joint client assignment and route planning problem. Experimental validation using two public datasets in a simulated network demonstrates consistent results with the theory, proving the efficacy of FedEx.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143957927",
                    "name": "Jieming Bian"
                },
                {
                    "authorId": "2113392954",
                    "name": "Cong Shen"
                },
                {
                    "authorId": "2111063052",
                    "name": "Jie Xu"
                }
            ]
        },
        {
            "paperId": "4bdb8888b106bd45a785d6a3afe6dac222fedb25",
            "title": "The 9th SIGKDD International Workshop on Mining and Learning from Time Series",
            "abstract": "Time series data has become pervasive across domains such as finance, transportation, retail, entertainment, and healthcare. This shift towards continuous monitoring and recording, fueled by advancements in sensing technologies, necessitates the development of new tools and solutions. Despite extensive study, the importance of time series analysis continues to increase. However, modern time series data present challenges to existing techniques, including irregular sampling and spatiotemporal structures. Time series mining research is both challenging and rewarding as it connects diverse disciplines and requires interdisciplinary solutions. The goals of this workshop are to (1) highlight the significant challenges that underpin learning and mining from time series data (e.g., irregular sampling, spatiotemporal structure, uncertainty quantification), (2) discuss recent algorithmic, theoretical, statistical, or systems-based developments for tackling these problems, and (3) to synergize the research activities and discuss both new and open problems in time series analysis and mining. In summary, our workshop will focus on both the theoretical and practical aspects of time series data analysis and will provide a platform for researchers and practitioners from academia and industry to discuss potential research directions and critical technical issues and present solutions to tackle related issues in practical applications. We will invite researchers and practitioners from the related areas of AI, machine learning, data science, statistics, and many others to contribute to this workshop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144735626",
                    "name": "S. Purushotham"
                },
                {
                    "authorId": "2451800",
                    "name": "Dongjin Song"
                },
                {
                    "authorId": "3308963",
                    "name": "Qingsong Wen"
                },
                {
                    "authorId": "2181310178",
                    "name": "Jun Huan"
                },
                {
                    "authorId": "2113392954",
                    "name": "Cong Shen"
                },
                {
                    "authorId": "2246897295",
                    "name": "Yuriy Nevmyvaka"
                }
            ]
        },
        {
            "paperId": "4ddda734ab3af90eef815a5304b200727de7dee6",
            "title": "Federated Learning via Indirect Server-Client Communications",
            "abstract": "Federated Learning (FL) is a communication-efficient and privacy-preserving distributed machine learning framework that has gained a significant amount of research attention recently. Despite the different forms of FL algorithms (e.g., synchronous FL, asynchronous FL) and the underlying optimization methods, nearly all existing works implicitly assumed the existence of a communication infrastructure that facilitates the direct communication between the server and the clients for the model data exchange. This assumption, however, does not hold in many real-world applications that can benefit from distributed learning but lack a proper communication infrastructure (e.g., smart sensing in remote areas). In this paper, we propose a novel FL framework, named FedEx (short for FL via Model Express Delivery), that utilizes mobile transporters (e.g., Unmanned Aerial Vehicles) to establish indirect communication channels between the server and the clients. Two algorithms, called FedEx-Sync and FedEx-Async, are developed depending on whether the transporters adopt a synchronized or an asynchronized schedule. Even though the indirect communications introduce heterogeneous delays to clients for both the global model dissemination and the local model collection, we prove the convergence of both versions of FedEx. The convergence analysis subsequently sheds lights on how to assign clients to different transporters and design the routes among the clients. The performance of FedEx is evaluated through experiments in a simulated network on two public datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143957927",
                    "name": "Jieming Bian"
                },
                {
                    "authorId": "2113392954",
                    "name": "Cong Shen"
                },
                {
                    "authorId": "2111063052",
                    "name": "Jie Xu"
                }
            ]
        },
        {
            "paperId": "7caef58f8641a7d1c8c13435534b0d47b6c6d109",
            "title": "On the Convergence of Hybrid Server-Clients Collaborative Training",
            "abstract": "Modern distributed machine learning (ML) paradigms, such as federated learning (FL), utilize data distributed at different clients to train a global model. In such paradigm, local datasets never leave the clients for better privacy protection, and the parameter server (PS) only performs simple aggregation. In practice, however, there is often some amount of data available at the PS, and its computation capability is strong enough to carry out more demanding tasks than simple model aggregation. The focus of this paper is to analyze the model convergence of a new hybrid learning architecture, which leverages the PS dataset and its computation power for collaborative model training with clients. Different from FL where stochastic gradient descent (SGD) is always computed in parallel across clients, the new architecture has both parallel SGD at clients and sequential SGD at PS. We analyze the convergence rate upper bounds of this aggregate-then-advance design for both strongly convex and non-convex loss functions. We show that when the local SGD has an $\\mathcal {O}(1/t)$ stepsize, the server SGD needs to scale its stepsize to no slower than $\\mathcal {O}(1/t^{2})$ in order to strictly outperform local SGD with strongly convex loss functions. The theoretical findings are corroborated by numerical experiments, where advantages in terms of both accuracy and convergence speed over clients-only (local SGD and FED AVG) and server-only training are demonstrated.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153440974",
                    "name": "Kun Yang"
                },
                {
                    "authorId": "2118528391",
                    "name": "Shengbo Chen"
                },
                {
                    "authorId": "2113392954",
                    "name": "Cong Shen"
                }
            ]
        },
        {
            "paperId": "93f15a82a0f1403c9b9145a1abdf70c84667f65d",
            "title": "Accelerating Hybrid Federated Learning Convergence Under Partial Participation",
            "abstract": "Over the past few years, Federated Learning (FL) has become a popular distributed machine learning paradigm. FL involves a group of clients with decentralized data who collaborate to learn a common model under the coordination of a centralized server, with the goal of protecting clients\u2019 privacy by ensuring that local datasets never leave the clients and that the server only performs model aggregation. However, in realistic scenarios, the server may be able to collect a small amount of data that approximately mimics the population distribution and has stronger computational ability to perform the learning process, resulting in the development of a hybrid FL framework. While previous hybrid FL work has shown that the alternative training of clients and server can increase convergence speed, it has focused on the scenario where clients fully participate and ignores the negative effect of partial participation. In this paper, we provide theoretical analysis of hybrid FL under clients\u2019 partial participation to validate that partial participation is the key constraint on the convergence speed. We then propose a new algorithm called FedCLG, which investigates the two-fold role of the server in hybrid FL. Firstly, the server needs to process the training steps using its small amount of local datasets. Secondly, the server's calculated gradient needs to guide the participating clients\u2019 training and the server's aggregation. We validate our theoretical findings through numerical experiments, which show that FedCLG outperforms state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143957927",
                    "name": "Jieming Bian"
                },
                {
                    "authorId": "2152511908",
                    "name": "Lei Wang"
                },
                {
                    "authorId": "2116715420",
                    "name": "Kun Yang"
                },
                {
                    "authorId": "2113392954",
                    "name": "Cong Shen"
                },
                {
                    "authorId": "2036814734",
                    "name": "Jie Xu"
                }
            ]
        },
        {
            "paperId": "bfa8ff46db120321dc953c0068cee8bdd72e11f0",
            "title": "Communication and Storage Efficient Federated Split Learning",
            "abstract": "Federated learning (FL) is a popular distributed machine learning (ML) paradigm, but is often limited by significant communication costs and edge device computation capabilities. Federated Split Learning (FSL) preserves the parallel model training principle of FL, with a reduced device computation requirement thanks to splitting the ML model between the server and clients. However, FSL still incurs very high communication overhead due to transmitting the smashed data and gradients between the clients and the server in each global round. Furthermore, the server has to maintain separate models for every client, resulting in a significant computation and storage requirement that grows linearly with the number of clients. This paper aims at solving these two issues by proposing a communication and storage efficient federated split learning (CSE-FSL) strategy, which utilizes an auxiliary network to locally update the client models while keeping only a single model at the server, hence avoiding the communication of gradients from the server and greatly reducing the server resource requirement. Communication cost is further reduced by only sending the smashed data in selected epochs from the clients. We provide a rigorous theoretical analysis of CSE-FSL that guarantees its convergence for non-convex loss functions. Extensive experimental results demonstrate that CSE-FSL has a significant communication reduction over existing FSL techniques, while achieving state-of-the-art convergence and model accuracy, using several real-world FL tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2152237255",
                    "name": "Yujia Mu"
                },
                {
                    "authorId": "2113392954",
                    "name": "Cong Shen"
                }
            ]
        },
        {
            "paperId": "261794ab608563e01b0809c0577af8875d53c111",
            "title": "Optimizing Federated Averaging over Fading Channels",
            "abstract": "Deep fading represents the typical error event when communicating over wireless channels. We show that deep fading is particularly detrimental for federated learning (FL) over wireless communications. In particular, the celebrated FEDAVG and several of its variants break down for FL tasks when deep fading exists in the communication phase. The main contribution of this paper is an optimal global model aggregation method at the parameter server, which allocates different weights to different clients based on not only their learning characteristics but also the instantaneous channel state information at the receiver (CSIR). This is accomplished by first deriving an upper bound on the parallel stochastic gradient descent (SGD) convergence over fading channels, and then solving an optimization problem for the server aggregation weights that minimizes this upper bound. The derived optimal aggregation solution is closed-form, and achieves the well-known O(1/t) convergence rate for strongly-convex loss functions under arbitrary fading and decaying learning rates. We validate our approach using several real-world FL tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152237255",
                    "name": "Yujia Mu"
                },
                {
                    "authorId": "2113392954",
                    "name": "Cong Shen"
                },
                {
                    "authorId": "143783379",
                    "name": "Yonina C. Eldar"
                }
            ]
        },
        {
            "paperId": "75d10a2ef3b2a5454532fcc11d6678ae90a90989",
            "title": "Joint Optimal Quantization and Aggregation of Federated Learning Scheme in VANETs",
            "abstract": "Vehicular ad hoc networks (VANETs) is one of the most promising approaches for the Intelligent Transportation Systems (ITS). With the rapid increase in the amount of traffic data, deep learning based algorithms have been used extensively in VANETs. The recently proposed federated learning is an attractive candidate for collaborative machine learning where instead of transferring a plethora of data to a centralized server, all clients train their respective local models and upload them to the server for model aggregation. Model quantization is an effective approach to address the communication efficiency issue in federated learning, and yet existing studies largely assume homogeneous quantization for all clients. However, in reality, clients are predominantly heterogeneous, where they support different quantization precision levels. In this work, we propose FedDO \u2013 Federated Learning with Double Optimization. Minimizing the drift term in the convergence analysis, which is a weighted sum of squared quantization errors (SQE) over all clients, leads to a double optimization at both clients and server sides. In particular, each client adopts a fully distributed, instantaneous (per learning round) and individualized (per client) quantization scheme that minimizes its own squared quantization error, and the server computes the aggregation weights that minimize the weighted sum of squared quantization errors over all clients. We show via numerical experiments that the minimal-SQE quantizer has a better performance than a widely adopted linear quantizer for federated learning. We also demonstrate the performance advantages of FedDO over the vanilla FedAvg with standard equal weights and linear quantization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187677401",
                    "name": "Yifei Li"
                },
                {
                    "authorId": "1623145468",
                    "name": "Yijia Guo"
                },
                {
                    "authorId": "2474250",
                    "name": "M. Alazab"
                },
                {
                    "authorId": "2118528391",
                    "name": "Shengbo Chen"
                },
                {
                    "authorId": "2113392954",
                    "name": "Cong Shen"
                },
                {
                    "authorId": "47841301",
                    "name": "K. Yu"
                }
            ]
        }
    ]
}