{
    "authorId": "51072831",
    "papers": [
        {
            "paperId": "27d0d2923a42bd2bced1b100844e232ff87368e3",
            "title": "SkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model",
            "abstract": "Skin and subcutaneous diseases rank high among the leading contributors to the global burden of nonfatal diseases, impacting a considerable portion of the population. Nonetheless, the field of dermatology diagnosis faces three significant hurdles. Firstly, there is a shortage of dermatologists accessible to diagnose patients, particularly in rural regions. Secondly, accurately interpreting skin disease images poses a considerable challenge. Lastly, generating patient-friendly diagnostic reports is usually a time-consuming and labor-intensive task for dermatologists. To tackle these challenges, we present SkinGPT-4, which is the world's first interactive dermatology diagnostic system powered by an advanced visual large language model. SkinGPT-4 leverages a fine-tuned version of MiniGPT-4, trained on an extensive collection of skin disease images (comprising 52,929 publicly available and proprietary images) along with clinical concepts and doctors' notes. We designed a two-step training process to allow SkinGPT to express medical features in skin disease images with natural language and make accurate diagnoses of the types of skin diseases. With SkinGPT-4, users could upload their own skin photos for diagnosis, and the system could autonomously evaluate the images, identifies the characteristics and categories of the skin conditions, performs in-depth analysis, and provides interactive treatment recommendations. Meanwhile, SkinGPT-4's local deployment capability and commitment to user privacy also render it an appealing choice for patients in search of a dependable and precise diagnosis of their skin ailments. To demonstrate the robustness of SkinGPT-4, we conducted quantitative evaluations on 150 real-life cases, which were independently reviewed by certified dermatologists, and showed that SkinGPT-4 could provide accurate diagnoses of skin diseases. Though SkinGPT-4 is not a substitute for doctors, it could enhance users' comprehension of their medical conditions, facilitate improve communication between patients and doctors, expedite the diagnostic process for dermatologists, and potentially promote human-centred care and healthcare equity in underdeveloped areas.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1502765940",
                    "name": "Juexiao Zhou"
                },
                {
                    "authorId": "3455313",
                    "name": "Xiao-Zhen He"
                },
                {
                    "authorId": "46732892",
                    "name": "Liyuan Sun"
                },
                {
                    "authorId": "2293560831",
                    "name": "Jiannan Xu"
                },
                {
                    "authorId": "46772896",
                    "name": "Xiuying Chen"
                },
                {
                    "authorId": "2166165505",
                    "name": "Yuetan Chu"
                },
                {
                    "authorId": "46696611",
                    "name": "Longxi Zhou"
                },
                {
                    "authorId": "51072831",
                    "name": "Xingyu Liao"
                },
                {
                    "authorId": "2119454424",
                    "name": "Bin Zhang"
                },
                {
                    "authorId": "2198273175",
                    "name": "Xin Gao"
                }
            ]
        },
        {
            "paperId": "9d6b5bd74f17b2768290189f6548312df83abb67",
            "title": "A unified method to revoke the private data of patients in intelligent healthcare with audit to forget",
            "abstract": "Revoking personal private data is one of the basic human rights, which has already been sheltered by several privacy-preserving laws in many countries. However, with the development of data science, machine learning and deep learning techniques, this right is usually neglected or violated as more and more patients\u2019 data are being collected and used for model training, especially in intelligent healthcare, thus making intelligent healthcare a sector where technology must meet the law, regulations, and privacy principles to ensure that the innovation is for the common good. In order to secure patients\u2019 right to be forgotten, we proposed a novel solution by using auditing to guide the forgetting process, where auditing means determining whether a dataset has been used to train the model and forgetting requires the information of a query dataset to be forgotten from the target model. We unified these two tasks by introducing a new approach called knowledge purification. To implement our solution, we developed AFS, a unified open-source software, which is able to evaluate and revoke patients\u2019 private data from pre-trained deep learning models. We demonstrated the generality of AFS by applying it to four tasks on different datasets with various data sizes and architectures of deep learning networks. The software is publicly available at https://github.com/JoshuaChou2018/AFS.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "1502765940",
                    "name": "Juexiao Zhou"
                },
                {
                    "authorId": "144911687",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "51072831",
                    "name": "Xingyu Liao"
                },
                {
                    "authorId": "2119454424",
                    "name": "Bin Zhang"
                },
                {
                    "authorId": null,
                    "name": "Wenjia He"
                },
                {
                    "authorId": "66545108",
                    "name": "Zhongxiao Li"
                },
                {
                    "authorId": "46696611",
                    "name": "Longxi Zhou"
                },
                {
                    "authorId": "2198273175",
                    "name": "Xin Gao"
                }
            ]
        },
        {
            "paperId": "611687a491a3c8072fbb8559b4032545501bf7ee",
            "title": "MultiNanopolish: refined grouping method for reducing redundant calculations in Nanopolish",
            "abstract": "MOTIVATION\nCompared with the second generation sequencing technologies, the third generation sequencing technologies allows us to obtain longer reads (average \u223c10kbps, maximum 900kbps), but brings a higher error rate (\u223c15% error rate). Nanopolish is a variant and methylation detection tool based on Hidden Markov Model (HMM), which uses Oxford Nanopore sequencing data for signal-level analysis. Nanopolish can greatly improve the accuracy of assembly, whereas it is limited by long running time since most executive parts of Nanopolish is a serial and computationally expensive process.\n\n\nRESULTS\nIn this paper, we present an effective polishing tool, Multithreading Nanopolish (MultiNanopolish), which decomposes the whole process of iterative calculation in Nanopolish into small independent calculation tasks, making it possible to run this process in the parallel mode. Experimental results show that MultiNanopolish reduces running time by 50% with read-uncorrected assembler (Miniasm) and 20% with read-corrected assembler (Canu and Flye) based on 40 threads mode compared to the original Nanopolish.\n\n\nAVAILABILITY\nMultiNanopolish is available at GitHub: https://github.com/BioinformaticsCSU/MultiNanopolish.\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary data are available at Bioinformatics online.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2055736642",
                    "name": "Kang Hu"
                },
                {
                    "authorId": "91944177",
                    "name": "Neng Huang"
                },
                {
                    "authorId": "145494002",
                    "name": "You Zou"
                },
                {
                    "authorId": "51072831",
                    "name": "Xingyu Liao"
                },
                {
                    "authorId": "2154791539",
                    "name": "Jianxin Wang"
                }
            ]
        },
        {
            "paperId": "eec951c4dcb75814f7be65f7154c4dddf2c95785",
            "title": "msRepDB: a comprehensive repetitive sequence database of over 80 000 species",
            "abstract": "Abstract Repeats are prevalent in the genomes of all bacteria, plants and animals, and they cover nearly half of the Human genome, which play indispensable roles in the evolution, inheritance, variation and genomic instability, and serve as substrates for chromosomal rearrangements that include disease-causing deletions, inversions, and translocations. Comprehensive identification, classification and annotation of repeats in genomes can provide accurate and targeted solutions towards understanding and diagnosis of complex diseases, optimization of plant properties and development of new drugs. RepBase and Dfam are two most frequently used repeat databases, but they are not sufficiently complete. Due to the lack of a comprehensive repeat database of multiple species, the current research in this field is far from being satisfactory. LongRepMarker is a new framework developed recently by our group for comprehensive identification of genomic repeats. We here propose msRepDB based on LongRepMarker, which is currently the most comprehensive multi-species repeat database, covering >80 000 species. Comprehensive evaluations show that msRepDB contains more species, and more complete repeats and families than RepBase and Dfam databases. (https://msrepdb.cbrc.kaust.edu.sa/pages/msRepDB/index.html).",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "51072831",
                    "name": "Xingyu Liao"
                },
                {
                    "authorId": "2055736642",
                    "name": "Kang Hu"
                },
                {
                    "authorId": "35233776",
                    "name": "Adil Salhi"
                },
                {
                    "authorId": "2113959576",
                    "name": "You Zou"
                },
                {
                    "authorId": "2154791539",
                    "name": "Jianxin Wang"
                },
                {
                    "authorId": "2109103060",
                    "name": "Xin Gao"
                }
            ]
        },
        {
            "paperId": "1be11d77c66e1e45f370b6d7a9755d3a1d595204",
            "title": "Improving de novo Assembly Based on Read Classification",
            "abstract": "Due to sequencing bias, sequencing error, and repeat problems, the genome assemblies usually contain misarrangements and gaps. When tackling these problems, current assemblers commonly consider the read libraries as a whole and adopt the same strategy to deal with them. However, if we can divide reads into different categories and take different assembly strategies for different read categories, we expect to reduce the mutual effects on problems in genome assembly and facilitate to produce satisfactory assemblies. In this paper, we present a new pipeline for genome assembly based on read classification (ARC). ARC classifies reads into three categories according to the frequencies of k-mers they contain. The three categories refer to (1) low depth reads, which contain a certain low frequency k-mers and are often caused by sequencing errors or bias; (2) high depth reads, which contain a certain high frequency k-mers and usually come from repetitive regions; and (3) normal depth reads, which are the rest of reads. After read classification, an existing assembler is used to assemble different read categories separately, which is beneficial to resolve problems in the genome assembly. ARC adopts loose assembly parameters for low depth reads, and strict assembly parameters for normal depth and high depth reads. We test ARC using five datasets. The experimental results show that, assemblers combining with ARC can generate better assemblies in terms of NA50, NGA50, and genome fraction.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "51072831",
                    "name": "Xingyu Liao"
                },
                {
                    "authorId": "49140537",
                    "name": "Min Li"
                },
                {
                    "authorId": "2119629",
                    "name": "Junwei Luo"
                },
                {
                    "authorId": "145494002",
                    "name": "You Zou"
                },
                {
                    "authorId": "145391153",
                    "name": "Fang-Xiang Wu"
                },
                {
                    "authorId": "144680113",
                    "name": "Yi Pan"
                },
                {
                    "authorId": "2072688908",
                    "name": "Feng Luo"
                },
                {
                    "authorId": "46583616",
                    "name": "Jianxin Wang"
                }
            ]
        },
        {
            "paperId": "4dfcdd7001325515bbba494660d9b4fd08e51440",
            "title": "Computational Approaches for Transcriptome Assembly Based on Sequencing Technologies",
            "abstract": "\n\nTranscriptome assembly plays a critical role in studying biological properties and\nexamining the expression levels of genomes in specific cells. It is also the basis of many\ndownstream analyses. With the increase of speed and the decrease in cost, massive sequencing\ndata continues to accumulate. A large number of assembly strategies based on different\ncomputational methods and experiments have been developed. How to efficiently perform\ntranscriptome assembly with high sensitivity and accuracy becomes a key issue. In this work, the\nissues with transcriptome assembly are explored based on different sequencing technologies.\nSpecifically, transcriptome assemblies with next-generation sequencing reads are divided into\nreference-based assemblies and de novo assemblies. The examples of different species are used to\nillustrate that long reads produced by the third-generation sequencing technologies can cover fulllength\ntranscripts without assemblies. In addition, different transcriptome assemblies using the\nHybrid-seq methods and other tools are also summarized. Finally, we discuss the future directions\nof transcriptome assemblies.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2215912262",
                    "name": "Yuefei Luo"
                },
                {
                    "authorId": "51072831",
                    "name": "Xingyu Liao"
                },
                {
                    "authorId": "145391153",
                    "name": "Fang-Xiang Wu"
                },
                {
                    "authorId": "2154791539",
                    "name": "Jianxin Wang"
                }
            ]
        },
        {
            "paperId": "8d14e9f34a24dc64e5b66bb3201625fcb176cb81",
            "title": "An Efficient Trimming Algorithm based on Multi-Feature Fusion Scoring Model for NGS Data",
            "abstract": "Next-generation sequencing (NGS) has enabled an exponential growth rate of sequencing data. However, several sequence artifacts, including error reads (base calling errors and small insertions or deletions) and poor quality reads, which can impose significant impact on the downstream sequence processing and analysis. Here, we present PE-Trimmer, a sensitive and special trimming algorithm for NGS sequence. First, PE-Trimmer removes technical sequences in paired-end reads based on the characteristics of low quality reads in NGS data. Second, PE-Trimmer determines the range of reads that need to be trimmed according to the quality score statistics histogram of reads in the library. To improve the accuracy of this algorithm, we design a light-weight and easy-to-explain scoring model to evaluate candidates in the pattern of trimming step. Finally, PE-Trimmer selects the appropriate trimming strategy to process the low quality reads based on the location determined by the scoring model. PE-Trimmer is able to locate and remove adapter residues from the paired-end reads. It is easily configurable and offers superior throughput in the multi-threaded mode. We test PE-Trimmer on five datasets, and compare it with the current five latest methods. The experimental results demonstrate that PE-Trimmer produces more superior results, compared with other trimmers.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51072831",
                    "name": "Xingyu Liao"
                },
                {
                    "authorId": "49140537",
                    "name": "Min Li"
                },
                {
                    "authorId": "145494002",
                    "name": "You Zou"
                },
                {
                    "authorId": "48093021",
                    "name": "Fangxiang Wu"
                },
                {
                    "authorId": "144680113",
                    "name": "Yi Pan"
                },
                {
                    "authorId": "46583616",
                    "name": "Jianxin Wang"
                }
            ]
        },
        {
            "paperId": "9b4e23676ad17f9f5115bd5aa1ba7e2616480420",
            "title": "MEC: Misassembly Error Correction in Contigs based on Distribution of Paired-End Reads and Statistics of GC-contents",
            "abstract": "The de novo assembly tools aim at reconstructing genomes from next-generation sequencing (NGS) data. However, the assembly tools usually generate a large amount of contigs containing many misassemblies, which are caused by problems of repetitive regions, chimeric reads, and sequencing errors. As they can improve the accuracy of assembly results, detecting and correcting the misassemblies in contigs are appealing, yet challenging. In this study, a novel method, called MEC, is proposed to identify and correct misassemblies in contigs. Based on the insert size distribution of paired-end reads and the statistical analysis of GC-contents, MEC can identify more misassemblies accurately. We evaluate our MEC with the metrics (NA50, NGA50) on four datasets, compared it with the most available misassembly correction tools, and carry out experiments to analyze the influence of MEC on scaffolding results, which shows that MEC can reduce misassemblies effectively and result in quantitative improvements in scaffolding quality. MEC is publicly available at https://github.com/bioinfomaticsCSU/MEC.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2152564024",
                    "name": "Binbin Wu"
                },
                {
                    "authorId": "49140537",
                    "name": "Min Li"
                },
                {
                    "authorId": "51072831",
                    "name": "Xingyu Liao"
                },
                {
                    "authorId": "2119629",
                    "name": "Junwei Luo"
                },
                {
                    "authorId": "48093021",
                    "name": "Fangxiang Wu"
                },
                {
                    "authorId": "144680113",
                    "name": "Yi Pan"
                },
                {
                    "authorId": "46583616",
                    "name": "Jianxin Wang"
                }
            ]
        },
        {
            "paperId": "0cfda5eabc55c6fdecfde084cd3d992b80ab77f2",
            "title": "de novo repeat detection based on the third generation sequencing reads",
            "abstract": "Repetitive sequences refer to fragments that appear at more than one location in a genome. Numerous studies have shown that the repetitive sequences in genomes play indispensable roles in the evolution, inheritance, variation, gene expression, transcriptional regulation, chromosome construction, and physiological metabolism of organisms. In many sequence and genome analyses such as read alignment, de novo assembly and genome annotation, repetitive sequences can pose major challenges. Detection and classification of repeats is one of the main steps for genome sequence analysis in bioinformatics. However, most existing de novo detection methods are difficult to achieve satisfactory results for marking repetitive regions in both size and accuracy due to the NGS reads are too short to identify long repeats and the raw SMS long reads are with the high error rates. In this study, we present a new de novo repeat detection method called DLR (Detection of Long Repeats) based on PacBio long reads. DLR first converts all long reads into unique $k-mers$ of a certain length, and screens out the $k-mers$ with the high frequency. Then, these high frequency $k-mers$ are aligned to long reads by using multiple sequence alignment, and the high frequency regions on long reads that are covered by those high frequency $k-mers$ are recorded. Finally, the recorded high frequency regions with inclusion relations are merged and the final repetitive sequences are obtained. The experimental results show that DLR achieves optimal results in terms of effective size and accuracy compared with other existing algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51072831",
                    "name": "Xingyu Liao"
                },
                {
                    "authorId": "1491631996",
                    "name": "Xiankai Zhang"
                },
                {
                    "authorId": "145391153",
                    "name": "Fang-Xiang Wu"
                },
                {
                    "authorId": "46583616",
                    "name": "Jianxin Wang"
                }
            ]
        },
        {
            "paperId": "6fab8a0089907afb232ff1a0f22361a4b49c1e39",
            "title": "A Sequence-Based Novel Approach for Quality Evaluation of Third-Generation Sequencing Reads",
            "abstract": "The advent of third-generation sequencing (TGS) technologies, such as the Pacific Biosciences (PacBio) and Oxford Nanopore machines, provides new possibilities for contig assembly, scaffolding, and high-performance computing in bioinformatics due to its long reads. However, the high error rate and poor quality of TGS reads provide new challenges for accurate genome assembly and long-read alignment. Efficient processing methods are in need to prioritize high-quality reads for improving the results of error correction and assembly. In this study, we proposed a novel Read Quality Evaluation and Selection Tool (REQUEST) for evaluating the quality of third-generation long reads. REQUEST generates training data of high-quality and low-quality reads which are characterized by their nucleotide combinations. A linear regression model was built to score the quality of reads. The method was tested on three datasets of different species. The results showed that the top-scored reads prioritized by REQUEST achieved higher alignment accuracies. The contig assembly results based on the top-scored reads also outperformed conventional approaches that use all reads. REQUEST is able to distinguish high-quality reads from low-quality ones without using reference genomes, making it a promising alternative sequence-quality evaluation method to alignment-based algorithms.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2108395118",
                    "name": "Wenjing Zhang"
                },
                {
                    "authorId": "91944177",
                    "name": "Neng Huang"
                },
                {
                    "authorId": "2046923271",
                    "name": "Jiantao Zheng"
                },
                {
                    "authorId": "51072831",
                    "name": "Xingyu Liao"
                },
                {
                    "authorId": "46583616",
                    "name": "Jianxin Wang"
                },
                {
                    "authorId": "2131645599",
                    "name": "Hong-Dong Li"
                }
            ]
        }
    ]
}