{
    "authorId": "2275119551",
    "papers": [
        {
            "paperId": "2c6638f6c817b7dc94365e217138d5b60cc699fa",
            "title": "A Chinese Dataset for Evaluating the Safeguards in Large Language Models",
            "abstract": "Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks when LLMs are deployed. Previous studies have proposed comprehensive taxonomies of the risks posed by LLMs, as well as corresponding prompts that can be used to examine the safety mechanisms of LLMs. However, the focus has been almost exclusively on English, and little has been explored for other languages. Here we aim to bridge this gap. We first introduce a dataset for the safety evaluation of Chinese LLMs, and then extend it to two other scenarios that can be used to better identify false negative and false positive examples in terms of risky prompt rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and automatic evaluation in terms of LLM response harmfulness. Our experiments on five LLMs show that region-specific risks are the prevalent type of risk, presenting the major issue with all Chinese LLMs we experimented with. Our data is available at https://github.com/Libr-AI/do-not-answer. Warning: this paper contains example data that may be offensive, harmful, or biased.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275119551",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "51230252",
                    "name": "Zenan Zhai"
                },
                {
                    "authorId": "49404498",
                    "name": "Haonan Li"
                },
                {
                    "authorId": "2110982198",
                    "name": "Xudong Han"
                },
                {
                    "authorId": "2284823063",
                    "name": "Lizhi Lin"
                },
                {
                    "authorId": "2284691442",
                    "name": "Zhenxuan Zhang"
                },
                {
                    "authorId": "2284725378",
                    "name": "Jingru Zhao"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2256987316",
                    "name": "Timothy Baldwin"
                }
            ]
        },
        {
            "paperId": "ed24b6814946c1975b0d86736055fa528b6419c0",
            "title": "Against The Achilles' Heel: A Survey on Red Teaming for Generative Models",
            "abstract": "Generative models are rapidly gaining popularity and being integrated into everyday applications, raising concerns over their safety issues as various vulnerabilities are exposed. Faced with the problem, the field of red teaming is experiencing fast-paced growth, which highlights the need for a comprehensive organization covering the entire pipeline and addressing emerging topics for the community. Our extensive survey, which examines over 120 papers, introduces a taxonomy of fine-grained attack strategies grounded in the inherent capabilities of language models. Additionally, we have developed the searcher framework that unifies various automatic red teaming approaches. Moreover, our survey covers novel areas including multimodal attacks and defenses, risks around multilingual models, overkill of harmless queries, and safety of downstream applications. We hope this survey can provide a systematic perspective on the field and unlock new areas of research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284823063",
                    "name": "Lizhi Lin"
                },
                {
                    "authorId": "2292039878",
                    "name": "Honglin Mu"
                },
                {
                    "authorId": "51230252",
                    "name": "Zenan Zhai"
                },
                {
                    "authorId": "2242189619",
                    "name": "Minghan Wang"
                },
                {
                    "authorId": "2275119551",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2215688800",
                    "name": "Renxi Wang"
                },
                {
                    "authorId": "2294509423",
                    "name": "Junjie Gao"
                },
                {
                    "authorId": "2266000475",
                    "name": "Yixuan Zhang"
                },
                {
                    "authorId": "2292032004",
                    "name": "Wanxiang Che"
                },
                {
                    "authorId": "2256987316",
                    "name": "Timothy Baldwin"
                },
                {
                    "authorId": "2110982198",
                    "name": "Xudong Han"
                },
                {
                    "authorId": "2266086775",
                    "name": "Haonan Li"
                }
            ]
        },
        {
            "paperId": "f68a7f40d755e87aa076df8ab74669e1c7cfdce3",
            "title": "Loki: An Open-Source Tool for Fact Verification",
            "abstract": "We introduce Loki, an open-source tool designed to address the growing problem of misinformation. Loki adopts a human-centered approach, striking a balance between the quality of fact-checking and the cost of human involvement. It decomposes the fact-checking task into a five-step pipeline: breaking down long texts into individual claims, assessing their check-worthiness, generating queries, retrieving evidence, and verifying the claims. Instead of fully automating the claim verification process, Loki provides essential information at each step to assist human judgment, especially for general users such as journalists and content moderators. Moreover, it has been optimized for latency, robustness, and cost efficiency at a commercially usable level. Loki is released under an MIT license and is available on GitHub. We also provide a video presenting the system and its capabilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2274084215",
                    "name": "Haonan Li"
                },
                {
                    "authorId": "2289460054",
                    "name": "Xudong Han"
                },
                {
                    "authorId": "2324499769",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2275119551",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2242189619",
                    "name": "Minghan Wang"
                },
                {
                    "authorId": "2323789608",
                    "name": "Rui Xing"
                },
                {
                    "authorId": "2323788235",
                    "name": "Yilin Geng"
                },
                {
                    "authorId": "51230252",
                    "name": "Zenan Zhai"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2256987316",
                    "name": "Timothy Baldwin"
                }
            ]
        },
        {
            "paperId": "f9f23c63e2822687096b86edf4ae9435cb579b8c",
            "title": "Do-Not-Answer: Evaluating Safeguards in LLMs",
            "abstract": "With the rapid evolution of large language models (LLMs), new and hard-to-predict harmful capabilities are emerging. This requires developers to identify potential risks through the evaluation of \u201cdangerous capabilities\u201d in order to responsibly deploy LLMs. Here we aim to facilitate this process. In particular, we collect an open-source dataset to evaluate the safeguards in LLMs, to facilitate the deployment of safer open-source LLMs at a low cost. Our dataset is curated and filtered to consist only of instructions that responsible language models should not follow. We assess the responses of six popular LLMs to these instructions, and we find that simple BERT-style classifiers can achieve results that are comparable to GPT-4 on automatic safety evaluation. Our data and code are available at https://github.com/Libr-AI/do-not-answer",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275119551",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "49404498",
                    "name": "Haonan Li"
                },
                {
                    "authorId": "2110982198",
                    "name": "Xudong Han"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2256987316",
                    "name": "Timothy Baldwin"
                }
            ]
        },
        {
            "paperId": "ec9414654469692d8f1de8e2401a3dcbc58ee11a",
            "title": "Demystifying Instruction Mixing for Fine-tuning Large Language Models",
            "abstract": "Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance, and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights into instruction mixtures, laying the foundations for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2215688800",
                    "name": "Renxi Wang"
                },
                {
                    "authorId": "49404498",
                    "name": "Haonan Li"
                },
                {
                    "authorId": "2145209409",
                    "name": "Minghao Wu"
                },
                {
                    "authorId": "2275119551",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2110982198",
                    "name": "Xudong Han"
                },
                {
                    "authorId": "50445559",
                    "name": "Chiyu Zhang"
                },
                {
                    "authorId": "2256987316",
                    "name": "Timothy Baldwin"
                }
            ]
        }
    ]
}