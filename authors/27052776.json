{
    "authorId": "27052776",
    "papers": [
        {
            "paperId": "843b86e94ee75f40f458b73de6efb4b125e8e4ee",
            "title": "Personalized and Diverse Task Composition in Crowdsourcing",
            "abstract": "We study task composition in crowdsourcing and the effect of personalization and diversity on performance. A central process in crowdsourcing is task assignment, the mechanism through which workers find tasks. On popular platforms such as Amazon Mechanical Turk, task assignment is facilitated by the ability to sort tasks by dimensions such as creation date or reward amount. Task composition improves task assignment by producing for each worker, a personalized summary of tasks, referred to as a Composite Task (CT). We propose different ways of producing CTs and formulate an optimization problem that finds for a worker, the most relevant and diverse CTs. We show empirically that workers\u2019 experience is greatly improved due to personalization that enforces an adequation of CTs with workers\u2019 skills and preferences. We also study and formalize various ways of diversifying tasks in each CT. Task diversity is grounded in organization studies that have shown its impact on worker motivation\u00a0 [33] . Our experiments show that diverse CTs contribute to improving outcome quality. More specifically, we show that while task throughput and worker retention are best with ranked lists, crowdwork quality reaches its best with CTs diversified by requesters, thereby confirming that workers look to expose their \u201cgood\u201d work to many requesters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27052776",
                    "name": "Maha Alsayasneh"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1732180",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "3416184",
                    "name": "Julien Pilourdault"
                },
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                },
                {
                    "authorId": "48449806",
                    "name": "Motomichi Toyama"
                },
                {
                    "authorId": "2822140",
                    "name": "J. Renders"
                }
            ]
        },
        {
            "paperId": "8cb41be8c18883bc1af9b8f882ccb25e20d5645d",
            "title": "The Case for Dynamic Placement of Distributed Systems Components",
            "abstract": "Today distributed systems are made of many software components with complex interactions. One of the key challenges in such an environment is determining how to place the components so that the system performs efficiently. In this paper, we illustrate the importance of component placement with a case study, examining the performance of a common stream processing pipeline comprising Kafka, Spark, and Cassandra. We study three applications (word count, Twitter sentiment analysis, machine learning) and three placement strategies. Our results show that (i) placement has a significant impact on the application throughput (up to 52%) and (ii) the placement achieving best results differs depending on the application. We discuss why existing solutions for performance troubleshooting in distributed systems are not sufficient to help choosing an efficient placement or to detect if a chosen placement is significantly under-performing compared to others. Finally, we describe research directions to address this open problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27052776",
                    "name": "Maha Alsayasneh"
                },
                {
                    "authorId": "1679769",
                    "name": "Vivien Qu\u00e9ma"
                },
                {
                    "authorId": "2045006",
                    "name": "Renaud Lachaize"
                }
            ]
        },
        {
            "paperId": "d3681507771ddb7918b82fdccc428367c3ec28bf",
            "title": "Crowdsourcing Strategies for Text Creation Tasks",
            "abstract": "We examine deployment strategies for text translation and text summarization tasks. We formalize a deployment strategy along three dimensions: work structure, workforce organization , and work style. Work structure can be either simultaneous or sequential, workforce organization independent or collaborative, and work style either crowd-only or hybrid. We use Amazon Mechanical Turk to evaluate the cost, latency, and quality of various deployment strategies. We asses our strategies for different scenarios: short/long text, presence/absence of an outline, and popular/unpopular topics. Our findings serve as a basis to automate the deployment of text creation tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                },
                {
                    "authorId": "27052776",
                    "name": "Maha Alsayasneh"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                }
            ]
        }
    ]
}