{
    "authorId": "2104644641",
    "papers": [
        {
            "paperId": "09d434d02138ad4049be421bf2351de4aa295412",
            "title": "Alexa, play with robot: Introducing the First Alexa Prize SimBot Challenge on Embodied AI",
            "abstract": "The Alexa Prize program has empowered numerous university students to explore, experiment, and showcase their talents in building conversational agents through challenges like the SocialBot Grand Challenge and the TaskBot Challenge. As conversational agents increasingly appear in multimodal and embodied contexts, it is important to explore the affordances of conversational interaction augmented with computer vision and physical embodiment. This paper describes the SimBot Challenge, a new challenge in which university teams compete to build robot assistants that complete tasks in a simulated physical environment. This paper provides an overview of the SimBot Challenge, which included both online and offline challenge phases. We describe the infrastructure and support provided to the teams including Alexa Arena, the simulated environment, and the ML toolkit provided to teams to accelerate their building of vision and language models. We summarize the approaches the participating teams took to overcome research challenges and extract key lessons learned. Finally, we provide analysis of the performance of the competing SimBots during the competition.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167861070",
                    "name": "Hangjie Shi"
                },
                {
                    "authorId": "2167616711",
                    "name": "Leslie Ball"
                },
                {
                    "authorId": "2028300167",
                    "name": "G. Thattai"
                },
                {
                    "authorId": "2187068732",
                    "name": "Desheng Zhang"
                },
                {
                    "authorId": "2144596247",
                    "name": "Lu Hu"
                },
                {
                    "authorId": "3193409",
                    "name": "Qiaozi Gao"
                },
                {
                    "authorId": "1831108414",
                    "name": "Suhaila Shakiah"
                },
                {
                    "authorId": "46757485",
                    "name": "Xiaofeng Gao"
                },
                {
                    "authorId": "2110665",
                    "name": "Aishwarya Padmakumar"
                },
                {
                    "authorId": "2119660672",
                    "name": "Bo Yang"
                },
                {
                    "authorId": "2210738933",
                    "name": "Cadence Chung"
                },
                {
                    "authorId": "2210732208",
                    "name": "Dinakar Guthy"
                },
                {
                    "authorId": "1732493",
                    "name": "G. Sukhatme"
                },
                {
                    "authorId": "2210732281",
                    "name": "Karthika Arumugam"
                },
                {
                    "authorId": "2210732608",
                    "name": "Matthew Wen"
                },
                {
                    "authorId": "2167638287",
                    "name": "Osman Ipek"
                },
                {
                    "authorId": "26882347",
                    "name": "P. Lange"
                },
                {
                    "authorId": "31606876",
                    "name": "Rohan Khanna"
                },
                {
                    "authorId": "31264280",
                    "name": "Shreyas Pansare"
                },
                {
                    "authorId": "144582538",
                    "name": "Vasu Sharma"
                },
                {
                    "authorId": "2256775738",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "1391844339",
                    "name": "C. Flagg"
                },
                {
                    "authorId": "34931641",
                    "name": "Daniel Pressel"
                },
                {
                    "authorId": "2167643644",
                    "name": "Lavina Vaz"
                },
                {
                    "authorId": "2115177932",
                    "name": "Luke Dai"
                },
                {
                    "authorId": "38774604",
                    "name": "Prasoon Goyal"
                },
                {
                    "authorId": "2167648845",
                    "name": "Sattvik Sahai"
                },
                {
                    "authorId": "2229954252",
                    "name": "Shaohua Liu"
                },
                {
                    "authorId": "2167771392",
                    "name": "Yao Lu"
                },
                {
                    "authorId": "1411423941",
                    "name": "Anna Gottardi"
                },
                {
                    "authorId": "2122825525",
                    "name": "Shui Hu"
                },
                {
                    "authorId": "2152801542",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "35834027",
                    "name": "Kate Bland"
                },
                {
                    "authorId": "151027894",
                    "name": "Heather Rocker"
                },
                {
                    "authorId": "2231135132",
                    "name": "James Jeun"
                },
                {
                    "authorId": "2210728984",
                    "name": "Yadunandana Rao"
                },
                {
                    "authorId": "2078507351",
                    "name": "Michael Johnston"
                },
                {
                    "authorId": "8621381",
                    "name": "Akshaya Iyengar"
                },
                {
                    "authorId": "33638380",
                    "name": "Arindam Mandal"
                },
                {
                    "authorId": "2104644641",
                    "name": "Premkumar Natarajan"
                },
                {
                    "authorId": "3306272",
                    "name": "R. Ghanadan"
                }
            ]
        },
        {
            "paperId": "7dc98ae2967d6ad9c115ccaa705540b7489e0d40",
            "title": "User-Controllable Arbitrary Style Transfer via Entropy Regularization",
            "abstract": "Ensuring the overall end-user experience is a challenging task in arbitrary style transfer (AST) due to the subjective nature of style transfer quality. A good practice is to provide users many instead of one AST result. However, existing approaches require to run multiple AST models or inference a diversified AST (DAST) solution multiple times, and thus they are either slow in speed or limited in diversity. In this paper, we propose a novel solution ensuring both efficiency and diversity for generating multiple user-controllable AST results by systematically modulating AST behavior at run-time. We begin with reformulating three prominent AST methods into a unified assign-and-mix problem and discover that the entropies of their assignment matrices exhibit a large variance. We then solve the unified problem in an optimal transport framework using the Sinkhorn-Knopp algorithm with a user input \u03b5 to control the said entropy and thus modulate stylization. Empirical results demonstrate the superiority of the proposed solution, with speed and stylization quality comparable to or better than existing AST and significantly more diverse than previous DAST works. Code is available at https://github.com/cplusx/eps-Assign-and-Mix.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33896010",
                    "name": "Jiaxin Cheng"
                },
                {
                    "authorId": "2109035901",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "26393556",
                    "name": "Ayush Jaiswal"
                },
                {
                    "authorId": "2213162331",
                    "name": "Xu Zhang"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "2104644641",
                    "name": "Premkumar Natarajan"
                }
            ]
        },
        {
            "paperId": "af4e2bc2da111f2a9da9846a6d19d43ce5e63729",
            "title": "Incorporating Fairness in Large Scale NLU Systems",
            "abstract": "NLU models power several user facing experiences such as conversations agents and chat bots. Building NLU models typically consist of 3 stages: a) building or finetuning a pre-trained model b) distilling or fine-tuning the pre-trained model to build task specific models and, c) deploying the task-specific model to production. In this presentation, we will identify fairness considerations that can be incorporated in the aforementioned three stages in the life-cycle of NLU model building: (i) selection/building of a large scale language model, (ii) distillation/fine-tuning the large model into task specific model and, (iii) deployment of the task specific model. We will present select metrics that can be used to quantify fairness in NLU models and fairness enhancement techniques that can be deployed in each of these stages. Finally, we will share some recommendations to successfully implement fairness considerations when building an industrial scale NLU system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2139538015",
                    "name": "Rahul Gupta"
                },
                {
                    "authorId": "12620674",
                    "name": "Lisa Bauer"
                },
                {
                    "authorId": "2782886",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "3475586",
                    "name": "J. Dhamala"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "3436466",
                    "name": "Palash Goyal"
                },
                {
                    "authorId": "2192042848",
                    "name": "Qian Hu"
                },
                {
                    "authorId": "33882114",
                    "name": "Avni Khatri"
                },
                {
                    "authorId": "3262272",
                    "name": "Rohit Parimi"
                },
                {
                    "authorId": "102648923",
                    "name": "Charith Peris"
                },
                {
                    "authorId": "3363380",
                    "name": "Apurv Verma"
                },
                {
                    "authorId": "1804104",
                    "name": "R. Zemel"
                },
                {
                    "authorId": "2104644641",
                    "name": "Premkumar Natarajan"
                }
            ]
        },
        {
            "paperId": "b7c676d7b14af6090bf247db2d538e33de8ccd58",
            "title": "AMPERE: AMR-Aware Prefix for Generation-Based Event Argument Extraction Model",
            "abstract": "Event argument extraction (EAE) identifies event arguments and their specific roles for a given event. Recent advancement in generation-based EAE models has shown great performance and generalizability over classification-based models. However, existing generation-based EAE models mostly focus on problem re-formulation and prompt design, without incorporating additional information that has been shown to be effective for classification-based models, such as the abstract meaning representation (AMR) of the input passages. Incorporating such information into generation-based models is challenging due to the heterogeneous nature of the natural language form prevalently used in generation-based models and the structured form of AMRs. In this work, we study strategies to incorporate AMR into generation-based EAE models. We propose AMPERE, which generates AMR-aware prefixes for every layer of the generation model. Thus, the prefix introduces AMR information to the generation-based EAE model and then improves the generation. We also introduce an adjusted copy mechanism to AMPERE to help overcome potential noises brought by the AMR graph. Comprehensive experiments and analyses on ACE2005 and ERE datasets show that AMPERE can get 4% - 10% absolute F1 score improvements with reduced training data and it is in general powerful across different training sizes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34809425",
                    "name": "I-Hung Hsu"
                },
                {
                    "authorId": "2099588738",
                    "name": "Zhiyu Xie"
                },
                {
                    "authorId": "3137324",
                    "name": "Kuan-Hao Huang"
                },
                {
                    "authorId": "2104644641",
                    "name": "Premkumar Natarajan"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "db4063aac6bd51ef08473b986bda71fb3cf16e87",
            "title": "DeepMaven: Deep Question Answering on Long-Distance Movie/TV Show Videos with Multimedia Knowledge Extraction and Synthesis",
            "abstract": "Long video content understanding poses a challenging set of research questions as it involves long-distance, cross-media reasoning and knowledge awareness. In this paper, we present a new benchmark for this problem domain, targeting the task of deep movie/TV question answering (QA) beyond previous work\u2019s focus on simple plot summary and short video moment settings. We define several baselines based on direct retrieval of relevant context for long-distance movie QA. Observing that real-world QAs may require higher-order multi-hop inferences, we further propose a novel framework, called the DeepMaven, which extracts events, entities, and relations from the rich multimedia content in long videos to pre-construct movie knowledge graphs (movieKGs), and at the time of QA inference, complements general semantics with structured knowledge for more effective information retrieval and knowledge reasoning. We also introduce our recently collected DeepMovieQA dataset, including 1,000 long-form QA pairs from 41 hours of videos, to serve as a new and useful resource for future work. Empirical results show the DeepMaven performs competitively for both the new DeepMovieQA and the pre-existing MovieQA dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51135899",
                    "name": "Y. Fung"
                },
                {
                    "authorId": "40465036",
                    "name": "H. Wang"
                },
                {
                    "authorId": "2116678322",
                    "name": "Tong Wang"
                },
                {
                    "authorId": "1873667",
                    "name": "Ali Kebarighotbi"
                },
                {
                    "authorId": "143977268",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "2072975661",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2104644641",
                    "name": "Premkumar Natarajan"
                }
            ]
        },
        {
            "paperId": "dc75bfd795308f2ec97a218952952690a1eb449d",
            "title": "TAGPRIME: A Unified Framework for Relational Structure Extraction",
            "abstract": "Many tasks in natural language processing require the extraction of relationship information for a given condition, such as event argument extraction, relation extraction, and task-oriented semantic parsing. Recent works usually propose sophisticated models for each of the tasks independently and pay less attention to the commonality of these tasks and to have a unified framework for all the tasks. In this work, we propose to take a unified view of all these tasks and introduce TAGPRIME to address relational structure extraction problems. TAGPRIME is a sequence tagging model that appends priming words about the information of the given condition (such as an event trigger) to the input text. With the self-attention mechanism in pre-trained language models, the priming words make the output contextualized representations contain more information about the given condition, and hence become more suitable for extracting specific relationships for the condition. Extensive experiments and analyses on three different tasks that cover ten datasets across five different languages demonstrate the generality and effectiveness of TAGPRIME.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34809425",
                    "name": "I-Hung Hsu"
                },
                {
                    "authorId": "3137324",
                    "name": "Kuan-Hao Huang"
                },
                {
                    "authorId": "2167035973",
                    "name": "Shuning Zhang"
                },
                {
                    "authorId": "2152125885",
                    "name": "Wen-Huang Cheng"
                },
                {
                    "authorId": "2104644641",
                    "name": "Premkumar Natarajan"
                },
                {
                    "authorId": "2782886",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "02720ba7a4c0c70506ef63e039387c10b227d8e3",
            "title": "Transform-Retrieve-Generate: Natural Language-Centric Outside-Knowledge Visual Question Answering",
            "abstract": "Outside-knowledge visual question answering (OK-VQA) requires the agent to comprehend the image, make use of relevant knowledge from the entire web, and digest all the information to answer the question. Most previous works address the problem by first fusing the image and question in the multi-modal space, which is inflexible for further fusion with a vast amount of external knowledge. In this paper, we call for an alternative paradigm for the OK-VQA task, which transforms the image into plain text, so that we can enable knowledge passage retrieval, and generative question-answering in the natural language space. This paradigm takes advantage of the sheer volume of gigantic knowledge bases and the richness of pretrained language models. A Transform-Retrieve-Generate framework (TRiG) framework is proposed11The code of this work will be made public., which can be plug-and-played with alternative image-to-text models and textual knowledge bases. Experimental results show that our TRiG framework outperforms all state-of-the-art supervised methods by at least 11.1 % absolute margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153408839",
                    "name": "Feng Gao"
                },
                {
                    "authorId": "40492634",
                    "name": "Q. Ping"
                },
                {
                    "authorId": "2028300167",
                    "name": "G. Thattai"
                },
                {
                    "authorId": "8856206",
                    "name": "Aishwarya N. Reganti"
                },
                {
                    "authorId": "47095971",
                    "name": "Yingting Wu"
                },
                {
                    "authorId": "2104644641",
                    "name": "Premkumar Natarajan"
                }
            ]
        },
        {
            "paperId": "196c925eb2d0be235d85b3944e013330d101901c",
            "title": "A Thousand Words Are Worth More Than a Picture: Natural Language-Centric Outside-Knowledge Visual Question Answering",
            "abstract": "Outside-knowledge visual question answering (OK-VQA) requires the agent to comprehend the image, make use of relevant knowledge from the entire web, and digest all the information to answer the question. Most previous works address the problem by first fusing the image and question in the multi-modal space, which is inflexible for further fusion with a vast amount of external knowledge. In this paper, we call for a paradigm shift for the OK-VQA task, which transforms the image into plain text, so that we can enable knowledge passage retrieval, and generative question-answering in the natural language space. This paradigm takes advantage of the sheer volume of gigantic knowledge bases and the richness of pre-trained language models. A Transform-Retrieve-Generate framework (TRiG) framework is proposed, which can be plug-and-played with alternative image-to-text models and textual knowledge bases. Experimental results show that our TRiG framework outperforms all state-of-the-art supervised methods by at least 11.1% absolute margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153408839",
                    "name": "Feng Gao"
                },
                {
                    "authorId": "40492634",
                    "name": "Q. Ping"
                },
                {
                    "authorId": "2028300167",
                    "name": "G. Thattai"
                },
                {
                    "authorId": "8856206",
                    "name": "Aishwarya N. Reganti"
                },
                {
                    "authorId": "47095971",
                    "name": "Yingting Wu"
                },
                {
                    "authorId": "2104644641",
                    "name": "Premkumar Natarajan"
                }
            ]
        },
        {
            "paperId": "914254fac74a2da051cccf6ca16afcaad416a079",
            "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
            "abstract": "In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2805456",
                    "name": "Saleh Soltan"
                },
                {
                    "authorId": "2773408",
                    "name": "Shankar Ananthakrishnan"
                },
                {
                    "authorId": "120590817",
                    "name": "Jack G. M. FitzGerald"
                },
                {
                    "authorId": "145542597",
                    "name": "Rahul Gupta"
                },
                {
                    "authorId": "1836135",
                    "name": "Wael Hamza"
                },
                {
                    "authorId": "144165565",
                    "name": "Haidar Khan"
                },
                {
                    "authorId": "102648923",
                    "name": "Charith Peris"
                },
                {
                    "authorId": "38696444",
                    "name": "Stephen Rawls"
                },
                {
                    "authorId": "146177177",
                    "name": "Andrew Rosenbaum"
                },
                {
                    "authorId": "1681193",
                    "name": "Anna Rumshisky"
                },
                {
                    "authorId": "1588348842",
                    "name": "Chandan Prakash"
                },
                {
                    "authorId": "1734869335",
                    "name": "Mukund Sridhar"
                },
                {
                    "authorId": "1761263",
                    "name": "Fabian Triefenbach"
                },
                {
                    "authorId": "3363380",
                    "name": "Apurv Verma"
                },
                {
                    "authorId": "5108268",
                    "name": "Gokhan Tur"
                },
                {
                    "authorId": "2104644641",
                    "name": "Premkumar Natarajan"
                }
            ]
        },
        {
            "paperId": "daa3af99c6421a60e4dc06cb27fc97a60a1aa54b",
            "title": "Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems",
            "abstract": "We present results from a large-scale experiment on pretraining encoders with non-embedding parameter counts ranging from 700M to 9.3B, their subsequent distillation into smaller models ranging from 17M-170M parameters, and their application to the Natural Language Understanding (NLU) component of a virtual assistant system. Though we train using 70% spoken-form data, our teacher models perform comparably to XLM-R and mT5 when evaluated on the written-form Cross-lingual Natural Language Inference (XNLI) corpus. We perform a second stage of pretraining on our teacher models using in-domain data from our system, improving error rates by 3.86% relative for intent classification and 7.01% relative for slot filling. We find that even a 170M-parameter model distilled from our Stage 2 teacher model has 2.88% better intent classification and 7.69% better slot filling error rates when compared to the 2.3B-parameter teacher trained only on public data (Stage 1), emphasizing the importance of in-domain data for pretraining. When evaluated offline using labeled NLU data, our 17M-parameter Stage 2 distilled model outperforms both XLM-R Base (85M params) and DistillBERT (42M params) by 4.23% to 6.14%, respectively. Finally, we present results from a full virtual assistant experimentation platform, where we find that models trained using our pretraining and distillation pipeline outperform models distilled from 85M-parameter teachers by 3.74%-4.91% on an automatic measurement of full-system user dissatisfaction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120590817",
                    "name": "Jack G. M. FitzGerald"
                },
                {
                    "authorId": "2773408",
                    "name": "Shankar Ananthakrishnan"
                },
                {
                    "authorId": "1757320",
                    "name": "Konstantine Arkoudas"
                },
                {
                    "authorId": "2081635546",
                    "name": "Davide Bernardi"
                },
                {
                    "authorId": "2170538298",
                    "name": "Abhishek Bhagia"
                },
                {
                    "authorId": "3257579",
                    "name": "Claudio Delli Bovi"
                },
                {
                    "authorId": "2109908684",
                    "name": "Jin Cao"
                },
                {
                    "authorId": "147887099",
                    "name": "Rakesh Chada"
                },
                {
                    "authorId": "2170537879",
                    "name": "Amit Chauhan"
                },
                {
                    "authorId": "1919815",
                    "name": "Luoxin Chen"
                },
                {
                    "authorId": "15854877",
                    "name": "Anurag Dwarakanath"
                },
                {
                    "authorId": "1900292156",
                    "name": "Satyam Dwivedi"
                },
                {
                    "authorId": "2820794",
                    "name": "Turan Gojayev"
                },
                {
                    "authorId": "145916630",
                    "name": "Karthik Gopalakrishnan"
                },
                {
                    "authorId": "2308928",
                    "name": "Thomas Gueudr\u00e9"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "1836135",
                    "name": "Wael Hamza"
                },
                {
                    "authorId": "2149932798",
                    "name": "Jonathan Hueser"
                },
                {
                    "authorId": "2117356",
                    "name": "Kevin Martin Jose"
                },
                {
                    "authorId": "144165565",
                    "name": "Haidar Khan"
                },
                {
                    "authorId": "2108662892",
                    "name": "Bei Liu"
                },
                {
                    "authorId": "2152961472",
                    "name": "Jianhua Lu"
                },
                {
                    "authorId": "47773962",
                    "name": "A. Manzotti"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "1980308",
                    "name": "Karolina Owczarzak"
                },
                {
                    "authorId": "2033358253",
                    "name": "Gokmen Oz"
                },
                {
                    "authorId": "40436342",
                    "name": "Enrico Palumbo"
                },
                {
                    "authorId": "102648923",
                    "name": "Charith Peris"
                },
                {
                    "authorId": "1588348842",
                    "name": "Chandan Prakash"
                },
                {
                    "authorId": "38696444",
                    "name": "Stephen Rawls"
                },
                {
                    "authorId": "146177177",
                    "name": "Andrew Rosenbaum"
                },
                {
                    "authorId": "37832169",
                    "name": "Anjali Shenoy"
                },
                {
                    "authorId": "2805456",
                    "name": "Saleh Soltan"
                },
                {
                    "authorId": "1734869335",
                    "name": "Mukund Sridhar"
                },
                {
                    "authorId": "2087344242",
                    "name": "Lizhen Tan"
                },
                {
                    "authorId": "1761263",
                    "name": "Fabian Triefenbach"
                },
                {
                    "authorId": "2162760612",
                    "name": "Pan Wei"
                },
                {
                    "authorId": "2119316118",
                    "name": "Haiyang Yu"
                },
                {
                    "authorId": "2000361443",
                    "name": "Shuai Zheng"
                },
                {
                    "authorId": "5108268",
                    "name": "Gokhan Tur"
                },
                {
                    "authorId": "2104644641",
                    "name": "Premkumar Natarajan"
                }
            ]
        }
    ]
}