{
    "authorId": "2266387313",
    "papers": [
        {
            "paperId": "098be01c95b4c18e2c7e8b4164d29dbb0903e71f",
            "title": "Can a Multichoice Dataset be Repurposed for Extractive Question Answering?",
            "abstract": "The rapid evolution of Natural Language Processing (NLP) has favored major languages such as English, leaving a significant gap for many others due to limited resources. This is especially evident in the context of data annotation, a task whose importance cannot be underestimated, but which is time-consuming and costly. Thus, any dataset for resource-poor languages is precious, in particular when it is task-specific. Here, we explore the feasibility of repurposing existing datasets for a new NLP task: we repurposed the Belebele dataset (Bandarkar et al., 2023), which was designed for multiple-choice question answering (MCQA), to enable extractive QA (EQA) in the style of machine reading comprehension. We present annotation guidelines and a parallel EQA dataset for English and Modern Standard Arabic (MSA). We also present QA evaluation results for several monolingual and cross-lingual QA pairs including English, MSA, and five Arabic dialects. Our aim is to enable others to adapt our approach for the 120+ other language variants in Belebele, many of which are deemed under-resourced. We also conduct a thorough analysis and share our insights from the process, which we hope will contribute to a deeper understanding of the challenges and the opportunities associated with task reformulation in NLP research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2298756162",
                    "name": "Teresa Lynn"
                },
                {
                    "authorId": "51935928",
                    "name": "Malik H. Altakrori"
                },
                {
                    "authorId": "148087360",
                    "name": "S. Magdy"
                },
                {
                    "authorId": "2211732585",
                    "name": "Rocktim Jyoti Das"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "2056258384",
                    "name": "Mohamed Nasr"
                },
                {
                    "authorId": "2282523149",
                    "name": "Younes Samih"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2111356",
                    "name": "S. Godbole"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                }
            ]
        },
        {
            "paperId": "0ba76fbb7a4a2e6a221b4c31321e9846eca2fe92",
            "title": "Reference-free Hallucination Detection for Large Vision-Language Models",
            "abstract": "Large vision-language models (LVLMs) have made significant progress in recent years. While LVLMs exhibit excellent ability in language understanding, question answering, and conversations of visual inputs, they are prone to producing hallucinations. While several methods are proposed to evaluate the hallucinations in LVLMs, most are reference-based and depend on external tools, which complicates their practical application. To assess the viability of alternative methods, it is critical to understand whether the reference-free approaches, which do not rely on any external tools, can efficiently detect hallucinations. Therefore, we initiate an exploratory study to demonstrate the effectiveness of different reference-free solutions in detecting hallucinations in LVLMs. In particular, we conduct an extensive study on three kinds of techniques: uncertainty-based, consistency-based, and supervised uncertainty quantification methods on four representative LVLMs across two different tasks. The empirical results show that the reference-free approaches are capable of effectively detecting non-factual responses in LVLMs, with the supervised uncertainty quantification method outperforming the others, achieving the best performance across different settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2295742465",
                    "name": "Qing Li"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "2093923066",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "2315991004",
                    "name": "Derui Zhu"
                },
                {
                    "authorId": "2315922847",
                    "name": "Maxim Panov"
                },
                {
                    "authorId": "2264150375",
                    "name": "Fakhri Karray"
                }
            ]
        },
        {
            "paperId": "3aabd69e13f64f10fd210e4e9e6b2e75c0e734d1",
            "title": "CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark",
            "abstract": "Visual Question Answering (VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models to understand and reason on knowledge present in both visual and textual data. However, most of the current VQA models use datasets that are primarily focused on English and a few major world languages, with images that are typically Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, although these datasets often extend their linguistic range via translation or some other approaches, they usually keep images the same, resulting in narrow cultural representation. To address these limitations, we construct CVQA, a new Culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures, where we engage native speakers and cultural experts in the data collection process. As a result, CVQA includes culturally-driven images and questions from across 28 countries on four continents, covering 26 languages with 11 scripts, providing a total of 9k questions. We then benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and show that the dataset is challenging for the current state-of-the-art models. This benchmark can serve as a probing evaluation suite for assessing the cultural capability and bias of multimodal models and hopefully encourage more research efforts toward increasing cultural awareness and linguistic diversity in this field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2305602940",
                    "name": "David Romero"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "49918371",
                    "name": "Haryo Akbarianto Wibowo"
                },
                {
                    "authorId": "2298756162",
                    "name": "Teresa Lynn"
                },
                {
                    "authorId": "3248560",
                    "name": "Injy Hamed"
                },
                {
                    "authorId": "2305619033",
                    "name": "Aditya Nanda Kishore"
                },
                {
                    "authorId": "2305622255",
                    "name": "Aishik Mandal"
                },
                {
                    "authorId": "2305619182",
                    "name": "Alina Dragonetti"
                },
                {
                    "authorId": "1396213362",
                    "name": "Artem Abzaliev"
                },
                {
                    "authorId": "2148631756",
                    "name": "A. Tonja"
                },
                {
                    "authorId": "2305622331",
                    "name": "Bontu Fufa Balcha"
                },
                {
                    "authorId": "2161240241",
                    "name": "Chenxi Whitehouse"
                },
                {
                    "authorId": "51124788",
                    "name": "Christian Salamea"
                },
                {
                    "authorId": "1994718316",
                    "name": "Dan John Velasco"
                },
                {
                    "authorId": "2273673245",
                    "name": "D. Adelani"
                },
                {
                    "authorId": "70145452",
                    "name": "D. Meur"
                },
                {
                    "authorId": "2183780558",
                    "name": "Emilio Villa-Cueva"
                },
                {
                    "authorId": "2789148",
                    "name": "Fajri Koto"
                },
                {
                    "authorId": "2266756359",
                    "name": "Fauzan Farooqui"
                },
                {
                    "authorId": "1738707459",
                    "name": "Frederico Belcavello"
                },
                {
                    "authorId": "2151970366",
                    "name": "Ganzorig Batnasan"
                },
                {
                    "authorId": "2305623074",
                    "name": "Gisela Vallejo"
                },
                {
                    "authorId": "2305619231",
                    "name": "Grainne Caulfield"
                },
                {
                    "authorId": "2213060824",
                    "name": "Guido Ivetta"
                },
                {
                    "authorId": "2980506",
                    "name": "Haiyue Song"
                },
                {
                    "authorId": "2305619369",
                    "name": "Henok Biadglign Ademtew"
                },
                {
                    "authorId": "2139773809",
                    "name": "Hern\u00e1n Maina"
                },
                {
                    "authorId": "116344405",
                    "name": "Holy Lovenia"
                },
                {
                    "authorId": "2304752238",
                    "name": "Israel Abebe Azime"
                },
                {
                    "authorId": "2282499634",
                    "name": "Jan Christian Blaise Cruz"
                },
                {
                    "authorId": "1992915388",
                    "name": "Jay Gala"
                },
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "1724941617",
                    "name": "Jes\u00fas-Germ\u00e1n Ortiz-Barajas"
                },
                {
                    "authorId": "90765684",
                    "name": "Jinheon Baek"
                },
                {
                    "authorId": "2305082736",
                    "name": "Jocelyn Dunstan"
                },
                {
                    "authorId": "2276687",
                    "name": "L. A. Alemany"
                },
                {
                    "authorId": "2290013575",
                    "name": "Kumaranage Ravindu Yasas Nagasinghe"
                },
                {
                    "authorId": "2066254822",
                    "name": "Luciana Benotti"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1738707461",
                    "name": "Marcelo Viridiano"
                },
                {
                    "authorId": "2168569460",
                    "name": "Marcos Estecha-Garitagoitia"
                },
                {
                    "authorId": "2305622553",
                    "name": "Maria Camila Buitrago Cabrera"
                },
                {
                    "authorId": "2220406508",
                    "name": "Mario Rodr'iguez-Cantelar"
                },
                {
                    "authorId": "71090258",
                    "name": "M\u00e9lanie Jouitteau"
                },
                {
                    "authorId": "121947924",
                    "name": "M. Mihaylov"
                },
                {
                    "authorId": "2305619376",
                    "name": "Mohamed Fazli Mohamed Imam"
                },
                {
                    "authorId": "2191731497",
                    "name": "Muhammad Farid Adilazuarda"
                },
                {
                    "authorId": "66556569",
                    "name": "Munkhjargal Gochoo"
                },
                {
                    "authorId": "2159634278",
                    "name": "Munkh-Erdene Otgonbold"
                },
                {
                    "authorId": "1742219452",
                    "name": "Naome A. Etori"
                },
                {
                    "authorId": "2305623065",
                    "name": "Olivier Niyomugisha"
                },
                {
                    "authorId": "2307313242",
                    "name": "Paula M'onica Silva"
                },
                {
                    "authorId": "2040713514",
                    "name": "Pranjal A. Chitale"
                },
                {
                    "authorId": "3209719",
                    "name": "Raj Dabre"
                },
                {
                    "authorId": "2148764367",
                    "name": "Rendi Chevi"
                },
                {
                    "authorId": "49775305",
                    "name": "Ruochen Zhang"
                },
                {
                    "authorId": "2197070752",
                    "name": "Ryandito Diandaru"
                },
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "2305622481",
                    "name": "Santiago G'ongora"
                },
                {
                    "authorId": "8599185",
                    "name": "Soyeong Jeong"
                },
                {
                    "authorId": "152881983",
                    "name": "Sukannya Purkayastha"
                },
                {
                    "authorId": "83446147",
                    "name": "Tatsuki Kuribayashi"
                },
                {
                    "authorId": "2219413815",
                    "name": "Thanmay Jayakumar"
                },
                {
                    "authorId": "2244512282",
                    "name": "T. Torrent"
                },
                {
                    "authorId": "2305621229",
                    "name": "Toqeer Ehsan"
                },
                {
                    "authorId": "2283931820",
                    "name": "Vladimir Araujo"
                },
                {
                    "authorId": "51208524",
                    "name": "Yova Kementchedjhieva"
                },
                {
                    "authorId": "2305621242",
                    "name": "Zara Burzo"
                },
                {
                    "authorId": "2305621264",
                    "name": "Zheng Wei Lim"
                },
                {
                    "authorId": "2282475073",
                    "name": "Zheng-Xin Yong"
                },
                {
                    "authorId": "2293317558",
                    "name": "Oana Ignat"
                },
                {
                    "authorId": "2218338376",
                    "name": "Joan Nwatu"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                },
                {
                    "authorId": "1794626",
                    "name": "T. Solorio"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                }
            ]
        },
        {
            "paperId": "c1024f8737e56111899950bea9ac7138070e3e8b",
            "title": "Benchmarking and Improving Long-Text Translation with Large Language Models",
            "abstract": "Recent studies have illuminated the promising capabilities of large language models (LLMs) in handling long texts. However, their performance in machine translation (MT) of long documents remains underexplored. This paper aims to shed light on how LLMs navigate this complex task, offering a comprehensive evaluation of their capabilities and limitations in long-text MT. First, we collect and construct an instruction-based benchmark dataset, specifically designed for the finetuning and evaluation of LLMs, encompassing multilingual, multi-domain, and document-level parallel data. Second, we conduct a comprehensive comparison between MT and LLM models concerning document-level translation. Our analysis uncovers that LLMs exhibit short-comings in long-text domains, and their performance diminishes as document size esca-lates. By exploiting various extrapolation strategies, we enhance the capacity of LLMs to translate longer texts. We release data, code, and models at https://github.com/ longyuewangdcu/Document-MT-LLM .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111542852",
                    "name": "Longyue Wang"
                },
                {
                    "authorId": "2316589787",
                    "name": "Zefeng Du"
                },
                {
                    "authorId": "12386833",
                    "name": "Wenxiang Jiao"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "2266902867",
                    "name": "Jianhui Pang"
                },
                {
                    "authorId": "2279792419",
                    "name": "Leyang Cui"
                },
                {
                    "authorId": "2316367524",
                    "name": "Kaiqiang Song"
                },
                {
                    "authorId": "2266923623",
                    "name": "Derek F. Wong"
                },
                {
                    "authorId": "2239383048",
                    "name": "Shuming Shi"
                },
                {
                    "authorId": "2256685351",
                    "name": "Zhaopeng Tu"
                }
            ]
        },
        {
            "paperId": "c290633698501ea83144d61d001eb7ac7a42d853",
            "title": "Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research. However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios. While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question. This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations. Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction. Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output probabilities rather than directly generating responses, owing to computational limitations. We illustrate that these probability-based approaches do not effectively correspond with generative predictions. The outcomes of our study can enhance the understanding of LLM evaluation methodologies and provide insights for future research in this domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "2145209409",
                    "name": "Minghao Wu"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                }
            ]
        },
        {
            "paperId": "d9867698618a242687a2185e8f53c5732b6ab387",
            "title": "Semantic Enrichment for Video Question Answering with Gated Graph Neural Networks",
            "abstract": "Video Question Answering (VideoQA) is a complex task that requires a deep understanding of a video to accurately answer questions. Existing methods often struggle to effectively integrate the visual and language-based semantic information, subsequently leading to an incomplete understanding of video content and sub-optimal performance. To address the challenge, we introduce a novel approach in this paper to enrich the semantics of video frames, questions, and answer candidates. Specifically, we parse video frames and questions into semantic graphs - visual semantic graph and question semantic graph, which captures information about objects, their attributes, and relationships. These graphs are then encoded using a Gated Graph Neural Network (GGNN). For answer candidates, we propose to verbalize them using Large Language Models (LLMs) to further inject more semantic information from visual and acoustic aspects. We evaluate our approach on benchmark VideoQA datasets: AVQA and Music-AVQA. Experimental results show that our approach outperforms competitive baseline models, achieving state-of-the-art performance on various question types.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "2248030941",
                    "name": "Wenxi Li"
                },
                {
                    "authorId": "8022678",
                    "name": "Tianbo Ji"
                },
                {
                    "authorId": "2292209874",
                    "name": "Yi Yu"
                },
                {
                    "authorId": "2111542852",
                    "name": "Longyue Wang"
                }
            ]
        },
        {
            "paperId": "11e043f6846e4b82f4473c391a066726af0f382e",
            "title": "GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation",
            "abstract": "While the recent advances in Multimodal Large Language Models (MLLMs) constitute a significant leap forward in the field, these models are predominantly confined to the realm of input-side multimodal comprehension, lacking the capacity for multimodal content generation. To fill this gap, we present GPT4Video, a unified multi-model framework that empowers Large Language Models (LLMs) with the capability of both video understanding and generation. Specifically, we develop an instruction-following-based approach integrated with the stable diffusion generative model, which has demonstrated to effectively and securely handle video generation scenarios. GPT4Video offers the following benefits: 1) It exhibits impressive capabilities in both video understanding and generation scenarios. For example, GPT4Video outperforms Valley by 11.8\\% on the Video Question Answering task, and surpasses NExt-GPT by 2.3\\% on the Text to Video generation task. 2) it endows the LLM/MLLM with video generation capabilities without requiring additional training parameters and can flexibly interface with a wide range of models to perform video generation. 3) it maintains a safe and healthy conversation not only in output-side but also the input side in an end-to-end manner. Qualitative and qualitative experiments demonstrate that GPT4Video holds the potential to function as a effective, safe and Humanoid-like video assistant that can handle both video understanding and generation scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268638120",
                    "name": "Zhanyu Wang"
                },
                {
                    "authorId": "2111542852",
                    "name": "Longyue Wang"
                },
                {
                    "authorId": "2261892943",
                    "name": "Zhen Zhao"
                },
                {
                    "authorId": "2145209409",
                    "name": "Minghao Wu"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "91956362",
                    "name": "Huayang Li"
                },
                {
                    "authorId": "2266753374",
                    "name": "Deng Cai"
                },
                {
                    "authorId": "2268723641",
                    "name": "Luping Zhou"
                },
                {
                    "authorId": "2239383048",
                    "name": "Shuming Shi"
                },
                {
                    "authorId": "2256685351",
                    "name": "Zhaopeng Tu"
                }
            ]
        },
        {
            "paperId": "34802b1f153d436d5ddb428642b8ae415485269e",
            "title": "Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models",
            "abstract": "The advancement of Large Language Models (LLMs) has brought substantial attention to the Chain of Thought (CoT) approach, primarily due to its ability to enhance the capability of LLMs on complex reasoning tasks. Moreover, the significance of CoT approaches extends to the application of LLMs for multi-modal tasks. However, the selection of optimal CoT demonstration examples in multi-modal reasoning remains less explored for LLMs due to the inherent complexity of multi-modal examples. In this paper, we introduce a novel approach that addresses this challenge by using retrieval mechanisms to dynamically and automatically select demonstration examples based on cross-modal and intra-modal similarities. Furthermore, we employ a Stratified Sampling method of categorising demonstration examples into groups based on their types and then retrieving examples from different groups respectively to promote the diversity of demonstration examples. Through a series of experiments on two popular benchmark datasets: ScienceQA and MathVista, we demonstrate that our approach significantly improves the performance of GPT-4 by 6% on ScienceQA and 12.9% on MathVista, and enhances the performance of GPT-4V on two datasets by 2.7%, substantially improving the performance of the most advanced LLMs and LMMs for complex multi-modal reasoning tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2269484821",
                    "name": "Bingshuai Liu"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "2192607321",
                    "name": "Zijun Min"
                },
                {
                    "authorId": "2268638120",
                    "name": "Zhanyu Wang"
                },
                {
                    "authorId": "2269468248",
                    "name": "Jinsong Su"
                },
                {
                    "authorId": "2269690569",
                    "name": "Longyue Wang"
                }
            ]
        },
        {
            "paperId": "8ac7df2d11170b0777b2d913d4a4b4887e127731",
            "title": "A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual Question Answering",
            "abstract": "The emergence of multimodal large models (MLMs) has significantly advanced the field of visual understanding, offering remarkable capabilities in the realm of visual question answering (VQA). Yet, the true challenge lies in the domain of knowledge-intensive VQA tasks, which necessitate not just recognition of visual elements, but also a deep comprehension of the visual information in conjunction with a vast repository of learned knowledge. To uncover such capabilities of MLMs, particularly the newly introduced GPT-4V and Gemini, we provide an in-depth evaluation from three perspectives: 1) Commonsense Knowledge, which assesses how well models can understand visual cues and connect to general knowledge; 2) Fine-grained World Knowledge, which tests the model's skill in reasoning out specific knowledge from images, showcasing their proficiency across various specialized fields; 3) Comprehensive Knowledge with Decision-making Rationales, which examines model's capability to provide logical explanations for its inference, facilitating a deeper analysis from the interpretability perspective. Additionally, we utilize a visual knowledge-enhanced training strategy and multimodal retrieval-augmented generation approach to enhance MLMs, highlighting the future need for advancements in this research direction. Extensive experiments indicate that: a) GPT-4V demonstrates enhanced explanation generation when using composite images as few-shots; b) GPT-4V and other MLMs produce severe hallucinations when dealing with world knowledge; c) Visual knowledge enhanced training and prompting technicals present potential to improve performance. Codes: https://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118046679",
                    "name": "Yunxin Li"
                },
                {
                    "authorId": "2111542852",
                    "name": "Longyue Wang"
                },
                {
                    "authorId": "33968873",
                    "name": "Baotian Hu"
                },
                {
                    "authorId": "2266425853",
                    "name": "Xinyu Chen"
                },
                {
                    "authorId": "2266437328",
                    "name": "Wanqi Zhong"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "2258690227",
                    "name": "Min Zhang"
                }
            ]
        }
    ]
}