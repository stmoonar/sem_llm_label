{
    "authorId": "73329314",
    "papers": [
        {
            "paperId": "4008f607e29cfe6c0cce0b5ae119827380b99031",
            "title": "PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous Link Prediction",
            "abstract": "Transparency and accountability have become major concerns for black-box machine learning (ML) models. Proper explanations for the model behavior increase model transparency and help researchers develop more accountable models. Graph neural networks (GNN) have recently shown superior performance in many graph ML problems than traditional methods, and explaining them has attracted increased interest. However, GNN explanation for link prediction (LP) is lacking in the literature. LP is an essential GNN task and corresponds to web applications like recommendation and sponsored search on web. Given existing GNN explanation methods only address node/graph-level tasks, we propose Path-based GNN Explanation for heterogeneous Link prediction (PaGE-Link) that generates explanations with connection interpretability, enjoys model scalability, and handles graph heterogeneity. Qualitatively, PaGE-Link can generate explanations as paths connecting a node pair, which naturally captures connections between the two nodes and easily transfer to human-interpretable explanations. Quantitatively, explanations generated by PaGE-Link improve AUC for recommendation on citation and user-item graphs by 9 - 35% and are chosen as better by 78.79% of responses in human evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145408511",
                    "name": "Shichang Zhang"
                },
                {
                    "authorId": "73329314",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "2118943843",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "2109461904",
                    "name": "Yizhou Sun"
                }
            ]
        },
        {
            "paperId": "5bf14dda76156d62a9b3b9ff59dba90ff7b9923d",
            "title": "OrthoReg: Improving Graph-regularized MLPs via Orthogonality Regularization",
            "abstract": "Graph Neural Networks (GNNs) are currently dominating in modeling graph-structure data, while their high reliance on graph structure for inference significantly impedes them from widespread applications. By contrast, Graph-regularized MLPs (GR-MLPs) implicitly inject the graph structure information into model weights, while their performance can hardly match that of GNNs in most tasks. This motivates us to study the causes of the limited performance of GR-MLPs. In this paper, we first demonstrate that node embeddings learned from conventional GR-MLPs suffer from dimensional collapse, a phenomenon in which the largest a few eigenvalues dominate the embedding space, through empirical observations and theoretical analysis. As a result, the expressive power of the learned node representations is constrained. We further propose OrthoReg, a novel GR-MLP model to mitigate the dimensional collapse issue. Through a soft regularization loss on the correlation matrix of node embeddings, OrthoReg explicitly encourages orthogonal node representations and thus can naturally avoid dimensionally collapsed representations. Experiments on traditional transductive semi-supervised classification tasks and inductive node classification for cold-start scenarios demonstrate its effectiveness and superiority.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35466544",
                    "name": "Hengrui Zhang"
                },
                {
                    "authorId": "2151226033",
                    "name": "Shen Wang"
                },
                {
                    "authorId": "40043851",
                    "name": "V. Ioannidis"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "73329314",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "2099585332",
                    "name": "Xiao Qin"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                },
                {
                    "authorId": "152297693",
                    "name": "Philip S. Yu"
                }
            ]
        },
        {
            "paperId": "c7fd29fdd2e0b50a571db4f607eab138e9ecb644",
            "title": "MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding",
            "abstract": "A large number of real-world graphs or networks are inherently heterogeneous, involving a diversity of node types and relation types. Heterogeneous graph embedding is to embed rich structural and semantic information of a heterogeneous graph into low-dimensional node representations. Existing models usually define multiple metapaths in a heterogeneous graph to capture the composite relations and guide neighbor selection. However, these models either omit node content features, discard intermediate nodes along the metapath, or only consider one metapath. To address these three limitations, we propose a new model named Metapath Aggregated Graph Neural Network (MAGNN) to boost the final performance. Specifically, MAGNN employs three major components, i.e., the node content transformation to encapsulate input node attributes, the intra-metapath aggregation to incorporate intermediate semantic nodes, and the inter-metapath aggregation to combine messages from multiple metapaths. Extensive experiments on three real-world heterogeneous graph datasets for node classification, node clustering, and link prediction show that MAGNN achieves more accurate prediction results than state-of-the-art baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "15473346",
                    "name": "Xinyu Fu"
                },
                {
                    "authorId": "73329314",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "1491449344",
                    "name": "Ziqiao Meng"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                }
            ]
        },
        {
            "paperId": "a3c6926a1d90385b746a16cbb9f0ad6fe714dc1c",
            "title": "STAR-GCN: Stacked and Reconstructed Graph Convolutional Networks for Recommender Systems",
            "abstract": "We propose a new STAcked and Reconstructed Graph Convolutional Networks (STAR-GCN) architecture to learn node representations for boosting the performance in recommender systems, especially in the cold start scenario. STAR-GCN employs a stack of GCN encoder-decoders combined with intermediate supervision to improve the final prediction performance. Unlike the graph convolutional matrix completion model with one-hot encoding node inputs, our STAR-GCN learns low-dimensional user and item latent factors as the input to restrain the model space complexity. Moreover, our STAR-GCN can produce node embeddings for new nodes by reconstructing masked input node embeddings, which essentially tackles the cold start problem. Furthermore, we discover a label leakage issue when training GCN-based models for link prediction tasks and propose a training strategy to avoid the issue. Empirical results on multiple rating prediction benchmarks demonstrate our model achieves state-of-the-art performance in four out of five real-world datasets and significant improvements in predicting ratings in the cold start scenario. The code implementation is available in https://github.com/jennyzhang0215/STAR-GCN.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "73329314",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "3008587",
                    "name": "Xingjian Shi"
                },
                {
                    "authorId": "2407526",
                    "name": "Shenglin Zhao"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                }
            ]
        },
        {
            "paperId": "19957eb68dff3e6122f2d0802917dea7006dd48b",
            "title": "Title-Guided Encoding for Keyphrase Generation",
            "abstract": "Keyphrase generation (KG) aims to generate a set of keyphrases given a document, which is a fundamental task in natural language processing (NLP). Most previous methods solve this problem in an extractive manner, while recently, several attempts are made under the generative setting using deep neural networks. However, the state-of-the-art generative methods simply treat the document title and the document main body equally, ignoring the leading role of the title to the overall document. To solve this problem, we introduce a new model called Title-Guided Network (TG-Net) for automatic keyphrase generation task based on the encoderdecoder architecture with two new features: (i) the title is additionally employed as a query-like input, and (ii) a titleguided encoder gathers the relevant information from the title to each word in the document. Experiments on a range of KG datasets demonstrate that our model outperforms the state-of-the-art models with a large margin, especially for documents with either very low or very high title length ratios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50504105",
                    "name": "Wang Chen"
                },
                {
                    "authorId": "1921742",
                    "name": "Yifan Gao"
                },
                {
                    "authorId": "73329314",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                },
                {
                    "authorId": "1785083",
                    "name": "Michael R. Lyu"
                }
            ]
        },
        {
            "paperId": "c097be22f1e87a846385047346b73610d91fea4e",
            "title": "GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs",
            "abstract": "We propose a new network architecture, Gated Attention Networks (GaAN), for learning on graphs. Unlike the traditional multi-head attention mechanism, which equally consumes all attention heads, GaAN uses a convolutional sub-network to control each attention head's importance. We demonstrate the effectiveness of GaAN on the inductive node classification problem. Moreover, with GaAN as a building block, we construct the Graph Gated Recurrent Unit (GGRU) to address the traffic speed forecasting problem. Extensive experiments on three real-world datasets show that our GaAN framework achieves state-of-the-art results on both tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "73329314",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "3008587",
                    "name": "Xingjian Shi"
                },
                {
                    "authorId": "2369548",
                    "name": "Junyuan Xie"
                },
                {
                    "authorId": "144988795",
                    "name": "Hao Ma"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                },
                {
                    "authorId": "1739816",
                    "name": "D. Yeung"
                }
            ]
        },
        {
            "paperId": "cf3dfce20514078413ea911164c872bf85a4d80a",
            "title": "Aspect-level Sentiment Classification with HEAT (HiErarchical ATtention) Network",
            "abstract": "Aspect-level sentiment classification is a fine-grained sentiment analysis task, which aims to predict the sentiment of a text in different aspects. One key point of this task is to allocate the appropriate sentiment words for the given aspect.Recent work exploits attention neural networks to allocate sentiment words and achieves the state-of-the-art performance. However, the prior work only attends to the sentiment information and ignores the aspect-related information in the text, which may cause mismatching between the sentiment words and the aspects when an unrelated sentiment word is semantically meaningful for the given aspect. To solve this problem, we propose a HiErarchical ATtention (HEAT) network for aspect-level sentiment classification. The HEAT network contains a hierarchical attention module, consisting of aspect attention and sentiment attention. The aspect attention extracts the aspect-related information to guide the sentiment attention to better allocate aspect-specific sentiment words of the text. Moreover, the HEAT network supports to extract the aspect terms together with aspect-level sentiment classification by introducing the Bernoulli attention mechanism. To verify the proposed method, we conduct experiments on restaurant and laptop review data sets from SemEval at both the sentence level and the review level. The experimental results show that our model better allocates appropriate sentiment expressions for a given aspect benefiting from the guidance of aspect terms. Moreover, our method achieves better performance on aspect-level sentiment classification than state-of-the-art models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "71881350",
                    "name": "Jiajun Cheng"
                },
                {
                    "authorId": "2407526",
                    "name": "Shenglin Zhao"
                },
                {
                    "authorId": "73329314",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                },
                {
                    "authorId": "145863025",
                    "name": "Xin Zhang"
                },
                {
                    "authorId": "2359832",
                    "name": "Hongya Wang"
                }
            ]
        },
        {
            "paperId": "5d5cde7a70ff3ed608704d81e3a8174b13edb5ae",
            "title": "A randomized tensor singular value decomposition based on the t\u2010product",
            "abstract": "The tensor SVD (t\u2010SVD) for third\u2010order tensors, previously proposed in the literature, has been applied successfully in many fields, such as computed tomography, facial recognition, and video completion. In this paper, we propose a method that extends a well\u2010known randomized matrix method to the t\u2010SVD. This method can produce a factorization with similar properties to the t\u2010SVD, but it is more computationally efficient on very large data sets. We present details of the algorithms and theoretical results and provide numerical results that show the promise of our approach for compressing and analyzing image\u2010based data sets. We also present an improved analysis of the randomized and simultaneous iteration for matrices, which may be of independent interest to the scientific community. We also use these new results to address the convergence properties of the new and randomized tensor method as well.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "73329314",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "144787792",
                    "name": "A. Saibaba"
                },
                {
                    "authorId": "145614783",
                    "name": "M. Kilmer"
                },
                {
                    "authorId": "1980683",
                    "name": "S. Aeron"
                }
            ]
        },
        {
            "paperId": "c96bdf1fe9a43e0a1af9459b9572eb1f69527755",
            "title": "Dynamic Key-Value Memory Networks for Knowledge Tracing",
            "abstract": "Knowledge Tracing (KT) is a task of tracing evolving knowledge state of students with respect to one or more concepts as they engage in a sequence of learning activities. One important purpose of KT is to personalize the practice sequence to help students learn knowledge concepts efficiently. However, existing methods such as Bayesian Knowledge Tracing and Deep Knowledge Tracing either model knowledge state for each predefined concept separately or fail to pinpoint exactly which concepts a student is good at or unfamiliar with. To solve these problems, this work introduces a new model called Dynamic Key-Value Memory Networks (DKVMN) that can exploit the relationships between underlying concepts and directly output a student's mastery level of each concept. Unlike standard memory-augmented neural networks that facilitate a single memory matrix or two static memory matrices, our model has one static matrix called key, which stores the knowledge concepts and the other dynamic matrix called value, which stores and updates the mastery levels of corresponding concepts. Experiments show that our model consistently outperforms the state-of-the-art model in a range of KT datasets. Moreover, the DKVMN model can automatically discover underlying concepts of exercises typically performed by human annotations and depict the changing knowledge state of a student.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "73329314",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "3008587",
                    "name": "Xingjian Shi"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                },
                {
                    "authorId": "1739816",
                    "name": "D. Yeung"
                }
            ]
        },
        {
            "paperId": "cd4b01a0d5549df50661ce36cd4417ec259ae4cf",
            "title": "Dynamic Key-Value Memory Network for Knowledge Tracing",
            "abstract": "The goal of knowledge tracing is to model students\u2019 mastering levels of underlying knowledge concepts, termed knowledge state , based on students\u2019 exercise performance data. However, existing methods, such as Bayesian Knowledge Tracing (BKT) or Deep Knowledge Tracing (DKT), either require costly human-labeled concept annotations or fail to exactly pinpoint which concepts a student is good at or unfamiliar with. To solve these problems, in this paper we introduce a new model called Dynamic Key-Value Memory Network (DKVMN) that can learn representations us-ing nonlinear transformations and directly output a student\u2019s mastering level of each concept. Unlike standard Memory-Augmented Neural Networks (MANNs) that facilitate a single memory matrix or two static memory matrices, our model has one static matrix called key that stores the knowledge concepts and the other dynamic matrix called value that stores and updates corresponding concepts\u2019 mastery levels. Experiments show that our DKVMN model, which is trained end-to-end, consistently outperforms the state-of-the-art model on a range of knowledge tracing data-sets. We also illustrate that the learned DKVMN can automatically discover underlying concepts of the exercises which are typically performed by human annotations, and depict a student\u2019s changing knowledge state.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "73329314",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "3008587",
                    "name": "Xingjian Shi"
                },
                {
                    "authorId": "2257174543",
                    "name": "Irwin King"
                },
                {
                    "authorId": "1739816",
                    "name": "D. Yeung"
                }
            ]
        }
    ]
}