{
    "authorId": "2216486213",
    "papers": [
        {
            "paperId": "1610e78380da2890a081892ac52a9c57f2b8e01f",
            "title": "A SMART Mnemonic Sounds like \"Glue Tonic\": Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick",
            "abstract": "Keyword mnemonics are memorable explanations that link new terms to simpler keywords. Prior work generates mnemonics for students, but they do not train models using mnemonics students prefer and aid learning. We build SMART, a mnemonic generator trained on feedback from real students learning new terms. To train SMART, we first fine-tune LLaMA-2 on a curated set of user-written mnemonics. We then use LLM alignment to enhance SMART: we deploy mnemonics generated by SMART in a flashcard app to find preferences on mnemonics students favor. We gather 2684 preferences from 45 students across two types: expressed (inferred from ratings) and observed (inferred from student learning), yielding three key findings. First, expressed and observed preferences disagree; what students think is helpful does not always capture what is truly helpful. Second, Bayesian models can synthesize complementary data from multiple preference types into a single effectiveness signal. SMART is tuned via Direct Preference Optimization on this signal, which resolves ties and missing labels in the typical method of pairwise comparisons, augmenting data for LLM output quality gains. Third, mnemonic experts assess SMART as matching GPT-4 at much lower deployment costs, showing the utility of capturing diverse student feedback to align LLMs in education.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216486213",
                    "name": "Nishant Balepur"
                },
                {
                    "authorId": "2138314131",
                    "name": "Matthew Shu"
                },
                {
                    "authorId": "2307915196",
                    "name": "Alexander Hoyle"
                },
                {
                    "authorId": "2307914904",
                    "name": "Alison Robey"
                },
                {
                    "authorId": "2284701194",
                    "name": "Shi Feng"
                },
                {
                    "authorId": "1405375772",
                    "name": "Seraphina Goldfarb-Tarrant"
                },
                {
                    "authorId": "2260114875",
                    "name": "Jordan L. Boyd-Graber"
                }
            ]
        },
        {
            "paperId": "2ecf11920fbc331333520a46711df2322b77f8b6",
            "title": "Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?",
            "abstract": "Multiple-choice question answering (MCQA) is often used to evaluate large language models (LLMs). To see if MCQA assesses LLMs as intended, we probe if LLMs can perform MCQA with choices-only prompts, where models must select the correct answer only from the choices. In three MCQA datasets and four LLMs, this prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy gain. To help explain this behavior, we conduct an in-depth, black-box analysis on memorization, choice dynamics, and question inference. Our key findings are threefold. First, we find no evidence that the choices-only accuracy stems from memorization alone. Second, priors over individual choices do not fully explain choices-only accuracy, hinting that LLMs use the group dynamics of choices. Third, LLMs have some ability to infer a relevant question from choices, and surprisingly can sometimes even match the original question. Inferring the original question is an impressive reasoning strategy, but it cannot fully explain the high choices-only accuracy of LLMs in MCQA. Thus, while LLMs are not fully incapable of reasoning in MCQA, we still advocate for the use of stronger baselines in MCQA benchmarks, the design of robust MCQA datasets for fair evaluations, and further efforts to explain LLM decision-making.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216486213",
                    "name": "Nishant Balepur"
                },
                {
                    "authorId": "3023068",
                    "name": "Abhilasha Ravichander"
                },
                {
                    "authorId": "2034613",
                    "name": "Rachel Rudinger"
                }
            ]
        },
        {
            "paperId": "6052b2bc64be4f3b4cab61ae16725e5e1306b77e",
            "title": "The Prompt Report: A Systematic Survey of Prompting Techniques",
            "abstract": "Generative Artificial Intelligence (GenAI) systems are being increasingly deployed across all parts of industry and research settings. Developers and end users interact with these systems through the use of prompting or prompt engineering. While prompting is a widespread and highly researched concept, there exists conflicting terminology and a poor ontological understanding of what constitutes a prompt due to the area's nascency. This paper establishes a structured understanding of prompts, by assembling a taxonomy of prompting techniques and analyzing their use. We present a comprehensive vocabulary of 33 vocabulary terms, a taxonomy of 58 text-only prompting techniques, and 40 techniques for other modalities. We further present a meta-analysis of the entire literature on natural language prefix-prompting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273678394",
                    "name": "Sander Schulhoff"
                },
                {
                    "authorId": "2305681554",
                    "name": "Michael Ilie"
                },
                {
                    "authorId": "2216486213",
                    "name": "Nishant Balepur"
                },
                {
                    "authorId": "2305682316",
                    "name": "Konstantine Kahadze"
                },
                {
                    "authorId": "2218163782",
                    "name": "Amanda Liu"
                },
                {
                    "authorId": "2260339100",
                    "name": "Chenglei Si"
                },
                {
                    "authorId": "2305733815",
                    "name": "Yinheng Li"
                },
                {
                    "authorId": "2305747427",
                    "name": "Aayush Gupta"
                },
                {
                    "authorId": "2269761795",
                    "name": "HyoJung Han"
                },
                {
                    "authorId": "2305681441",
                    "name": "Sevien Schulhoff"
                },
                {
                    "authorId": "2283846803",
                    "name": "P. S. Dulepet"
                },
                {
                    "authorId": "2305682102",
                    "name": "Saurav Vidyadhara"
                },
                {
                    "authorId": "2218391299",
                    "name": "Dayeon Ki"
                },
                {
                    "authorId": "5112699",
                    "name": "Sweta Agrawal"
                },
                {
                    "authorId": "2264486147",
                    "name": "Chau Minh Pham"
                },
                {
                    "authorId": "151011877",
                    "name": "Gerson C. Kroiz"
                },
                {
                    "authorId": "2306393122",
                    "name": "Feileen Li"
                },
                {
                    "authorId": "2305666064",
                    "name": "Hudson Tao"
                },
                {
                    "authorId": "2305768655",
                    "name": "Ashay Srivastava"
                },
                {
                    "authorId": "2305682268",
                    "name": "Hevander Da Costa"
                },
                {
                    "authorId": "2306060338",
                    "name": "Saloni Gupta"
                },
                {
                    "authorId": "2262014537",
                    "name": "Megan L. Rogers"
                },
                {
                    "authorId": "2219399081",
                    "name": "Inna Goncearenco"
                },
                {
                    "authorId": "2271416237",
                    "name": "Giuseppe Sarli"
                },
                {
                    "authorId": "117732368",
                    "name": "I. Galynker"
                },
                {
                    "authorId": "2221286885",
                    "name": "Denis Peskoff"
                },
                {
                    "authorId": "2954727",
                    "name": "Marine Carpuat"
                },
                {
                    "authorId": "2306067799",
                    "name": "Jules White"
                },
                {
                    "authorId": "2275246437",
                    "name": "Shyamal Anadkat"
                },
                {
                    "authorId": "49462969",
                    "name": "Alexander Miserlis Hoyle"
                },
                {
                    "authorId": "2262217773",
                    "name": "Philip Resnik"
                }
            ]
        },
        {
            "paperId": "a41fc0be1e63dc1a272ba3fbf245efd6d3d04d69",
            "title": "KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students",
            "abstract": "Flashcard schedulers rely on 1) student models to predict the flashcards a student knows; and 2) teaching policies to pick which cards to show next via these predictions. Prior student models, however, just use study data like the student's past responses, ignoring the text on cards. We propose content-aware scheduling, the first schedulers exploiting flashcard content. To give the first evidence that such schedulers enhance student learning, we build KARL, a simple but effective content-aware student model employing deep knowledge tracing (DKT), retrieval, and BERT to predict student recall. We train KARL by collecting a new dataset of 123,143 study logs on diverse trivia questions. KARL bests existing student models in AUC and calibration error. To ensure our improved predictions lead to better student learning, we create a novel delta-based teaching policy to deploy KARL online. Based on 32 study paths from 27 users, KARL improves learning efficiency over SOTA, showing KARL's strength and encouraging researchers to look beyond historical study data to fully capture student abilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2138314131",
                    "name": "Matthew Shu"
                },
                {
                    "authorId": "2216486213",
                    "name": "Nishant Balepur"
                },
                {
                    "authorId": "2284701194",
                    "name": "Shi Feng"
                },
                {
                    "authorId": "2260114875",
                    "name": "Jordan L. Boyd-Graber"
                }
            ]
        },
        {
            "paperId": "c11e7f5fb8f85da8a537c0207bf977898dca6ecb",
            "title": "Is Your Large Language Model Knowledgeable or a Choices-Only Cheater?",
            "abstract": "Recent work shows that large language models (LLMs) can answer multiple-choice questions using only the choices, but does this mean that MCQA leaderboard rankings of LLMs are largely influenced by abilities in choices-only settings? To answer this, we use a contrast set that probes if LLMs over-rely on choices-only shortcuts in MCQA. While previous works build contrast sets via expensive human annotations or model-generated data which can be biased, we employ graph mining to extract contrast sets from existing MCQA datasets. We use our method on UnifiedQA, a group of six commonsense reasoning datasets with high choices-only accuracy, to build an 820-question contrast set. After validating our contrast set, we test 12 LLMs, finding that these models do not exhibit reliance on choice-only shortcuts when given both the question and choices. Thus, despite the susceptibility of MCQA to high choices-only accuracy, we argue that LLMs are not obtaining high ranks on MCQA leaderboards solely due to their ability to exploit choices-only shortcuts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216486213",
                    "name": "Nishant Balepur"
                },
                {
                    "authorId": "2034613",
                    "name": "Rachel Rudinger"
                }
            ]
        },
        {
            "paperId": "37b115139dbeff3ffbcb9927d5383084443c27e2",
            "title": "Expository Text Generation: Imitate, Retrieve, Paraphrase",
            "abstract": "Expository documents are vital resources for conveying complex information to readers. Despite their usefulness, writing expository text by hand is a challenging process that requires careful content planning, obtaining facts from multiple sources, and the ability to clearly synthesize these facts. To ease these burdens, we propose the task of expository text generation, which seeks to automatically generate an accurate and stylistically consistent expository text for a topic by intelligently searching a knowledge source. We solve our task by developing IRP, a framework that overcomes the limitations of retrieval-augmented models and iteratively performs content planning, fact retrieval, and rephrasing. Through experiments on three diverse, newly-collected datasets, we show that IRP produces factual and organized expository texts that accurately inform readers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216486213",
                    "name": "Nishant Balepur"
                },
                {
                    "authorId": "1490651934",
                    "name": "Jie Huang"
                },
                {
                    "authorId": "143922493",
                    "name": "K. Chang"
                }
            ]
        },
        {
            "paperId": "74c58985723a439df4736a1c7f885a68aec73c62",
            "title": "Text Fact Transfer",
            "abstract": "Text style transfer is a prominent task that aims to control the style of text without inherently changing its factual content. To cover more text modification applications, such as adapting past news for current events and repurposing educational materials, we propose the task of text fact transfer, which seeks to transfer the factual content of a source text between topics without modifying its style. We find that existing language models struggle with text fact transfer, due to their inability to preserve the specificity and phrasing of the source text, and tendency to hallucinate errors. To address these issues, we design ModQGA, a framework that minimally modifies a source text with a novel combination of end-to-end question generation and specificity-aware question answering. Through experiments on four existing datasets adapted for text fact transfer, we show that ModQGA can accurately transfer factual content without sacrificing the style of the source text.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216486213",
                    "name": "Nishant Balepur"
                },
                {
                    "authorId": "1490651934",
                    "name": "Jie Huang"
                },
                {
                    "authorId": "2260292765",
                    "name": "Kevin Chen-Chuan Chang"
                }
            ]
        },
        {
            "paperId": "cd9f3efbe5995855e646ea1cfd368a3ed067d1a3",
            "title": "It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning",
            "abstract": "Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE), when used with COT, can enhance self-consistency, interpretability, and tasks such as medical diagnoses of exclusion. Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct error analyses and give suggestions for future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216486213",
                    "name": "Nishant Balepur"
                },
                {
                    "authorId": "2166311031",
                    "name": "Shramay Palta"
                },
                {
                    "authorId": "2034613",
                    "name": "Rachel Rudinger"
                }
            ]
        },
        {
            "paperId": "d72ff44e3094dd9466b66d0522aff5dd1cf6b614",
            "title": "Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation",
            "abstract": "When answering complex questions, large language models (LLMs) may produce answers that do not satisfy all criteria of the question. While existing self-evaluation techniques aim to detect if such answers are correct, these techniques are unable to determine which criteria of the question are satisfied by the generated answers. To address this issue, we propose answer-based claim decomposition (ABCD), a prompting strategy that decomposes questions into a series of true/false claims that can be used to verify which criteria of the input question an answer satisfies. Using the decomposed ABCD claims, we perform fine-grained self-evaluation. Through preliminary experiments on three datasets, including a newly-collected challenge dataset ObscureQA, we find that GPT-3.5 has some ability to determine to what extent its answer satisfies the criteria of the input question, and can give insights into the errors and knowledge gaps of the model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216486213",
                    "name": "Nishant Balepur"
                },
                {
                    "authorId": "1490651934",
                    "name": "Jie Huang"
                },
                {
                    "authorId": "2202545203",
                    "name": "Samraj Moorjani"
                },
                {
                    "authorId": "1775376",
                    "name": "Hari Sundaram"
                },
                {
                    "authorId": "143922493",
                    "name": "K. Chang"
                }
            ]
        },
        {
            "paperId": "ec9e63863b02626129607dd0e67b8cabc72ed84c",
            "title": "DynaMiTE: Discovering Explosive Topic Evolutions with User Guidance",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216486213",
                    "name": "Nishant Balepur"
                },
                {
                    "authorId": "1923351",
                    "name": "Shivam Agarwal"
                },
                {
                    "authorId": "2223160493",
                    "name": "Karthik Venkat Ramanan"
                },
                {
                    "authorId": "3396235",
                    "name": "Susik Yoon"
                },
                {
                    "authorId": "2143919864",
                    "name": "Diyi Yang"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        }
    ]
}