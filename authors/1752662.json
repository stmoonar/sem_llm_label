{
    "authorId": "1752662",
    "papers": [
        {
            "paperId": "2f4d3ab8ce8a37b035bcf9d007b78cd88bf1453d",
            "title": "Alt-Text with Context: Improving Accessibility for Images on Twitter",
            "abstract": "In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. More than just a special case of image captioning, alt-text is both more literally descriptive and context-specific. Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative. We address this task with a multimodal model that conditions on both textual information from the associated social media post as well as visual signal from the image, and demonstrate that the utility of these two information sources stacks. We put forward a new dataset of 371k images paired with alt-text and tweets scraped from Twitter and evaluate on it across a variety of automated metrics as well as human evaluation. We show that our approach of conditioning on both tweet text and visual information significantly outperforms prior work, by more than 2x on BLEU@4.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2122931760",
                    "name": "Nikita Srivatsan"
                },
                {
                    "authorId": "151367676",
                    "name": "Sofia Samaniego"
                },
                {
                    "authorId": "1752662",
                    "name": "Omar U. Florez"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                }
            ]
        },
        {
            "paperId": "160472215dfeee9f79b5da5cc0d5bd6b3bef6a34",
            "title": "Non-Parametric Temporal Adaptation for Social Media Topic Classification",
            "abstract": "User-generated social media data is constantly changing as new trends influence online discussion and personal information is deleted due to privacy concerns. However, most current NLP models are static and rely on fixed training data, which means they are unable to adapt to temporal change -- both test distribution shift and deleted training data -- without frequent, costly re-training. In this paper, we study temporal adaptation through the task of longitudinal hashtag prediction and propose a non-parametric dense retrieval technique, which does not require re-training, as a simple but effective solution. In experiments on a newly collected, publicly available, year-long Twitter dataset exhibiting temporal distribution shift, our method improves by 64.12% over the best parametric baseline without any of its costly gradient-based updating. Our dense retrieval approach is also particularly well-suited to dynamically deleted user data in line with data privacy laws, with negligible computational cost and performance loss.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115471757",
                    "name": "Fatemehsadat Mireshghallah"
                },
                {
                    "authorId": "46623434",
                    "name": "Nikolai Vogler"
                },
                {
                    "authorId": "2109932032",
                    "name": "Junxian He"
                },
                {
                    "authorId": "1752662",
                    "name": "Omar U. Florez"
                },
                {
                    "authorId": "1398503968",
                    "name": "Ahmed El-Kishky"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                }
            ]
        },
        {
            "paperId": "76453afd79968f2de8356beaca5e0468715feab8",
            "title": "Non-Parametric Temporal Adaptation for Social Media Topic Classification",
            "abstract": "User-generated social media data is constantly changing as new trends in\ufb02uence online discussion, causing distribution shift in test data for social media NLP applications. In addition, training data is often subject to change as user data is deleted. Most current NLP systems are static and rely on \ufb01xed training data. As a result, they are unable to adapt to temporal change \u2013 both test distribution shift and deleted training data \u2013 without frequent, costly re-training. In this paper, we study temporal adaptation through the task of longitudinal hash-tag prediction and propose a non-parametric technique as a simple but effective solution: non-parametric classi\ufb01ers use datastores which can be updated, either to adapt to test distribution shift or training data deletion, without re-training. We release a new benchmark dataset comprised of 7 . 13 M Tweets from 2021, along with their hashtags, broken into consecutive temporal buckets. We compare parametric neural hash-tag classi\ufb01cation and hashtag generation models, which need re-training for adaptation, with a non-parametric, training-free dense retrieval method that returns the nearest neighbor\u2019s hashtags based on text embedding distance. In experiments on our longitudinal Twitter dataset we \ufb01nd that dense nearest neighbor retrieval has a relative performance gain of 64 . 12% over the best parametric baseline on test sets that exhibit distribution shift without requiring gradient-based re-training. Furthermore, we show that our datastore approach is particularly well-suited to dynamically deleted user data, with negligible computational cost and performance loss. Our novel benchmark dataset and empirical analysis can support future inquiry into the important challenges presented by temporal-ity in the deployment of AI systems on real-world user data",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115471757",
                    "name": "Fatemehsadat Mireshghallah"
                },
                {
                    "authorId": "46623434",
                    "name": "Nikolai Vogler"
                },
                {
                    "authorId": "2109932032",
                    "name": "Junxian He"
                },
                {
                    "authorId": "1752662",
                    "name": "Omar U. Florez"
                },
                {
                    "authorId": "1398503968",
                    "name": "Ahmed El-Kishky"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                }
            ]
        },
        {
            "paperId": "f5888d776f122f53292973bd3693628ebd265bc6",
            "title": "TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter",
            "abstract": "Pre-trained language models (PLMs) are fundamental for natural language processing applications. Most existing PLMs are not tailored to the noisy user-generated text on social media, and the pre-training does not factor in the valuable social engagement logs available in a social network. We present TwHIN-BERT, a multilingual language model productionized at Twitter, trained on in-domain data from the popular social network. TwHIN-BERT differs from prior pre-trained language models as it is trained with not only text-based self-supervision but also with a social objective based on the rich social engagements within a Twitter heterogeneous information network (TwHIN). Our model is trained on 7 billion tweets covering over 100 distinct languages, providing a valuable representation to model short, noisy, user-generated text. We evaluate our model on various multilingual social recommendation and semantic understanding tasks and demonstrate significant metric improvement over established pre-trained language models. We open-source TwHIN-BERT and our curated hashtag prediction and social engagement benchmark datasets to the research community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108262811",
                    "name": "Xinyang Zhang"
                },
                {
                    "authorId": "2104662",
                    "name": "Yury Malkov"
                },
                {
                    "authorId": "1752662",
                    "name": "Omar U. Florez"
                },
                {
                    "authorId": "152662346",
                    "name": "Serim Park"
                },
                {
                    "authorId": "2953855",
                    "name": "B. McWilliams"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "1398503968",
                    "name": "Ahmed El-Kishky"
                }
            ]
        },
        {
            "paperId": "034799d966cd036aaac0a07c1feec4f2f9192f94",
            "title": "Predicting Number of Hospital Appointments When No Data Is Available",
            "abstract": "Usually, in a hospital, the data generated by each department or section is treated in isolation, believing that there is no relationship between them. It is thought that while one department is in high demand, it can not influence that another may have the same demand or not have any demand. In this paper, we question this approach by considering information from departments as components of a large system in the hospital. Thus, we present an algorithm to predict the appointments of departments when data is not available using data from other departments. This algorithm uses a model based on multiple linear regression using a correlation matrix to measure the rela-tionship between the departments with different time windows. After running our algorithm for different time windows and departments, we experimentally find that while we increase the extension of a time window and learn dependencies in the data, its corresponding precision decreases. Indeed, a month of data is the minimum sweet spot to leverage information from other departments and still provide accurate predictions. These results are important to develop per-department health policies under limited data, an interesting problem that we plan to investigate in future works.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2022960599",
                    "name": "Harold Caceres"
                },
                {
                    "authorId": "2077216519",
                    "name": "Nelson Fuentes"
                },
                {
                    "authorId": "1517537083",
                    "name": "J. Aguilar"
                },
                {
                    "authorId": "2080732208",
                    "name": "Cesar Baluarte"
                },
                {
                    "authorId": "108353658",
                    "name": "Karim Guevara"
                },
                {
                    "authorId": "1412403775",
                    "name": "E. Castro-Guti\u00e9rrez"
                },
                {
                    "authorId": "1752662",
                    "name": "Omar U. Florez"
                }
            ]
        },
        {
            "paperId": "5e8957adb0ce63e1e37ec7599d40ff758c848d3d",
            "title": "Aging Memories Generate More Fluent Dialogue Responses with Memory Augmented Neural Networks.",
            "abstract": "Memory Networks have emerged as effective models to incorporate Knowledge Bases (KB) into neural networks. By storing KB embeddings into a memory component, these models can learn meaningful representations that are grounded to external knowledge. However, as the memory unit becomes full, the oldest memories are replaced by newer representations. In this paper, we question this approach and provide experimental evidence that conventional Memory Networks store highly correlated vectors during training. While increasing the memory size mitigates this problem, this also leads to overfitting as the memory stores a large number of training latent representations. To address these issues, we propose a novel regularization mechanism named memory dropout which 1) Samples a single latent vector from the distribution of redundant memories. 2) Ages redundant memories thus increasing their probability of overwriting them during training. This fully differentiable technique allows us to achieve state-of-the-art response generation in the Stanford Multi-Turn Dialogue and Cambridge Restaurant datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1752662",
                    "name": "Omar U. Florez"
                },
                {
                    "authorId": "102554062",
                    "name": "Erik T. Mueller"
                }
            ]
        },
        {
            "paperId": "4084114115b5e209b727265de5a0617a53be907a",
            "title": "Learning to Control Latent Representations for Few-Shot Learning of Named Entities",
            "abstract": "Humans excel in continuously learning with small data without forgetting how to solve old problems. However, neural networks require large datasets to compute latent representations across different tasks while minimizing a loss function. For example, a natural language understanding (NLU) system will often deal with emerging entities during its deployment as interactions with users in realistic scenarios will generate new and infrequent names, events, and locations. Here, we address this scenario by introducing an RL trainable controller that disentangles the representation learning of a neural encoder from its memory management role. \nOur proposed solution is straightforward and simple: we train a controller to execute an optimal sequence of reading and writing operations on an external memory with the goal of leveraging diverse activations from the past and provide accurate predictions. Our approach is named Learning to Control (LTC) and allows few-shot learning with two degrees of memory plasticity. We experimentally show that our system obtains accurate results for few-shot learning of entity recognition in the Stanford Task-Oriented Dialogue dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1752662",
                    "name": "Omar U. Florez"
                },
                {
                    "authorId": "102554062",
                    "name": "Erik T. Mueller"
                }
            ]
        },
        {
            "paperId": "7ec8fc4d2caaf8104214f66b40e84c9a542be481",
            "title": "On the Unintended Social Bias of Training Language Generation Models with Data from Local Media",
            "abstract": "There are concerns that neural language models may preserve some of the stereotypes of the underlying societies that generate the large corpora needed to train these models. For example, gender bias is a significant problem when generating text, and its unintended memorization could impact the user experience of many applications (e.g., the smart-compose feature in Gmail). \nIn this paper, we introduce a novel architecture that decouples the representation learning of a neural model from its memory management role. This architecture allows us to update a memory module with an equal ratio across gender types addressing biased correlations directly in the latent space. We experimentally show that our approach can mitigate the gender bias amplification in the automatic generation of articles news while providing similar perplexity values when extending the Sequence2Sequence architecture.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1752662",
                    "name": "Omar U. Florez"
                }
            ]
        },
        {
            "paperId": "afaab37cf20adfaa4fc7bd053fc002a7d0b41f57",
            "title": "Aging Memories Generate More Fluent Dialogue Responses with Memory Networks",
            "abstract": "The integration of a Knowledge Base (KB) into a neural dialogue agent is one of the key challenges in Conversational AI. Memory networks has proven to be effective to encode KB information into an external memory to thus generate more fluent and informed responses. Unfortunately, such memory becomes full of latent representations during training, so the most common strategy is to overwrite old memory entries randomly. \nIn this paper, we question this approach and provide experimental evidence showing that conventional memory networks generate many redundant latent vectors resulting in overfitting and the need for larger memories. \nWe introduce memory dropout as an automatic technique that encourages diversity in the latent space by 1) Aging redundant memories to increase their probability of being overwritten during training 2) Sampling new memories that summarize the knowledge acquired by redundant memories. This technique allows us to incorporate Knowledge Bases to achieve state-of-the-art dialogue generation in the Stanford Multi-Turn Dialogue dataset. Considering the same architecture, its use provides an improvement of +2.2 BLEU points for the automatic generation of responses and an increase of +8.1% in the recognition of named entities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1752662",
                    "name": "Omar U. Florez"
                },
                {
                    "authorId": "102554062",
                    "name": "Erik T. Mueller"
                }
            ]
        },
        {
            "paperId": "3cdbce25a30c7b33026872b2dbdfea468362c2a5",
            "title": "Supporting data aspects in pig latin",
            "abstract": "In this paper we apply the aspect-oriented programming (AOP) paradigm to Pig Latin, a dataflow language for cloud computing, used primarily for the analysis of massive data sets. Missing from Pig Latin is support for cross-cutting data concerns. Data, like code, has cross-cutting concerns such as versioning, privacy, and reliability. AOP techniques can be used to weave metadata around Pig data. The metadata imbues the data with additional semantics that must be observed in the evaluation of Pig Latin programs. In this paper we show how to modify Pig Latin to process data woven together with metadata. The data weaver is a layer that maps a Pig Latin program to an augmented Pig Latin program using Pig Latin templates or patterns. We also show how to model additional levels of advice, i.e., meta-metadata.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2286094834",
                    "name": "Curtis E. Dyreson"
                },
                {
                    "authorId": "1752662",
                    "name": "Omar U. Florez"
                },
                {
                    "authorId": "143653499",
                    "name": "A. Thakre"
                },
                {
                    "authorId": "2112608140",
                    "name": "V. Sharma"
                }
            ]
        }
    ]
}