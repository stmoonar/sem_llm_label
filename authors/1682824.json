{
    "authorId": "1682824",
    "papers": [
        {
            "paperId": "52fcda1bfd02b92d946ef22f4dd001c61f7da039",
            "title": "A New Paradigm for Counterfactual Reasoning in Fairness and Recourse",
            "abstract": "Counterfactuals underpin numerous techniques for auditing and understanding artificial intelligence (AI) systems. The traditional paradigm for counterfactual reasoning in this literature is the interventional counterfactual, where hypothetical interventions are imagined and simulated. For this reason, the starting point for causal reasoning about legal protections and demographic data in AI is an imagined intervention on a legally-protected characteristic, such as ethnicity, race, gender, disability, age, etc. We ask, for example, what would have happened had your race been different? An inherent limitation of this paradigm is that some demographic interventions \u2014 like interventions on race \u2014 may not be well-defined or translate into the formalisms of interventional counterfactuals. In this work, we explore a new paradigm based instead on the backtracking counterfactual, where rather than imagine hypothetical interventions on legally-protected characteristics, we imagine alternate initial conditions while holding these characteristics fixed. We ask instead, what would explain a counterfactual outcome for you as you actually are or could be? This alternate framework allows us to address many of the same social concerns, but to do so while asking fundamentally different questions that do not rely on demographic interventions.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "8b9a3f71ad489f61a403847a5851927cdbac6f87",
            "title": "Query Refinement for Diverse Top-$k$ Selection",
            "abstract": "Database queries are often used to select and rank items as decision support for many applications. As automated decision-making tools become more prevalent, there is a growing recognition of the need to diversify their outcomes. In this paper, we define and study the problem of modifying the selection conditions of an ORDER BY query so that the result of the modified query closely fits some user-defined notion of diversity while simultaneously maintaining the intent of the original query. We show the hardness of this problem and propose a Mixed Integer Linear Programming (MILP) based solution. We further present optimizations designed to enhance the scalability and applicability of the solution in real-life scenarios. We investigate the performance characteristics of our algorithm and show its efficiency and the usefulness of our optimizations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293394122",
                    "name": "Felix S. Campbell"
                },
                {
                    "authorId": "2236742952",
                    "name": "Alon Silberstein"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "2330778",
                    "name": "Y. Moskovitch"
                }
            ]
        },
        {
            "paperId": "ba1a77c3a93854461be2fe642b1952eaf3e27bd7",
            "title": "Fairness in Algorithmic Recourse Through the Lens of Substantive Equality of Opportunity",
            "abstract": "Algorithmic recourse -- providing recommendations to those affected negatively by the outcome of an algorithmic system on how they can take action and change that outcome -- has gained attention as a means of giving persons agency in their interactions with artificial intelligence (AI) systems. Recent work has shown that even if an AI decision-making classifier is ``fair'' (according to some reasonable criteria), recourse itself may be unfair due to differences in the initial circumstances of individuals, compounding disparities for marginalized populations and requiring them to exert more effort than others. There is a need to define more methods and metrics for evaluating fairness in recourse that span a range of normative views of the world, and specifically those that take into account time. Time is a critical element in recourse because the longer it takes an individual to act, the more the setting may change due to model or data drift. This paper seeks to close this research gap by proposing two notions of fairness in recourse that are in normative alignment with substantive equality of opportunity, and that consider time. The first considers the (often repeated) effort individuals exert per successful recourse event, and the second considers time per successful recourse event. Building upon an agent-based framework for simulating recourse, this paper demonstrates how much effort is needed to overcome disparities in initial circumstances. We then proposes an intervention to improve the fairness of recourse by rewarding effort, and compare it to existing strategies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2239195821",
                    "name": "Andrew Bell"
                },
                {
                    "authorId": "2239195588",
                    "name": "Jo\u00e3o Fonseca"
                },
                {
                    "authorId": "89449460",
                    "name": "Carlo Abrate"
                },
                {
                    "authorId": "2179558887",
                    "name": "Francesco Bonchi"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "f9b63f79f5b5f6451ff4155103263bade279bb27",
            "title": "Making Transparency Influencers: A Case Study of an Educational Approach to Improve Responsible AI Practices in News and Media",
            "abstract": "Concerns about the risks posed by artificial intelligence (AI) have resulted in growing interest in algorithmic transparency. While algorithmic transparency is well-studied, there is evidence that many organizations do not value implementing transparency. In this case study, we test a ground-up approach to ensuring better real-world algorithmic transparency by creating transparency influencers \u2014 motivated individuals within organizations who advocate for transparency. We held an interactive online workshop on algorithmic transparency and advocacy for 15 professionals from news, media, and journalism. We reflect on workshop design choices and presents insights from participant interviews. We found positive evidence for our approach: In the days following the workshop, three participants had done pro-transparency advocacy. Notably, one of them advocated for algorithmic transparency at an organization-wide AI strategy meeting. In the words of a participant: \u201cif you are questioning whether or not you need to tell people [about AI], you need to tell people.\u201d",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2239195821",
                    "name": "Andrew Bell"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "02dd9ca2145393ac1865dd7c026970a460fafb49",
            "title": "Personal Data for Personal Use: Vision or Reality?",
            "abstract": "The vision of collecting all of one's personal information into one searchable database has been around at least since Vannevar Bush's 1945 paper on the Memex System [2]. In the late 1990's, Gordon Bell and his colleagues at Microsoft Research built MyLifeBits [1, 6], which was the first serious attempt to build such a database. Since then, there has been continued interest in our community to build personal information management systems [3-5, 7, 8, 10]. Recently, the Solid Project proposes a more radical approach to personal information, arguing that all of one's data should reside in their own data pod, and applications should be redesigned to fetch data from the pod [9].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2155883057",
                    "name": "Bo Li"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "1699730",
                    "name": "A. Tung"
                },
                {
                    "authorId": "1751591",
                    "name": "G. Weikum"
                },
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "2131764025",
                    "name": "Wang-Chiew Tan"
                }
            ]
        },
        {
            "paperId": "0385d39a5f32175b9007bd8034d25f9207f87857",
            "title": "Subset Modelling: A Domain Partitioning Strategy for Data-efficient Machine-Learning",
            "abstract": "The success of machine learning (ML) systems depends on data availability, volume, quality, and efficient computing resources. A challenge in this context is to reduce computational costs while maintaining adequate accuracy of the models. This paper presents a framework to address this challenge. The idea is to identify \u201csubdomains\u201d within the input space, train local models that produce better predictions for samples from that specific subdomain, instead of training a single global model on the full dataset. We experimentally evaluate our approach on two real-world datasets. Our results indicate that subset modelling (i) improves the predictive performance compared to a single global model and (ii) allows data-efficient training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2255558225",
                    "name": "Vitor Ribeiro"
                },
                {
                    "authorId": "2255545324",
                    "name": "Eduardo H. M. Pena"
                },
                {
                    "authorId": "2255436137",
                    "name": "Raphael Saldanha"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1410017982",
                    "name": "Falaah Arif Khan"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "2255565502",
                    "name": "F\u00e1bio Porto"
                }
            ]
        },
        {
            "paperId": "08c0353aa13f2ee410dad2accf671f02791b9e47",
            "title": "A Simple and Practical Method for Reducing the Disparate Impact of Differential Privacy",
            "abstract": "Differentially private (DP) mechanisms have been deployed in a variety of high-impact social settings (perhaps most notably by the U.S. Census). Since all DP mechanisms involve adding noise to results of statistical queries, they are expected to impact our ability to accurately analyze and learn from data, in effect trading off privacy with utility. Alarmingly, the impact of DP on utility can vary significantly among different sub-populations. A simple way to reduce this disparity is with stratification. First compute an independent private estimate for each group in the data set (which may be the intersection of several protected classes), then, to compute estimates of global statistics, appropriately recombine these group estimates. Our main observation is that naive stratification often yields high-accuracy estimates of population-level statistics, without the need for additional privacy budget. We support this observation theoretically and empirically. Our theoretical results center on the private mean estimation problem, while our empirical results center on extensive experiments on private data synthesis to demonstrate the effectiveness of stratification on a variety of private mechanisms. Overall, we argue that this straightforward approach provides a strong baseline against which future work on reducing utility disparities of DP mechanisms should be compared.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066297941",
                    "name": "Lucas Rosenblatt"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "2275240652",
                    "name": "Christopher Musco"
                }
            ]
        },
        {
            "paperId": "1825bf69badd79607a2ee8e7c09fdbfdd6291e87",
            "title": "Query Refinement for Diversity Constraint Satisfaction",
            "abstract": "Diversity, group representation, and similar needs often apply to query results, which in turn require constraints on the sizes of various subgroups in the result set. Traditional relational queries only specify conditions as part of the query predicate(s), and do not support such restrictions on the output. In this paper, we study the problem of modifying queries to have the result satisfy constraints on the sizes of multiple subgroups in it. This problem, in the worst case, cannot be solved in polynomial time. Yet, with the help of provenance annotation, we are able to develop a query refinement method that works quite efficiently, as we demonstrate through extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2142943278",
                    "name": "Jinyang Li"
                },
                {
                    "authorId": "2330778",
                    "name": "Y. Moskovitch"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "2270923804",
                    "name": "H. V. Jagadish"
                }
            ]
        },
        {
            "paperId": "333d6631e0f0ad2e96903e8f28fd43d28f189735",
            "title": "Automated Data Cleaning Can Hurt Fairness in Machine Learning-based Decision Making",
            "abstract": "In this paper, we interrogate whether data quality issues track demographic characteristics such as sex, race and age, and whether automated data cleaning \u2014 of the kind commonly used in production ML systems \u2014 impacts the fairness of predictions made by these systems. To the best of our knowledge, the impact of data cleaning on fairness in downstream tasks has not been investigated in the literature.We first analyze the tuples flagged by common error detection strategies in five research datasets. We find that, while specific data quality issues, such as higher rates of missing values, are associated with membership in historically disadvantaged groups, poor data quality does not generally track demographic group membership. As a follow-up, we conduct a large-scale empirical study on the impact of automated data cleaning on fairness, involving more than 26,000 model evaluations on five datasets. We observe that, while automated data cleaning has an insignificant impact on both accuracy and fairness in the majority of cases, it is more likely to worsen fairness than to improve it, especially when the cleaning techniques are not carefully chosen. This finding is both significant and worrying, given that it potentially implicates many production ML systems. We make our code and experimental results publicly available.The analysis we conducted in this paper is difficult, primarily because it requires that we think holistically about disparities in data quality, disparities in the effectiveness of data cleaning methods, and impacts of such disparities on ML model performance for different demographic groups. Such holistic analysis can and should be supported with the help of data engineering research. Towards this goal, we envision the development of fairness-aware data cleaning methods, and their integration into complex pipelines for ML-based decision making.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3375291",
                    "name": "Shubha Guha"
                },
                {
                    "authorId": "1410017982",
                    "name": "Falaah Arif Khan"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "2180399",
                    "name": "Sebastian Schelter"
                }
            ]
        },
        {
            "paperId": "4f3f4d4704a64d6714bfa21693d25ecc106e2fdb",
            "title": "ERICA: Query Refinement for Diversity Constraint Satisfaction",
            "abstract": "Relational queries are commonly used to support decision making in critical domains like hiring and college admissions. For example, a college admissions officer may need to select a subset of the applicants for in-person interviews, who individually meet the qualification requirements (e.g., have a sufficiently high GPA) and are collectively demographically diverse (e.g., include a sufficient number of candidates of each gender and of each race). However, traditional relational queries only support selection conditions checked against each input tuple, and they do not support diversity conditions checked against multiple, possibly overlapping, groups of output tuples. To address this shortcoming, we present Erica, an interactive system that proposes minimal modifications for selection queries to have them satisfy constraints on the cardinalities of multiple groups in the result. We demonstrate the effectiveness of Erica using several real-life datasets and diversity requirements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2142943278",
                    "name": "Jinyang Li"
                },
                {
                    "authorId": "2236742952",
                    "name": "Alon Silberstein"
                },
                {
                    "authorId": "2330778",
                    "name": "Y. Moskovitch"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                }
            ]
        }
    ]
}