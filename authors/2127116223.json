{
    "authorId": "2127116223",
    "papers": [
        {
            "paperId": "f1505b0153099e10e10109bca69bbe1f73467209",
            "title": "G^2SAM: Graph-Based Global Semantic Awareness Method for Multimodal Sarcasm Detection",
            "abstract": "Multimodal sarcasm detection, aiming to detect the ironic sentiment within multimodal social data, has gained substantial popularity in both the natural language processing and computer vision communities. Recently, graph-based studies by drawing sentimental relations to detect multimodal sarcasm have made notable advancements. However, they have neglected exploiting graph-based global semantic congruity from existing instances to facilitate the prediction, which ultimately hinders the model's performance. In this paper, we introduce a new inference paradigm that leverages global graph-based semantic awareness to handle this task. Firstly, we construct fine-grained multimodal graphs for each instance and integrate them into semantic space to draw graph-based relations. During inference, we leverage global semantic congruity to retrieve k-nearest neighbor instances in semantic space as references for voting on the final prediction. To enhance the semantic correlation of representation in semantic space, we also introduce label-aware graph contrastive learning to further improve the performance. Experimental results demonstrate that our model achieves state-of-the-art (SOTA) performance in multimodal sarcasm detection. The code will be available at https://github.com/upccpu/G2SAM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "19261873",
                    "name": "Yiwei Wei"
                },
                {
                    "authorId": "1429199889",
                    "name": "Shaozu Yuan"
                },
                {
                    "authorId": "2293407944",
                    "name": "Hengyang Zhou"
                },
                {
                    "authorId": "2127116223",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2143627170",
                    "name": "Zhiling Yan"
                },
                {
                    "authorId": "2293554459",
                    "name": "Ruosong Yang"
                },
                {
                    "authorId": "2294672309",
                    "name": "Meng Chen"
                }
            ]
        },
        {
            "paperId": "a8c65a6eb594f4e283a82c15a03576097bfe30b4",
            "title": "Tackling Modality Heterogeneity with Multi-View Calibration Network for Multimodal Sentiment Detection",
            "abstract": "With the popularity of social media, detecting sentiment from multimodal posts (e.g. image-text pairs) has attracted substantial attention recently. Existing works mainly focus on fusing different features but ignore the challenge of modality heterogeneity. Specifically, different modalities with inherent disparities may bring three problems: 1) introducing redundant visual features during feature fusion; 2) causing feature shift in the representation space; 3) leading to inconsistent annotations for different modal data. All these issues will increase the difficulty in understanding the sentiment of the multimodal content. In this paper, we propose a novel Multi-View Calibration Network (MVCN) to alleviate the above issues systematically. We first propose a text-guided fusion module with novel Sparse-Attention to reduce the negative impacts of redundant visual elements. We then devise a sentiment-based congruity constraint task to calibrate the feature shift in the representation space. Finally, we introduce an adaptive loss calibration strategy to tackle inconsistent annotated labels. Extensive experiments demonstrate the competitiveness of MVCN against previous approaches and achieve state-of-the-art results on two public benchmark datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "19261873",
                    "name": "Yiwei Wei"
                },
                {
                    "authorId": "1429199889",
                    "name": "Shaozu Yuan"
                },
                {
                    "authorId": "8092850",
                    "name": "Ruosong Yang"
                },
                {
                    "authorId": "80439483",
                    "name": "Lei Shen"
                },
                {
                    "authorId": "2217543048",
                    "name": "Zhang Li"
                },
                {
                    "authorId": "2127116223",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "1745546",
                    "name": "Meng Chen"
                }
            ]
        },
        {
            "paperId": "e14fa05cf95d0a54ac8ff296eef50e5d608392c0",
            "title": "Multi-Modal Sarcasm Detection Based on Cross-Modal Composition of Inscribed Entity Relations",
            "abstract": "Sarcasm, a linguistic technique employed to express emotions opposite to their literal meaning, has garnered significant attention from researchers due to the rise of social media. Detecting sarcasm in a multi-modal context has become a focal point in recent studies. However, existing research primarily relies on identifying inconsistencies between text semantics and image semantics, often lacking a deep understanding of images. Consequently, capturing inconsistencies between images and texts poses a challenge in many cases. In this paper, we propose the Entity-Relational Graph Convolutional Network (ERGCN) as a solution to detect sarcasm by examining the relationship between entities within images. Our approach involves extracting entities and text descriptions from each image, which provides valuable entity information. Subsequently, we employ external knowledge to construct a cross-modal graph for each text and image pair, emphasizing the presence of internal contradictory information. Finally, we utilize the graph convolutional network to identify inconsistent information across modalities and successfully detect sarcasm. Experimental results demonstrate that our model achieves state-of-the-art performance on a widely used multimodal Twitter dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275738704",
                    "name": "Lingshan Li"
                },
                {
                    "authorId": "2221795887",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2275686405",
                    "name": "Fengyu Guo"
                },
                {
                    "authorId": "2127116223",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                }
            ]
        },
        {
            "paperId": "ff98a579cd9030258d52816a8d2fb0fcb120b3ab",
            "title": "Commonsense Knowledge Enhanced Sentiment Dependency Graph for Sarcasm Detection",
            "abstract": "Sarcasm is widely utilized on social media platforms such as Twitter and Reddit. Sarcasm detection is required for analyzing people's true feelings since sarcasm is commonly used to portray a reversed emotion opposing the literal meaning. The syntactic structure is the key to make better use of commonsense when detecting sarcasm. However, it is extremely challenging to effectively and explicitly explore the information implied in syntactic structure and commonsense simultaneously. In this paper, we apply the pre-trained COMET model to generate relevant commonsense knowledge, and explore a novel scenario of constructing a commonsense-augmented sentiment graph and a commonsense-replaced dependency graph for each text. Based on this, a Commonsense Sentiment Dependency Graph Convolutional Network (CSDGCN) framework is proposed to explicitly depict the role of external commonsense and inconsistent expressions over the context for sarcasm detection by interactively modeling the sentiment and dependency information. Experimental results on several benchmark datasets reveal that our proposed method beats the state-of-the-art methods in sarcasm detection, and has a stronger interpretability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2230238348",
                    "name": "Zhe Yu"
                },
                {
                    "authorId": "2221795887",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2222317038",
                    "name": "Yawen Li"
                },
                {
                    "authorId": "2127116223",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "144699460",
                    "name": "J. Dang"
                }
            ]
        },
        {
            "paperId": "27b46889b8dd93f9ba90337694c045c569248acf",
            "title": "Amer: A New Attribute-Missing Network Embedding Approach",
            "abstract": "Network embedding which aims to learn a low dimensional representation of nodes is a powerful technique for network analysis. While network embedding for networks with complete attributes has been widely investigated, in many real-world applications the attributes of partial nodes are unobserved (i.e., missing) due to privacy concern or resource limit. Very recently, several network embedding methods have been proposed for attribute-missing networks. They first complete the missing attributes and then use the complemented network to learn network embedding. The parameters of these two processes cannot be adjusted by each other, resulting in compromised results. To address this problem, we propose a unified model in which the process of completing missing attributes and the process of learning embedding are not separated but closely intertwined. Being specific, completing missing attributes is under the guidance of learning network representation via mutual information maximization, and the complemented attributes directly enter network representation module which will generate further feedback for completing missing attributes. We further impose attribute-structure relationship constraint for completing missing attributes by designing a new generative adversarial networks (GANs) model. To the best of our knowledge, this is the first unified model for attribute-missing network embedding. Empirical results on real-world datasets show the superiority of our new method over other state-of-the-art methods on four network analysis tasks, including node classification, node clustering, link prediction, and network visualization.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2151037901",
                    "name": "Rui Wang"
                },
                {
                    "authorId": null,
                    "name": "Tao Wang"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2163787981",
                    "name": "Weiping Ding"
                },
                {
                    "authorId": "2108728683",
                    "name": "Yuxiao Huang"
                },
                {
                    "authorId": "2127116223",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "1731634",
                    "name": "W. Pedrycz"
                }
            ]
        }
    ]
}