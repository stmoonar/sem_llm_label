{
    "authorId": "34170717",
    "papers": [
        {
            "paperId": "fb4dc0178e5d7347b1615c48caf05347b6e5eb48",
            "title": "TrustLLM: Trustworthiness in Large Language Models",
            "abstract": "Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257131651",
                    "name": "Lichao Sun"
                },
                {
                    "authorId": "2257084278",
                    "name": "Yue Huang"
                },
                {
                    "authorId": "2256769280",
                    "name": "Haoran Wang"
                },
                {
                    "authorId": "2254867423",
                    "name": "Siyuan Wu"
                },
                {
                    "authorId": "2254328621",
                    "name": "Qihui Zhang"
                },
                {
                    "authorId": "2279094112",
                    "name": "Chujie Gao"
                },
                {
                    "authorId": "2282234921",
                    "name": "Yixin Huang"
                },
                {
                    "authorId": "2279022836",
                    "name": "Wenhan Lyu"
                },
                {
                    "authorId": "2257107248",
                    "name": "Yixuan Zhang"
                },
                {
                    "authorId": "2118053386",
                    "name": "Xiner Li"
                },
                {
                    "authorId": "2145977326",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "2254346817",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "2279093879",
                    "name": "Yijue Wang"
                },
                {
                    "authorId": "2275287781",
                    "name": "Zhikun Zhang"
                },
                {
                    "authorId": "1749353",
                    "name": "B. Kailkhura"
                },
                {
                    "authorId": "2266469680",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "2256992325",
                    "name": "Chaowei Xiao"
                },
                {
                    "authorId": "2268756316",
                    "name": "Chun-Yan Li"
                },
                {
                    "authorId": "2243234805",
                    "name": "Eric P. Xing"
                },
                {
                    "authorId": "2268686199",
                    "name": "Furong Huang"
                },
                {
                    "authorId": "2240876242",
                    "name": "Haodong Liu"
                },
                {
                    "authorId": "2271097936",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2254303011",
                    "name": "Hongyi Wang"
                },
                {
                    "authorId": "2237996727",
                    "name": "Huan Zhang"
                },
                {
                    "authorId": "18307037",
                    "name": "Huaxiu Yao"
                },
                {
                    "authorId": "2143693283",
                    "name": "M. Kellis"
                },
                {
                    "authorId": "2095762",
                    "name": "M. Zitnik"
                },
                {
                    "authorId": "2279159644",
                    "name": "Meng Jiang"
                },
                {
                    "authorId": "2253396640",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "2278917478",
                    "name": "James Zou"
                },
                {
                    "authorId": "2228505567",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "2238123544",
                    "name": "Jian Liu"
                },
                {
                    "authorId": "2256227183",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "2259869648",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "2266698166",
                    "name": "Jieyu Zhao"
                },
                {
                    "authorId": "2279062891",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "2145270616",
                    "name": "Jindong Wang"
                },
                {
                    "authorId": "2279260447",
                    "name": "John Mitchell"
                },
                {
                    "authorId": "2241470375",
                    "name": "Kai Shu"
                },
                {
                    "authorId": "2267887786",
                    "name": "Kaidi Xu"
                },
                {
                    "authorId": "2256646491",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "2254874151",
                    "name": "Lifang He"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                },
                {
                    "authorId": "152981628",
                    "name": "M. Backes"
                },
                {
                    "authorId": "2249536787",
                    "name": "Neil Zhenqiang Gong"
                },
                {
                    "authorId": "2258679535",
                    "name": "Philip S. Yu"
                },
                {
                    "authorId": "2279077171",
                    "name": "Pin-Yu Chen"
                },
                {
                    "authorId": "2279024252",
                    "name": "Quanquan Gu"
                },
                {
                    "authorId": "2279097262",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "2279023269",
                    "name": "Rex Ying"
                },
                {
                    "authorId": "2279225650",
                    "name": "Shuiwang Ji"
                },
                {
                    "authorId": "39400201",
                    "name": "S. Jana"
                },
                {
                    "authorId": "2265221446",
                    "name": "Tian-Xiang Chen"
                },
                {
                    "authorId": "2254792886",
                    "name": "Tianming Liu"
                },
                {
                    "authorId": "2144116530",
                    "name": "Tianying Zhou"
                },
                {
                    "authorId": "2281072607",
                    "name": "William Wang"
                },
                {
                    "authorId": "2280943906",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "2261601059",
                    "name": "Xiang-Yu Zhang"
                },
                {
                    "authorId": "2282386985",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2164984576",
                    "name": "Xingyao Xie"
                },
                {
                    "authorId": "2257123882",
                    "name": "Xun Chen"
                },
                {
                    "authorId": "2282196445",
                    "name": "Xuyu Wang"
                },
                {
                    "authorId": "2275033850",
                    "name": "Yan Liu"
                },
                {
                    "authorId": "2279157256",
                    "name": "Yanfang Ye"
                },
                {
                    "authorId": "2279101306",
                    "name": "Yinzhi Cao"
                },
                {
                    "authorId": "2254062898",
                    "name": "Yue Zhao"
                }
            ]
        },
        {
            "paperId": "13b5b69355555e0c8b702261c5de3b4172ba653c",
            "title": "The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning",
            "abstract": "Chain-of-Thought prompting (CoT) enables large-scale language models to solve complex reasoning problems by decomposing the problem and tackling it step-by-step. However, Chain-of-Thought is a greedy thinking process that requires the language model to come up with a starting point and generate the next step solely based on previous steps. This thinking process is different from how humans approach a complex problem e.g., we proactively raise sub-problems related to the original problem and recursively answer them. In this work, we propose S OCRATIC Q UESTIONING , a divide-and-conquer fashion algorithm that simulates the self-questioning and recursive thinking process. S OCRATIC Q UESTIONING is driven by a S ELF -Q UESTIONING module that employs a large-scale language model to propose sub-problems related to the original problem as intermediate steps and S OCRATIC Q UESTION - ING recursively backtracks and answers the sub-problems until reaches the original problem. We apply our proposed algorithm to the visual question-answering task as a case study and by evaluating it on three public benchmark datasets, we observe a significant performance improvement over all baselines on (almost) all datasets. In addition, the qualitative analysis clearly demonstrates the intermediate thinking steps elicited by S OCRATIC Q UESTIONING are similar to the human\u2019s recursively thinking process of a complex reasoning problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149459050",
                    "name": "Jingyuan Qi"
                },
                {
                    "authorId": "2136442661",
                    "name": "Zhiyang Xu"
                },
                {
                    "authorId": "2218172912",
                    "name": "Ying Shen"
                },
                {
                    "authorId": "2123130842",
                    "name": "Minqian Liu"
                },
                {
                    "authorId": "2219267090",
                    "name": "dingnan jin"
                },
                {
                    "authorId": "2145778781",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                }
            ]
        },
        {
            "paperId": "1cdfa7c3465943a295f8df2d2097c4bb3e222426",
            "title": "Rationalization for explainable NLP: a survey",
            "abstract": "Recent advances in deep learning have improved the performance of many Natural Language Processing (NLP) tasks such as translation, question-answering, and text classification. However, this improvement comes at the expense of model explainability. Black-box models make it difficult to understand the internals of a system and the process it takes to arrive at an output. Numerical (LIME, Shapley) and visualization (saliency heatmap) explainability techniques are helpful; however, they are insufficient because they require specialized knowledge. These factors led rationalization to emerge as a more accessible explainable technique in NLP. Rationalization justifies a model's output by providing a natural language explanation (rationale). Recent improvements in natural language generation have made rationalization an attractive technique because it is intuitive, human-comprehensible, and accessible to non-technical users. Since rationalization is a relatively new field, it is disorganized. As the first survey, rationalization literature in NLP from 2007 to 2022 is analyzed. This survey presents available methods, explainable evaluations, code, and datasets used across various NLP tasks that use rationalization. Further, a new subfield in Explainable AI (XAI), namely, Rational AI (RAI), is introduced to advance the current state of rationalization. A discussion on observed insights, challenges, and future directions is provided to point to promising research opportunities.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1573578354",
                    "name": "Sai Gurrapu"
                },
                {
                    "authorId": "2054229124",
                    "name": "Ajay Kulkarni"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                },
                {
                    "authorId": "2099420",
                    "name": "Ismini Lourentzou"
                },
                {
                    "authorId": "2106224635",
                    "name": "Laura J. Freeman"
                },
                {
                    "authorId": "144105786",
                    "name": "Feras A. Batarseh"
                }
            ]
        },
        {
            "paperId": "2736abd76c8fd66614ed5d64cab2e6ae04871965",
            "title": "Ameli: Enhancing Multimodal Entity Linking with Fine-Grained Attributes",
            "abstract": "We propose attribute-aware multimodal entity linking, where the input consists of a mention described with a text paragraph and images, and the goal is to predict the corresponding target entity from a multimodal knowledge base (KB) where each entity is also accompanied by a text description, visual images, and a collection of attributes that present the meta-information of the entity in a structured format. To facilitate this research endeavor, we construct Ameli, encompassing a new multimodal entity linking benchmark dataset that contains 16,735 mentions described in text and associated with 30,472 images, and a multimodal knowledge base that covers 34,690 entities along with 177,873 entity images and 798,216 attributes. To establish baseline performance on Ameli, we experiment with several state-of-the-art architectures for multimodal entity linking and further propose a new approach that incorporates attributes of entities into disambiguation. Experimental results and extensive qualitative analysis demonstrate that extracting and understanding the attributes of mentions from their text descriptions and visual images play a vital role in multimodal entity linking. To the best of our knowledge, we are the first to integrate attributes in the multimodal entity linking task. The programs, model checkpoints, and the dataset are publicly available at https://github.com/VT-NLP/Ameli.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2166311346",
                    "name": "Barry Menglong Yao"
                },
                {
                    "authorId": "2181535101",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "145196279",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "2116423012",
                    "name": "Sijia Wang"
                },
                {
                    "authorId": "2123130842",
                    "name": "Minqian Liu"
                },
                {
                    "authorId": "2136442661",
                    "name": "Zhiyang Xu"
                },
                {
                    "authorId": "2112477373",
                    "name": "Licheng Yu"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                }
            ]
        },
        {
            "paperId": "3b9abe8d06e7ae7c1ee3aeec2d10fca68f52923d",
            "title": "Teamwork Is Not Always Good: An Empirical Study of Classifier Drift in Class-incremental Information Extraction",
            "abstract": "Class-incremental learning (CIL) aims to develop a learning system that can continually learn new classes from a data stream without forgetting previously learned classes. When learning classes incrementally, the classifier must be constantly updated to incorporate new classes, and the drift in decision boundary may lead to severe forgetting. This fundamental challenge, however, has not yet been studied extensively, especially in the setting where no samples from old classes are stored for rehearsal. In this paper, we take a closer look at how the drift in the classifier leads to forgetting, and accordingly, design four simple yet (super-) effective solutions to alleviate the classifier drift: an Individual Classifiers with Frozen Feature Extractor (ICE) framework where we individually train a classifier for each learning session, and its three variants ICE-PL, ICE-O, and ICE-PL&O which further take the logits of previously learned classes from old sessions or a constant logit of an Other class as a constraint to the learning of new classifiers. Extensive experiments and analysis on 6 class-incremental information extraction tasks demonstrate that our solutions, especially ICE-O, consistently show significant improvement over the previous state-of-the-art approaches with up to 44.7% absolute F-score gain, providing a strong baseline and insights for future research on class-incremental learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2123130842",
                    "name": "Minqian Liu"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                }
            ]
        },
        {
            "paperId": "492b7e23c99ccff873d49bfc0d2a294bacf6c09c",
            "title": "Iteratively Improving Biomedical Entity Linking and Event Extraction via Hard Expectation-Maximization",
            "abstract": "Biomedical entity linking and event extraction are two crucial tasks to support text understanding and retrieval in the biomedical domain. These two tasks intrinsically benefit each other: entity linking disambiguates the biomedical concepts by referring to external knowledge bases and the domain knowledge further provides additional clues to understand and extract the biological processes, while event extraction identifies a key trigger and entities involved to describe each biological process which also captures the structural context to better disambiguate the biomedical entities. However, previous research typically solves these two tasks separately or in a pipeline, leading to error propagation. What's more, it's even more challenging to solve these two tasks together as there is no existing dataset that contains annotations for both tasks. To solve these challenges, we propose joint biomedical entity linking and event extraction by regarding the event structures and entity references in knowledge bases as latent variables and updating the two task-specific models in a hard Expectation-Maximization (EM) fashion: (1) predicting the missing variables for each partially annotated dataset based on the current two task-specific models, and (2) updating the parameters of each model on the corresponding pseudo completed dataset. Experimental results on two benchmark datasets: Genia 2011 for event extraction and BC4GO for entity linking, show that our joint framework significantly improves the model for each individual task and outperforms the strong baselines for both tasks. We will make the code and model checkpoints publicly available once the paper is accepted.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145438096",
                    "name": "Xiaochu Li"
                },
                {
                    "authorId": "2123130842",
                    "name": "Minqian Liu"
                },
                {
                    "authorId": "2136442661",
                    "name": "Zhiyang Xu"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                }
            ]
        },
        {
            "paperId": "57fd185b38fab0311deb4be4da2d3f4206db429a",
            "title": "RE^2: Region-Aware Relation Extraction from Visually Rich Documents",
            "abstract": "Current research in form understanding predominantly relies on large pre-trained language models, necessitating extensive data for pre-training. However, the importance of layout structure (i.e., the spatial relationship between the entity blocks in the visually rich document) to relation extraction has been overlooked. In this paper, we propose \\textbf{RE}gion-Aware \\textbf{R}elation \\textbf{E}xtraction (\\bf{RE^2}) that leverages region-level spatial structure among the entity blocks to improve their relation prediction. We design an edge-aware graph attention network to learn the interaction between entities while considering their spatial relationship defined by their region-level representations. We also introduce a constraint objective to regularize the model towards consistency with the inherent constraints of the relation extraction task. To support the research on relation extraction from visually rich documents and demonstrate the generalizability of \\bf{RE^2}, we build a new benchmark dataset, {DiverseForm}, that covers a wide range of domains. Extensive experiments on {DiverseForm} and several public benchmark datasets demonstrate significant superiority and transferability of \\bf{RE^2} across various domains and languages, with up to 18.88% absolute F-score gain over all high-performing baselines",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218196172",
                    "name": "Pritika Ramu"
                },
                {
                    "authorId": "2116423012",
                    "name": "Sijia Wang"
                },
                {
                    "authorId": "2153095",
                    "name": "Lalla Mouatadid"
                },
                {
                    "authorId": "2218905547",
                    "name": "Joy Rimchala"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                }
            ]
        },
        {
            "paperId": "840ccc871924c5bd0d51f79618a80084a7f53b09",
            "title": "Understand the Dynamic World: An End-to-End Knowledge Informed Framework for Open Domain Entity State Tracking",
            "abstract": "Open domain entity state tracking aims to predict reasonable state changes of entities (i.e., [attribute] of [entity] was [before_state] and [after_state] afterwards) given the action descriptions. It's important to many reasoning tasks to support human everyday activities. However, it's challenging as the model needs to predict an arbitrary number of entity state changes caused by the action while most of the entities are implicitly relevant to the actions and their attributes as well as states are from open vocabularies. To tackle these challenges, we propose a novel end-to-end Knowledge Informed framework for open domain Entity State Tracking, namely KIEST, which explicitly retrieves the relevant entities and attributes from external knowledge graph (i.e., ConceptNet) and incorporates them to autoregressively generate all the entity state changes with a novel dynamic knowledge grained encoder-decoder framework. To enforce the logical coherence among the predicted entities, attributes, and states, we design a new constraint decoding strategy and employ a coherence reward to improve the decoding process. Experimental results show that our proposed KIEST framework significantly outperforms the strong baselines on the public benchmark dataset - OpenPI",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47629178",
                    "name": "Mingchen Li"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                }
            ]
        },
        {
            "paperId": "0798e42c4fed7bf7d79cfaa3e86980149da0e3e0",
            "title": "Incremental Prompting: Episodic Memory Prompt for Lifelong Event Detection",
            "abstract": "Lifelong event detection aims to incrementally update a model with new event types and data while retaining the capability on previously learned old types. One critical challenge is that the model would catastrophically forget old types when continually trained on new data. In this paper, we introduce Episodic Memory Prompts (EMP) to explicitly retain the learned task-specific knowledge. Our method adopts continuous prompt for each task and they are optimized to instruct the model prediction and learn event-specific representation. The EMPs learned in previous tasks are carried along with the model in subsequent tasks, and can serve as a memory module that keeps the old knowledge and transferring to new tasks. Experiment results demonstrate the effectiveness of our method. Furthermore, we also conduct a comprehensive analysis of the new and old event types in lifelong learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2123130842",
                    "name": "Minqian Liu"
                },
                {
                    "authorId": "3307026",
                    "name": "Shiyu Chang"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                }
            ]
        },
        {
            "paperId": "0c0300f53c01ae609c97395c98de4c9d85d92876",
            "title": "MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning",
            "abstract": "Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric \u2013 Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2136442661",
                    "name": "Zhiyang Xu"
                },
                {
                    "authorId": "2115382645",
                    "name": "Ying Shen"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                }
            ]
        }
    ]
}