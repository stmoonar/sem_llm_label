{
    "authorId": "1452686038",
    "papers": [
        {
            "paperId": "07c53cf78f0ec5b95e89dd4fc4f5774ab486c4b5",
            "title": "DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages",
            "abstract": "Language technologies should be judged on their usefulness in real-world use cases. An often overlooked aspect in natural language processing (NLP) research and evaluation is language variation in the form of non-standard dialects or language varieties (hereafter, varieties). Most NLP benchmarks are limited to standard language varieties. To fill this gap, we propose DIALECTBENCH, the first-ever large-scale benchmark for NLP on varieties, which aggregates an extensive set of task-varied variety datasets (10 text-level tasks covering 281 varieties). This allows for a comprehensive evaluation of NLP system performance on different language varieties. We provide substantial evidence of performance disparities between standard and non-standard language varieties, and we also identify language clusters with large performance divergence across tasks. We believe DIALECTBENCH provides a comprehensive view of the current state of NLP for language varieties and one step towards advancing it further. Code/data: https://github.com/ffaisal93/DialectBench",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48556979",
                    "name": "FAHIM FAISAL"
                },
                {
                    "authorId": "1452686038",
                    "name": "Orevaoghene Ahia"
                },
                {
                    "authorId": "2169175069",
                    "name": "Aarohi Srivastava"
                },
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "2263760783",
                    "name": "David Chiang"
                },
                {
                    "authorId": "2287930119",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2273733474",
                    "name": "Antonios Anastasopoulos"
                }
            ]
        },
        {
            "paperId": "8efaa8206874c2f7a79bba2a9bcba542e4cabf31",
            "title": "MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling",
            "abstract": "A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts. Although contemporary text encoding methods cover most of the world's writing systems, they exhibit bias towards the high-resource languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and diminishes the perplexity gap throughout diverse languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1666636295",
                    "name": "Tomasz Limisiewicz"
                },
                {
                    "authorId": "3443287",
                    "name": "Terra Blevins"
                },
                {
                    "authorId": "1821892",
                    "name": "Hila Gonen"
                },
                {
                    "authorId": "1452686038",
                    "name": "Orevaoghene Ahia"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                }
            ]
        },
        {
            "paperId": "958239531fbe13c7c9553cb509d4ba4c6001cde1",
            "title": "Extracting Lexical Features from Dialects via Interpretable Dialect Classifiers",
            "abstract": "Identifying linguistic differences between dialects of a language often requires expert knowledge and meticulous human analysis. This is largely due to the complexity and nuance involved in studying various dialects. We present a novel approach to extract distinguishing lexical features of dialects by utilizing interpretable dialect classifiers, even in the absence of human experts. We explore both post-hoc and intrinsic approaches to interpretability, conduct experiments on Mandarin, Italian, and Low Saxon, and experimentally demonstrate that our method successfully identifies key language-specific lexical features that contribute to dialectal variations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287928451",
                    "name": "Roy Xie"
                },
                {
                    "authorId": "1452686038",
                    "name": "Orevaoghene Ahia"
                },
                {
                    "authorId": "2287930119",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2273733474",
                    "name": "Antonios Anastasopoulos"
                }
            ]
        },
        {
            "paperId": "e03648463405a77515c6af6cae4947a029b465ae",
            "title": "Teaching LLMs to Abstain across Languages via Multilingual Feedback",
            "abstract": "Multilingual LLMs often have knowledge disparities across languages, with larger gaps in under-resourced languages. Teaching LLMs to abstain in the face of knowledge gaps is thus a promising strategy to mitigate hallucinations in multilingual settings. However, previous studies on LLM abstention primarily focus on English; we find that directly applying existing solutions beyond English results in up to 20.5% performance gaps between high and low-resource languages, potentially due to LLMs' drop in calibration and reasoning beyond a few resource-rich languages. To this end, we propose strategies to enhance LLM abstention by learning from multilingual feedback, where LLMs self-reflect on proposed answers in one language by generating multiple feedback items in related languages: we show that this helps identifying the knowledge gaps across diverse languages, cultures, and communities. Extensive experiments demonstrate that our multilingual feedback approach outperforms various strong baselines, achieving up to 9.2% improvement for low-resource languages across three black-box and open models on three datasets, featuring open-book, closed-book, and commonsense QA. Further analysis reveals that multilingual feedback is both an effective and a more equitable abstain strategy to serve diverse language speakers, and cultural factors have great impact on language selection and LLM abstention behavior, highlighting future directions for multilingual and multi-cultural reliable language modeling.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284701198",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "2254168375",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2108853330",
                    "name": "Yike Wang"
                },
                {
                    "authorId": "2282214127",
                    "name": "Wenxuan Ding"
                },
                {
                    "authorId": "1452686038",
                    "name": "Orevaoghene Ahia"
                },
                {
                    "authorId": "2295954288",
                    "name": "Shuyue Stella Li"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2256989615",
                    "name": "Sunayana Sitaram"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "17fbffb05fa14e21d1c506fd5f0f568b955fe983",
            "title": "Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models",
            "abstract": "Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1452686038",
                    "name": "Orevaoghene Ahia"
                },
                {
                    "authorId": "51467955",
                    "name": "Sachin Kumar"
                },
                {
                    "authorId": "1821892",
                    "name": "Hila Gonen"
                },
                {
                    "authorId": "11348687",
                    "name": "Jungo Kasai"
                },
                {
                    "authorId": "3407646",
                    "name": "David R. Mortensen"
                },
                {
                    "authorId": "144365875",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "481d29cc4cd7d5d41faed2c3a84bbb19ebc5b027",
            "title": "AfriQA: Cross-lingual Open-Retrieval Question Answering for African Languages",
            "abstract": "African languages have far less in-language content available digitally, making it challenging for question answering systems to satisfy the information needs of users. Cross-lingual open-retrieval question answering (XOR QA) systems -- those that retrieve answer content from other languages while serving people in their native language -- offer a means of filling this gap. To this end, we create AfriQA, the first cross-lingual QA dataset with a focus on African languages. AfriQA includes 12,000+ XOR QA examples across 10 African languages. While previous datasets have focused primarily on languages where cross-lingual QA augments coverage from the target language, AfriQA focuses on languages where cross-lingual answer content is the only high-coverage source of answer content. Because of this, we argue that African languages are one of the most important and realistic use cases for XOR QA. Our experiments demonstrate the poor performance of automatic translation and multilingual retrieval methods. Overall, AfriQA proves challenging for state-of-the-art QA models. We hope that the dataset enables the development of more equitable QA technology.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2166106776",
                    "name": "Odunayo Ogundepo"
                },
                {
                    "authorId": "2352354",
                    "name": "T. Gwadabe"
                },
                {
                    "authorId": "2059157845",
                    "name": "Clara Rivera"
                },
                {
                    "authorId": "144797264",
                    "name": "J. Clark"
                },
                {
                    "authorId": "2884561",
                    "name": "Sebastian Ruder"
                },
                {
                    "authorId": "2518906",
                    "name": "David Ifeoluwa Adelani"
                },
                {
                    "authorId": "1591111757",
                    "name": "Bonaventure F. P. Dossou"
                },
                {
                    "authorId": "2087650847",
                    "name": "Abdoulahat Diop"
                },
                {
                    "authorId": "1573690693",
                    "name": "Claytone Sikasote"
                },
                {
                    "authorId": "2114722803",
                    "name": "Gilles Hacheme"
                },
                {
                    "authorId": "1395556657",
                    "name": "Happy Buzaaba"
                },
                {
                    "authorId": "50203736",
                    "name": "Ignatius M Ezeani"
                },
                {
                    "authorId": "2140114498",
                    "name": "Rooweither Mabuya"
                },
                {
                    "authorId": "1486204986",
                    "name": "Salomey Osei"
                },
                {
                    "authorId": "1591176064",
                    "name": "Chris C. Emezue"
                },
                {
                    "authorId": "66495690",
                    "name": "A. Kahira"
                },
                {
                    "authorId": "7744881",
                    "name": "Shamsuddeen Hassan Muhammad"
                },
                {
                    "authorId": "2175480812",
                    "name": "Akintunde Oladipo"
                },
                {
                    "authorId": "2188832804",
                    "name": "A. Owodunni"
                },
                {
                    "authorId": "2148631756",
                    "name": "A. Tonja"
                },
                {
                    "authorId": "2163094493",
                    "name": "Iyanuoluwa Shode"
                },
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "98725872",
                    "name": "T. Ajayi"
                },
                {
                    "authorId": "2056776870",
                    "name": "Clemencia Siro"
                },
                {
                    "authorId": "2058369323",
                    "name": "Steven Arthur"
                },
                {
                    "authorId": "2056770646",
                    "name": "Mofetoluwa Adeyemi"
                },
                {
                    "authorId": "1452686038",
                    "name": "Orevaoghene Ahia"
                },
                {
                    "authorId": "2047583795",
                    "name": "Aremu Anuoluwapo"
                },
                {
                    "authorId": "117288935",
                    "name": "O. Awosan"
                },
                {
                    "authorId": "73054967",
                    "name": "C. Chukwuneke"
                },
                {
                    "authorId": "2060867724",
                    "name": "Bernard Opoku"
                },
                {
                    "authorId": "88500872",
                    "name": "A. Ayodele"
                },
                {
                    "authorId": "90459153",
                    "name": "V. Otiende"
                },
                {
                    "authorId": "2282726762",
                    "name": "Christine Mwase"
                },
                {
                    "authorId": "81032038",
                    "name": "B. Sinkala"
                },
                {
                    "authorId": "2158994685",
                    "name": "Andre Niyongabo Rubungo"
                },
                {
                    "authorId": "1712191202",
                    "name": "Daniel Ajisafe"
                },
                {
                    "authorId": "2215213107",
                    "name": "Emeka Onwuegbuzia"
                },
                {
                    "authorId": "2216786549",
                    "name": "Habib Mbow"
                },
                {
                    "authorId": "2216795443",
                    "name": "Emile Niyomutabazi"
                },
                {
                    "authorId": "2216889086",
                    "name": "Eunice Mukonde"
                },
                {
                    "authorId": "1588248096",
                    "name": "F. I. Lawan"
                },
                {
                    "authorId": "153795444",
                    "name": "I. Ahmad"
                },
                {
                    "authorId": "122367036",
                    "name": "Jesujoba Oluwadara Alabi"
                },
                {
                    "authorId": "2216891085",
                    "name": "Martin Namukombo"
                },
                {
                    "authorId": "2216786546",
                    "name": "Mbonu Chinedu"
                },
                {
                    "authorId": "2216787102",
                    "name": "Mofya Phiri"
                },
                {
                    "authorId": "2216798369",
                    "name": "Neo Putini"
                },
                {
                    "authorId": "2216787095",
                    "name": "Ndumiso Mngoma"
                },
                {
                    "authorId": "2190281321",
                    "name": "Priscilla Amuok"
                },
                {
                    "authorId": "82370080",
                    "name": "R. Iro"
                },
                {
                    "authorId": "2216878430",
                    "name": "Sonia Adhiambo34"
                }
            ]
        },
        {
            "paperId": "dc9f9419bef4aa2c00e3db1315c6b4dd18ed8c47",
            "title": "AfriWOZ: Corpus for Exploiting Cross-Lingual Transfer for Dialogue Generation in Low-Resource, African Languages",
            "abstract": "Dialogue generation is an important NLP task fraught with many challenges. The challenges become more daunting for low-resource African languages. To enable the creation of dialogue agents for African languages, we contribute the first high-quality dialogue datasets for 6 African languages: Swahili, Wolof, Hausa, Nigerian Pidgin English, Kinyarwanda & Yor\u00f9b\u00e1. There are a total of 9,000 turns, each language having 1,500 turns, which we translate from a portion of the English multi-domain MultiWOZ dataset. Subsequently, we benchmark by investigating & analyzing the effectiveness of modelling through transfer learning by utilziing state-of-the-art (SoTA) deep monolingual models: DialoGPT and BlenderBot. We compare the models with a simple seq2seq baseline using perplexity. Besides this, we conduct human evaluation of single-turn conversations by using majority votes and measure inter-annotator agreement (IAA). We find that the hypothesis that deep monolingual models learn some abstractions that generalize across languages holds. We observe human-like conversations, to different degrees, in 5 out of the 6 languages. The language with the most transferable properties is the Nigerian Pidgin English, with a human-likeness score of 78.1%, of which 34.4% are unanimous. We freely provide the datasets and host the model checkpoints/demos on the HuggingFace hub for public access.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51221489",
                    "name": "Tosin P. Adewumi"
                },
                {
                    "authorId": "2056770646",
                    "name": "Mofetoluwa Adeyemi"
                },
                {
                    "authorId": "2047583795",
                    "name": "Aremu Anuoluwapo"
                },
                {
                    "authorId": "2162783577",
                    "name": "Bukola Peters"
                },
                {
                    "authorId": "1395556657",
                    "name": "Happy Buzaaba"
                },
                {
                    "authorId": "2135913982",
                    "name": "Oyerinde Samuel"
                },
                {
                    "authorId": "2162781574",
                    "name": "Amina Mardiyyah Rufai"
                },
                {
                    "authorId": "83263885",
                    "name": "Benjamin Ayoade Ajibade"
                },
                {
                    "authorId": "2162782825",
                    "name": "Tajudeen Gwadabe"
                },
                {
                    "authorId": "2226522729",
                    "name": "Mory Moussou Koulibaly Traore"
                },
                {
                    "authorId": "98725872",
                    "name": "T. Ajayi"
                },
                {
                    "authorId": "7744881",
                    "name": "Shamsuddeen Hassan Muhammad"
                },
                {
                    "authorId": "114850513",
                    "name": "Ahmed Baruwa"
                },
                {
                    "authorId": "2105439683",
                    "name": "Paul Owoicho"
                },
                {
                    "authorId": "2145191211",
                    "name": "Tol\u00falop\u00e9 \u00d2g\u00fanr\u00e8m\u00ed"
                },
                {
                    "authorId": "2162782962",
                    "name": "Phylis Ngigi"
                },
                {
                    "authorId": "1452686038",
                    "name": "Orevaoghene Ahia"
                },
                {
                    "authorId": "2162783296",
                    "name": "Ruqayya Nasir"
                },
                {
                    "authorId": "80342407",
                    "name": "F. Liwicki"
                },
                {
                    "authorId": "1743758",
                    "name": "M. Liwicki"
                }
            ]
        },
        {
            "paperId": "f9a5af5b21563b9bdd09630a8dec62d515479678",
            "title": "LEXPLAIN: Improving Model Explanations via Lexicon Supervision",
            "abstract": "Model explanations that shed light on the model\u2019s predictions are becoming a desired additional output of NLP models, alongside their predictions. Challenges in creating these explanations include making them trustworthy and faithful to the model\u2019s predictions. In this work, we propose a novel framework for guiding model explanations by supervising them explicitly. To this end, our method, LEXplain, uses task-related lexicons to directly supervise model explanations. This approach consistently improves the model\u2019s explanations without sacrificing performance on the task, as we demonstrate on sentiment analysis and toxicity detection. Our analyses show that our method also demotes spurious correlations (i.e., with respect to African American English dialect) when performing the task, improving fairness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1452686038",
                    "name": "Orevaoghene Ahia"
                },
                {
                    "authorId": "1821892",
                    "name": "Hila Gonen"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "144365875",
                    "name": "Noah A. Smith"
                }
            ]
        },
        {
            "paperId": "3df820fdac427cd6d60753a2878d812f57b3b972",
            "title": "\u00cct\u00e0k\u00far\u00f2so: Exploiting Cross-Lingual Transferability for Natural Language Generation of Dialogues in Low-Resource, African Languages",
            "abstract": "We investigate the possibility of cross-lingual transfer from a state-of-the-art (SoTA) deep monolingual model (Di-aloGPT) to 6 African languages and compare with 2 baselines (BlenderBot 90M, another SoTA, and a simple Seq2Seq). The languages are Swahili, Wolof, Hausa, Nigerian Pidgin English, Kinyarwanda & Yor\u00f9b\u00e1. Generation of dialogues is known to be a challenging task for many reasons. It becomes more challenging for African languages which are low-resource in terms of data. Therefore, we translate a small portion of the English multi-domain MultiWOZ dataset for each target language. Besides intrinsic evaluation (i.e. perplexity), we conduct human evaluation of single-turn conversations by using majority votes and measure inter-annotator agreement (IAA). The results show that the hypothesis that deep monolingual models learn some abstractions that generalise across languages holds. We observe human-like conversations in 5 out of the 6 languages. It, however, applies to different degrees in different languages, which is expected. The language with the most transferable properties is the Nigerian Pidgin English, with a human-likeness score of 78.1%, of which 34.4% are unanimous. The main contributions of this paper include the representation (through the provision of high-quality dialogue data) of under-represented African languages and demonstrating the cross-lingual transferability hypothesis for dialogue systems. We also provide the datasets and host the model check-points/demos on the HuggingFace hub for public access.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51221489",
                    "name": "Tosin P. Adewumi"
                },
                {
                    "authorId": "2056770646",
                    "name": "Mofetoluwa Adeyemi"
                },
                {
                    "authorId": "2047583795",
                    "name": "Aremu Anuoluwapo"
                },
                {
                    "authorId": "2162783577",
                    "name": "Bukola Peters"
                },
                {
                    "authorId": "1395556657",
                    "name": "Happy Buzaaba"
                },
                {
                    "authorId": "2135913982",
                    "name": "Oyerinde Samuel"
                },
                {
                    "authorId": "2162781574",
                    "name": "Amina Mardiyyah Rufai"
                },
                {
                    "authorId": "83263885",
                    "name": "Benjamin Ayoade Ajibade"
                },
                {
                    "authorId": "2162782825",
                    "name": "Tajudeen Gwadabe"
                },
                {
                    "authorId": "2162713888",
                    "name": "M. Traore"
                },
                {
                    "authorId": "98725872",
                    "name": "T. Ajayi"
                },
                {
                    "authorId": "7744881",
                    "name": "Shamsuddeen Hassan Muhammad"
                },
                {
                    "authorId": "114850513",
                    "name": "Ahmed Baruwa"
                },
                {
                    "authorId": "2105439683",
                    "name": "Paul Owoicho"
                },
                {
                    "authorId": "2145191211",
                    "name": "Tol\u00falop\u00e9 \u00d2g\u00fanr\u00e8m\u00ed"
                },
                {
                    "authorId": "2162782962",
                    "name": "Phylis Ngigi"
                },
                {
                    "authorId": "1452686038",
                    "name": "Orevaoghene Ahia"
                },
                {
                    "authorId": "2162783296",
                    "name": "Ruqayya Nasir"
                },
                {
                    "authorId": "80342407",
                    "name": "F. Liwicki"
                },
                {
                    "authorId": "1743758",
                    "name": "M. Liwicki"
                }
            ]
        },
        {
            "paperId": "586eee780c15d69179e8ce0dd1ee94a79697191d",
            "title": "What a Creole Wants, What a Creole Needs",
            "abstract": "In recent years, the natural language processing (NLP) community has given increased attention to the disparity of efforts directed towards high-resource languages over low-resource ones. Efforts to remedy this delta often begin with translations of existing English datasets into other languages. However, this approach ignores that different language communities have different needs. We consider a group of low-resource languages, creole languages. Creoles are both largely absent from the NLP literature, and also often ignored by society at large due to stigma, despite these languages having sizable and vibrant communities. We demonstrate, through conversations with creole experts and surveys of creole-speaking communities, how the things needed from language technology can change dramatically from one language to another, even when the languages are considered to be very similar to each other, as with creoles. We discuss the prominent themes arising from these conversations, and ultimately demonstrate that useful language technology cannot be built without involving the relevant community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49568895",
                    "name": "Heather Lent"
                },
                {
                    "authorId": "1452683268",
                    "name": "Kelechi Ogueji"
                },
                {
                    "authorId": "3295381",
                    "name": "Miryam de Lhoneux"
                },
                {
                    "authorId": "1452686038",
                    "name": "Orevaoghene Ahia"
                },
                {
                    "authorId": "1700187",
                    "name": "Anders S\u00f8gaard"
                }
            ]
        }
    ]
}