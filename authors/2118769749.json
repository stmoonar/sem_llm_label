{
    "authorId": "2118769749",
    "papers": [
        {
            "paperId": "53dcdf3c6e0640bef458f97332063bbf7389721b",
            "title": "Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs",
            "abstract": "Large Language Models (LLMs) excel in various natural language processing tasks but struggle with hallucination issues. Existing solutions have considered utilizing LLMs' inherent reasoning abilities to alleviate hallucination, such as self-correction and diverse sampling methods. However, these methods often overtrust LLMs' initial answers due to inherent biases. The key to alleviating this issue lies in overriding LLMs' inherent biases for answer inspection. To this end, we propose a CounterFactual Multi-Agent Debate (CFMAD) framework. CFMAD presets the stances of LLMs to override their inherent biases by compelling LLMs to generate justifications for a predetermined answer's correctness. The LLMs with different predetermined stances are engaged with a skeptical critic for counterfactual debate on the rationality of generated justifications. Finally, the debate process is evaluated by a third-party judge to determine the final answer. Extensive experiments on four datasets of three tasks demonstrate the superiority of CFMAD over existing methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307040115",
                    "name": "Yi Fang"
                },
                {
                    "authorId": "2118769749",
                    "name": "Moxin Li"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2306959787",
                    "name": "Hui Lin"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                }
            ]
        },
        {
            "paperId": "5d3105a5ffa133b873537bda8ff1ec6244c2b841",
            "title": "Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers",
            "abstract": "Confidence estimation aiming to evaluate output trustability is crucial for the application of large language models (LLM), especially the black-box ones. Existing confidence estimation of LLM is typically not calibrated due to the overconfidence of LLM on its generated incorrect answers. Existing approaches addressing the overconfidence issue are hindered by a significant limitation that they merely consider the confidence of one answer generated by LLM. To tackle this limitation, we propose a novel paradigm that thoroughly evaluates the trusta-bility of multiple candidate answers to mitigate the overconfidence on incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation. This framework can be integrated with existing confidence estimation approaches for superior calibration. Experimental results on six datasets of three tasks demonstrate the rationality and effectiveness of the proposed framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118769749",
                    "name": "Moxin Li"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "31734386",
                    "name": "Fengbin Zhu"
                },
                {
                    "authorId": "2260433198",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "6f28e3cf3c0a532dd01afc1e74ab69ecab92fe75",
            "title": "Dual-Phase Accelerated Prompt Optimization",
            "abstract": "Gradient-free prompt optimization methods have made significant strides in enhancing the performance of closed-source Large Language Models (LLMs) across a wide range of tasks. However, existing approaches make light of the importance of high-quality prompt initialization and the identification of effective optimization directions, thus resulting in substantial optimization steps to obtain satisfactory performance. In this light, we aim to accelerate prompt optimization process to tackle the challenge of low convergence rate. We propose a dual-phase approach which starts with generating high-quality initial prompts by adopting a well-designed meta-instruction to delve into task-specific information, and iteratively optimize the prompts at the sentence level, leveraging previous tuning experience to expand prompt candidates and accept effective ones. Extensive experiments on eight datasets demonstrate the effectiveness of our proposed method, achieving a consistent accuracy gain over baselines with less than five optimization steps.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307559026",
                    "name": "Muchen Yang"
                },
                {
                    "authorId": "2118769749",
                    "name": "Moxin Li"
                },
                {
                    "authorId": "2307555695",
                    "name": "Yongle Li"
                },
                {
                    "authorId": "2307513797",
                    "name": "Zijun Chen"
                },
                {
                    "authorId": "2265123543",
                    "name": "Chongming Gao"
                },
                {
                    "authorId": "2307557096",
                    "name": "Junqi Zhang"
                },
                {
                    "authorId": "2307555570",
                    "name": "Yangyang Li"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                }
            ]
        },
        {
            "paperId": "aa1fbd6e8d1c8e99b6ca34c17bcdb36e987b68a6",
            "title": "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection",
            "abstract": "Self-detection for Large Language Models (LLMs) seeks to evaluate the trustworthiness of the LLM's output by leveraging its own capabilities, thereby alleviating the issue of output hallucination. However, existing self-detection approaches only retrospectively evaluate answers generated by LLM, typically leading to the over-trust in incorrectly generated answers. To tackle this limitation, we propose a novel self-detection paradigm that considers the comprehensive answer space beyond LLM-generated answers. It thoroughly compares the trustworthiness of multiple candidate answers to mitigate the over-trust in LLM-generated incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each candidate answer, and then aggregates the justifications for comprehensive target answer evaluation. This framework can be seamlessly integrated with existing approaches for superior self-detection. Extensive experiments on six datasets spanning three tasks demonstrate the effectiveness of the proposed framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118769749",
                    "name": "Moxin Li"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "31734386",
                    "name": "Fengbin Zhu"
                },
                {
                    "authorId": "2260433198",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "b7f9f6ac44fee822c692cdc1147c852a150f4aea",
            "title": "Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction",
            "abstract": "The rapid advancement of Large Language Models (LLMs) in the realm of mathematical reasoning necessitates comprehensive evaluations to gauge progress and inspire future directions. Existing assessments predominantly focus on problem-solving from the examinee perspective, overlooking a dual perspective of examiner regarding error identification and correction. From the examiner perspective, we define four evaluation tasks for error identification and correction along with a new dataset with annotated error types and steps. We also design diverse prompts to thoroughly evaluate eleven representative LLMs. Our principal findings indicate that GPT-4 outperforms all models, while open-source model LLaMA-2-7B demonstrates comparable abilities to closed-source models GPT-3.5 and Gemini Pro. Notably, calculation error proves the most challenging error type. Moreover, prompting LLMs with the error types can improve the average correction accuracy by 47.9\\%. These results reveal potential directions for developing the mathematical reasoning abilities of LLMs. Our code and dataset is available on https://github.com/LittleCirc1e/EIC.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2304613272",
                    "name": "Xiaoyuan Li"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2118769749",
                    "name": "Moxin Li"
                },
                {
                    "authorId": "2304467364",
                    "name": "Junrong Guo"
                },
                {
                    "authorId": "2145957648",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                }
            ]
        },
        {
            "paperId": "c08f383ac0aa40260f430f4607ebdfe7a8b30ba0",
            "title": "Don't Just Say\"I don't know\"! Self-aligning Large Language Models for Responding to Unknown Questions with Explanations",
            "abstract": "Despite the remarkable abilities of Large Language Models (LLMs) to answer questions, they often display a considerable level of overconfidence even when the question does not have a definitive answer. To avoid providing hallucinated answers to these unknown questions, existing studies typically investigate approaches to refusing to answer these questions. In this work, we propose a novel and scalable self-alignment method to utilize the LLM itself to enhance its response-ability to different types of unknown questions, being capable of not only refusing to answer but also providing explanation to the unanswerability of unknown questions. Specifically, the Self-Align method first employ a two-stage class-aware self-augmentation approach to generate a large amount of unknown question-response data. Then we conduct disparity-driven self-curation to select qualified data for fine-tuning the LLM itself for aligning the responses to unknown questions as desired. Experimental results on two datasets across four types of unknown questions validate the superiority of the Self-Align method over existing baselines in terms of three types of task formulation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145843537",
                    "name": "Yang Deng"
                },
                {
                    "authorId": "2287771182",
                    "name": "Yong Zhao"
                },
                {
                    "authorId": "2118769749",
                    "name": "Moxin Li"
                },
                {
                    "authorId": "2241348826",
                    "name": "See-kiong Ng"
                },
                {
                    "authorId": "2257036129",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "ecc3415b74717b3f786760e12934a31b37d98312",
            "title": "TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data",
            "abstract": "In this work, we address question answering (QA) over a hybrid of tabular and textual data that are very common content on the Web (e.g. SEC filings), where discrete reasoning capabilities are often required. Recently, large language models (LLMs) like GPT-4 have demonstrated strong multi-step reasoning capabilities. We then consider harnessing the amazing power of LLMs to solve our task. We abstract a Step-wise Pipeline for tabular and textual QA, which consists of three key steps, including Extractor, Reasoner and Executor, and initially design an instruction to instantiate the pipeline and validate that GPT-4 outperforms all existing methods. However, utilizing an online LLM like GPT-4 holds various challenges in terms of cost, latency, and data security risk, which motivates us to specialize smaller LLMs in this task. We develop a TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated automatically from existing expert-annotated datasets following the Step-wise Pipeline. The experimental results have verified that our TAT-LLM model can outperform all baseline models, including the previous best fine-tuned models and very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31734386",
                    "name": "Fengbin Zhu"
                },
                {
                    "authorId": "2281025248",
                    "name": "Ziyang Liu"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2144448019",
                    "name": "Chao Wang"
                },
                {
                    "authorId": "2118769749",
                    "name": "Moxin Li"
                },
                {
                    "authorId": "143779329",
                    "name": "Tat-seng Chua"
                }
            ]
        },
        {
            "paperId": "0ed565e9c2ddb80e3d6cc54c921e08f95e569eb0",
            "title": "Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents via Semantic-Oriented Hierarchical Graphs",
            "abstract": "Table-text document (e.g., financial reports) understanding has attracted increasing attention in recent two years. TAT-DQA is a realistic setting for the understanding of visually-rich table-text documents, which involves answering associated questions requiring discrete reasoning. Most existing work relies on token-level semantics, falling short in the reasoning across document elements such as quantities and dates. To address this limitation, we propose a novel Doc2SoarGraph model that exploits element-level semantics and employs Semantic-oriented hierarchical Graph structures to capture the differences and correlations among different elements within the given document and question. Extensive experiments on the TAT-DQA dataset reveal that our model surpasses the state-of-the-art conventional method (i.e., MHST) and large language model (i.e., ChatGPT) by 17.73 and 6.49 points respectively in terms of Exact Match (EM) metric, demonstrating exceptional effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31734386",
                    "name": "Fengbin Zhu"
                },
                {
                    "authorId": "2144448019",
                    "name": "Chao Wang"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2158173064",
                    "name": "Zifeng Ren"
                },
                {
                    "authorId": "2118769749",
                    "name": "Moxin Li"
                },
                {
                    "authorId": "143779329",
                    "name": "Tat-seng Chua"
                }
            ]
        },
        {
            "paperId": "2504035356a92cf2c8ad2beafd361719ac8aa6da",
            "title": "SoarGraph: Numerical Reasoning over Financial Table-Text Data via Semantic-Oriented Hierarchical Graphs",
            "abstract": "Towards the intelligent understanding of table-text data in the finance domain, previous research explores numerical reasoning over table-text content with Question Answering (QA) tasks. A general framework is to extract supporting evidence from the table and text and then perform numerical reasoning over extracted evidence for inferring the answer. However, existing models are vulnerable to missing supporting evidence, which limits their performance. In this work, we propose a novel Semantic-Oriented Hierarchical Graph (SoarGraph) that models the semantic relationships and dependencies among the different elements (e.g., question, table cells, text paragraphs, quantities, and dates) using hierarchical graphs to facilitate supporting evidence extraction and enhance numerical reasoning capability. We conduct our experiments on two popular benchmarks, FinQA and TAT-QA datasets, and the results show that our SoarGraph significantly outperforms all strong baselines, demonstrating remarkable effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31734386",
                    "name": "Fengbin Zhu"
                },
                {
                    "authorId": "2118769749",
                    "name": "Moxin Li"
                },
                {
                    "authorId": "66358686",
                    "name": "Junbin Xiao"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2144448019",
                    "name": "Chao Wang"
                },
                {
                    "authorId": "143779329",
                    "name": "Tat-seng Chua"
                }
            ]
        },
        {
            "paperId": "3b0c49ca5ac0f441c302c9ca4def4804253552d5",
            "title": "Robust Prompt Optimization for Large Language Models Against Distribution Shifts",
            "abstract": "Large Language Model (LLM) has demonstrated significant ability in various Natural Language Processing tasks. However, their effectiveness is highly dependent on the phrasing of the task prompt, leading to research on automatic prompt optimization using labeled task data. We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis. In this light, we propose a new problem of robust prompt optimization for LLMs against distribution shifts, which requires the prompt optimized over the labeled source group can simultaneously generalize to an unlabeled target group. To solve this problem, we propose Generalized Prompt Optimization framework, which incorporates the unlabeled data from the target group into prompt optimization. Extensive experimental results demonstrate the effectiveness of the proposed framework with significant performance improvement on the target group and comparable performance on the source group.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118769749",
                    "name": "Moxin Li"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2258806194",
                    "name": "Yixin Cao"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "143779329",
                    "name": "Tat-seng Chua"
                }
            ]
        }
    ]
}