{
    "authorId": "2024066",
    "papers": [
        {
            "paperId": "d64398a12dee441da3a86b1e00f12e9369280217",
            "title": "Reproducibility Crisis in the LOD Cloud? Studying the Impact of Ontology Accessibility and Archiving as a Counter Measure",
            "abstract": ". The reproducibility crisis is an ongoing problem that a\ufb00ects data-driven science to a big extent. The highly connected decentral Web of Ontologies represents the backbone for semantic data and the Linked Open Data Cloud and provides terminological context information crucial for the usage and interpretation of the data, which in turn is key for the reproducibility of research results making use of it. In this paper, we identify, analyze, and quantify reproducibility issues related to capturing terminological context (e.g. caused by unavailable ontologies) and delineate the impact on the reproducibility crisis in the Linked Open Data Cloud. Our examinations are backed by a frequent and ongoing monitoring of online available vocabularies and ontologies that results in the DBpedia Archivo dataset. We also show the extent to which the reproducibility crisis can be countered with the aid of ontology archiving in DBpedia Archivo and the Linked Open Vocabularies platforms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "2003784069",
                    "name": "Denis Streitmatter"
                },
                {
                    "authorId": "2258592",
                    "name": "Natanael Arndt"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "21798e23fc4af57aace2e76850daf372229ac02c",
            "title": "Towards a Systematic Approach to Sync Factual Data across Wikipedia, Wikidata and External Data Sources",
            "abstract": ". This paper addresses one of the largest and most complex data curation work\ufb02ows in existence: Wikipedia and Wikidata, with a high number of users and curators adding factual information from exter-nal sources via a non-systematic Wiki work\ufb02ow to Wikipedia\u2019s infoboxes and Wikidata items. We present high-level analyses of the current state, the challenges and limitations in this work\ufb02ow and supplement it with a quantitative and semantic analysis of the resulting data spaces by deploying DBpedia\u2019s integration and extraction capabilities. Based on an analysis of millions of references from Wikipedia infoboxes in di\ufb00erent languages, we can \ufb01nd the most important sources which can be used to enrich other knowledge bases with information of better quality. An initial tool is presented, the GlobalFactSync browser, as a prototype to discuss further measures to develop a more systematic approach for data curation in the WikiVerse.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                },
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "24163430",
                    "name": "M. Hofer"
                },
                {
                    "authorId": "1819564",
                    "name": "Milan Dojchinovski"
                },
                {
                    "authorId": "3223508",
                    "name": "Krzysztof W\u0119cel"
                },
                {
                    "authorId": "2513991",
                    "name": "W\u0142odzimierz Lewoniewski"
                }
            ]
        },
        {
            "paperId": "42c87b2caac3f41cd472a191e4305011504739c3",
            "title": "FAIR Linked Data - Towards a Linked Data Backbone for Users and Machines",
            "abstract": "Although many FAIR principles could be fulfilled by 5-star Linked Open Data, the successful realization of FAIR poses a multitude of challenges. FAIR publishing and retrieval of Linked Data is still rather a FAIRytale than reality, for users and machines. In this paper, we give an overview on four major approaches that tackle individual challenges of FAIR data and present our vision of a FAIR Linked Data backbone. We propose 1) DBpedia Databus - a flexible, heavily automatable dataset management and publishing platform based on DataID metadata; that is extended by 2) the novel Databus Mods architecture which allows for flexible, unified, community-specific metadata extensions and (search/annotation) overlay systems; 3) DBpedia Archivo an archiving solution for unified handling and improvement of FAIRness for ontologies on publisher and consumer side; as well as 4) the DBpedia Global ID management and lookup services to cluster and discover equivalent entities and properties",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "d17e7db6ab57ea0c5a1c97cb3d394423f980c014",
            "title": "DACOC3 - DBpedia Archivo Challenging Ontology Consistency Check Collection",
            "abstract": "DBpedia Archivo is an online ontology interface and open augmented archive, containing more than 1,400 ontologies. It uses several fully automated ontology discovery mechanisms that run each week and have turned Archivo into one of the most exhaustive, and recent ontology archives. Archivo daily checks for new ontology versions and performs automated tests to evaluate the fitness for use of an ontology. As part of a 4-star quality rating, a logical consistency check is applied. However, several ontologies contained in Archivo cause problems with current reasoner implementations when verifying consistency, leading to timeouts and other runtime failures. In this paper, we present an approach to create a collection of such challenging ontologies and report key characteristics of these ontologies, that can be easily consumed by reasoning applications in order to evaluate their performance and stability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "2003784069",
                    "name": "Denis Streitmatter"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "38d37fc0e471b7e7ade321bc839c04f4ab28a65a",
            "title": "MMoOn Core - the Multilingual Morpheme Ontology",
            "abstract": "In the last years a rapid emergence of lexical resources has evolved in the Semantic Web. Whereas most of the linguistic information is already machine-readable, we found that morphological information is mostly absent or only contained in semi-structured strings. An integration of morphemic data has not yet been undertaken due to the lack of existing domain-specific ontologies and explicit morphemic data. In this paper, we present the Multilingual Morpheme Ontology called MMoOn Core which can be regarded as the first comprehensive ontology for the linguistic domain of morphological language data. It will be described how crucial concepts like morphs, morphemes, word forms and meanings are represented and interrelated and how language-specific morpheme inventories can be created as a new possibility of morphological datasets. The aim of the MMoOn Core ontology is to serve as a shared semantic model for linguists and NLP researchers alike to enable the creation, conversion, exchange, reuse and enrichment of morphological language data across different data-dependent language sciences. Therefore, various use cases are illustrated to draw attention to the cross-disciplinary potential which can be realized with the MMoOn Core ontology in the context of the existing Linguistic Linked Data research landscape.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3448570",
                    "name": "Bettina Klimek"
                },
                {
                    "authorId": "37683106",
                    "name": "Markus Ackermann"
                },
                {
                    "authorId": "2223670",
                    "name": "Martin Br\u00fcmmer"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "db92477534c0440aec1252663efad02a631da5ac",
            "title": "Evaluation of metadata representations in RDF stores",
            "abstract": "The article analyses the contemporary situation in the research on childhood and generalises the prevailing approaches to a child, childhood, as well as to research methodology. The methodological basis of the discussion\u2014who should be the main subjects of research\u2014children or adults\u2014is revealed. The views of those researchers who maintain that children are the main participants of research are supported. Reference is made to the psycho-social and cultural characteristics of children, which dominate social-humanitarian research, which are relevant to a researcher when selecting qualitative or quantitative research methods. The limits of trust in children as subjects of research are indicated. The article discloses the features of the research ethics that are determined by a researcher\u2019s approach to a child\u2019s (children\u2019s) status in the research. The core of the responsibility of the researcher as a representative of adults and their culture is explained. It is pointed out that a researcher in childhood research represents the type of relations with children that is characteristic of the society where the researcher lives. The article emphasises the mission of the childhood researcher\u2014to reveal the unknown, unfamiliar and therefore sometimes underestimated socio-cultural aspects and problems of the children\u2019s\u2019 world to society. The conclusion is made that a childhood researcher in his/her research always advocates the social-cultural meaningfulness of the childhood world against adults and society as a whole. Research that involves children should answer a question\u2014whether a child (children) is a reliable informant, and whether they are sincere when providing information. Often observed lies or insincerity of children should be understood by a researcher as a child\u2019s intention to protect themselves from trouble. Various forms of avoidant behaviour depend on the age of the children. Childhood researchers must demonstrate trust in children even when the latter do not tell the truth. Besides, a researcher is recommended to avoid exerting pressure, manipulation and deception on children. A researcher should have certain character features: curiosity, being able to show an emphatic interest in a child\u2019s life and to become an explorer striving to learn about and understand the child\u2019s world.",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "2113612289",
                    "name": "Kay M\u00fcller"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                },
                {
                    "authorId": "1747414",
                    "name": "E. Rahm"
                },
                {
                    "authorId": "143858195",
                    "name": "Maria-Esther Vidal"
                }
            ]
        },
        {
            "paperId": "30596da17f14943eb98f428844e100724e28456d",
            "title": "DBpedia NIF: Open, Large-Scale and Multilingual Knowledge Extraction Corpus",
            "abstract": "In the past decade, the DBpedia community has put significant amount of effort on developing technical infrastructure and methods for efficient extraction of structured information from Wikipedia. These efforts have been primarily focused on harvesting, refinement and publishing semi-structured information found in Wikipedia articles, such as information from infoboxes, categorization information, images, wikilinks and citations. Nevertheless, still vast amount of valuable information is contained in the unstructured Wikipedia article texts. In this paper, we present DBpedia NIF - a large-scale and multilingual knowledge extraction corpus. The aim of the dataset is two-fold: to dramatically broaden and deepen the amount of structured information in DBpedia, and to provide large-scale and multilingual language resource for development of various NLP and IR task. The dataset provides the content of all articles for 128 Wikipedia languages. We describe the dataset creation process and the NLP Interchange Format (NIF) used to model the content, links and the structure the information of the Wikipedia articles. The dataset has been further enriched with about 25% more links and selected partitions published as Linked Data. Finally, we describe the maintenance and sustainability plans, and selected use cases of the dataset from the TextExt knowledge extraction challenge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1819564",
                    "name": "Milan Dojchinovski"
                },
                {
                    "authorId": "123017598",
                    "name": "Julio Hernandez"
                },
                {
                    "authorId": "37683106",
                    "name": "Markus Ackermann"
                },
                {
                    "authorId": "35114918",
                    "name": "Amit Kirschenbaum"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "5378e824c6a3c224fb285ce6a8782412a888c4fd",
            "title": "Inference of Latent Shape Expressions Associated to DBpedia Ontology",
            "abstract": "In order to perform any operation in an RDF graph, it is recommendable to know the expected topology of the targeted information. Some technologies have been developed in the last years to describe the expected shapes in an RDF graph, such as ShEx or SHACL. In general, a domain expert can define the expected shapes in a graph, but there are some scenarios in which the schema cannot be predicted a priori, but it emerges at the same time that the graph is filled with new information (the shapes are latent). We have developed a prototype which is able to infer shapes of classes in a knowledge graph and used it with classes of DBpedia ontology. We serialize our results using ShEx.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1400380328",
                    "name": "Daniel Fern\u00e1ndez-\u00c1lvarez"
                },
                {
                    "authorId": "1409722166",
                    "name": "Herminio Garc\u00eda-Gonz\u00e1lez"
                },
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                },
                {
                    "authorId": "1729679",
                    "name": "Jose Emilio Labra Gayo"
                }
            ]
        },
        {
            "paperId": "7600ad22e3e80871acc1f3218ca338858bf2312a",
            "title": "Predicting incorrect mappings: a data-driven approach applied to DBpedia",
            "abstract": "DBpedia releases consist of more than 70 multilingual datasets that cover data extracted from different language-specific Wikipedia instances. The data extracted from those Wikipedia instances are transformed into RDF using mappings created by the DBpedia community. Nevertheless, not all the mappings are correct and consistent across all the distinct language-specific DBpedia datasets. As these incorrect mappings are spread in a large number of mappings, it is not feasible to inspect all such mappings manually to ensure their correctness. Thus, the goal of this work is to propose a data-driven method to detect incorrect mappings automatically by analyzing the information from both instance data as well as ontological axioms. We propose a machine learning based approach to building a predictive model which can detect incorrect mappings. We have evaluated different supervised classification algorithms for this task and our best model achieves 93% accuracy. These results help us to detect incorrect mappings and achieve a high-quality DBpedia.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145000677",
                    "name": "M. Rico"
                },
                {
                    "authorId": "2689774",
                    "name": "Nandana Mihindukulasooriya"
                },
                {
                    "authorId": "2627116",
                    "name": "D. Kontokostas"
                },
                {
                    "authorId": "1802726",
                    "name": "Heiko Paulheim"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                },
                {
                    "authorId": "1398348796",
                    "name": "Asunci\u00f3n G\u00f3mez-P\u00e9rez"
                }
            ]
        },
        {
            "paperId": "0b9c2d76835f45639a20b4a8a9d6818897916f84",
            "title": "IDOL: Comprehensive & Complete LOD Insights",
            "abstract": "Over the last decade, we observed a steadily increasing amount of RDF datasets made available on the web of data. The decentralized nature of the web, however, makes it hard to identify all these datasets. Even more so, when downloadable data distributions are discovered, only insufficient metadata is available to describe the datasets properly, thus posing barriers on its usefulness and reuse. In this paper, we describe an attempt to exhaustively identify the whole linked open data cloud by harvesting metadata from multiple sources, providing insights about duplicated data and the general quality of the available metadata. This was only possible by using a probabilistic data structure called Bloom filter. Finally, we published a dump file containing metadata which can further be used to enrich existent datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32124073",
                    "name": "C. Baron"
                },
                {
                    "authorId": "2627116",
                    "name": "D. Kontokostas"
                },
                {
                    "authorId": "35114918",
                    "name": "Amit Kirschenbaum"
                },
                {
                    "authorId": "3489323",
                    "name": "G. Publio"
                },
                {
                    "authorId": "145538480",
                    "name": "Diego Esteves"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        }
    ]
}