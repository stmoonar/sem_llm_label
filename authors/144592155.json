{
    "authorId": "144592155",
    "papers": [
        {
            "paperId": "2c4b9e3c85876c71ea14229b8d3b59f2bc3cdf99",
            "title": "MIRAI: Evaluating LLM Agents for Event Forecasting",
            "abstract": "Recent advancements in Large Language Models (LLMs) have empowered LLM agents to autonomously collect world information, over which to conduct reasoning to solve complex problems. Given this capability, increasing interests have been put into employing LLM agents for predicting international events, which can influence decision-making and shape policy development on an international scale. Despite such a growing interest, there is a lack of a rigorous benchmark of LLM agents' forecasting capability and reliability. To address this gap, we introduce MIRAI, a novel benchmark designed to systematically evaluate LLM agents as temporal forecasters in the context of international events. Our benchmark features an agentic environment with tools for accessing an extensive database of historical, structured events and textual news articles. We refine the GDELT event database with careful cleaning and parsing to curate a series of relational prediction tasks with varying forecasting horizons, assessing LLM agents' abilities from short-term to long-term forecasting. We further implement APIs to enable LLM agents to utilize different tools via a code-based interface. In summary, MIRAI comprehensively evaluates the agents' capabilities in three dimensions: 1) autonomously source and integrate critical information from large global databases; 2) write codes using domain-specific APIs and libraries for tool-use; and 3) jointly reason over historical knowledge from diverse formats and time to accurately predict future events. Through comprehensive benchmarking, we aim to establish a reliable framework for assessing the capabilities of LLM agents in forecasting international events, thereby contributing to the development of more accurate and trustworthy models for international relation analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2306897813",
                    "name": "Chenchen Ye"
                },
                {
                    "authorId": "2309190965",
                    "name": "Ziniu Hu"
                },
                {
                    "authorId": "2303979338",
                    "name": "Yihe Deng"
                },
                {
                    "authorId": "2309201786",
                    "name": "Zijie Huang"
                },
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "2303919444",
                    "name": "Wei Wang"
                }
            ]
        },
        {
            "paperId": "4a776e757e46c0c41020cb15e574a62aceb20cf5",
            "title": "Instructional Fingerprinting of Large Language Models",
            "abstract": "The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (eg restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110519123",
                    "name": "Jiashu Xu"
                },
                {
                    "authorId": "2267154244",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "2303396379",
                    "name": "Pang Wei Koh"
                },
                {
                    "authorId": "2256992327",
                    "name": "Chaowei Xiao"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                }
            ]
        },
        {
            "paperId": "9957879871784d9d2b352a31bda2abbecfd65476",
            "title": "Improving Event Definition Following For Zero-Shot Event Detection",
            "abstract": "Existing approaches on zero-shot event detection usually train models on datasets annotated with known event types, and prompt them with unseen event definitions. These approaches yield sporadic successes, yet generally fall short of expectations. In this work, we aim to improve zero-shot event detection by training models to better follow event definitions. We hypothesize that a diverse set of event types and definitions are the key for models to learn to follow event definitions while existing event extraction datasets focus on annotating many high-quality examples for a few event types. To verify our hypothesis, we construct an automatically generated Diverse Event Definition (DivED) dataset and conduct comparative studies. Our experiments reveal that a large number of event types (200) and diverse event definitions can significantly boost event extraction performance; on the other hand, the performance does not scale with over ten examples per event type. Beyond scaling, we incorporate event ontology information and hard-negative samples during training, further boosting the performance. Based on these findings, we fine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that surpasses SOTA large language models like GPT-3.5 across three open benchmarks on zero-shot event detection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117632647",
                    "name": "Zefan Cai"
                },
                {
                    "authorId": "2008339028",
                    "name": "Po-Nien Kung"
                },
                {
                    "authorId": "51153314",
                    "name": "Ashima Suvarna"
                },
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "103404553",
                    "name": "Hritik Bansal"
                },
                {
                    "authorId": "2261083637",
                    "name": "Baobao Chang"
                },
                {
                    "authorId": "1970636",
                    "name": "P. Brantingham"
                },
                {
                    "authorId": "2256611613",
                    "name": "Wei Wang"
                },
                {
                    "authorId": "2253599901",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "b87fb8913486bb7ef2d9c2b5b0dfcb96116d5f10",
            "title": "CLIMB: A Benchmark of Clinical Bias in Large Language Models",
            "abstract": "Large language models (LLMs) are increasingly applied to clinical decision-making. However, their potential to exhibit bias poses significant risks to clinical equity. Currently, there is a lack of benchmarks that systematically evaluate such clinical bias in LLMs. While in downstream tasks, some biases of LLMs can be avoided such as by instructing the model to answer\"I'm not sure...\", the internal bias hidden within the model still lacks deep studies. We introduce CLIMB (shorthand for A Benchmark of Clinical Bias in Large Language Models), a pioneering comprehensive benchmark to evaluate both intrinsic (within LLMs) and extrinsic (on downstream tasks) bias in LLMs for clinical decision tasks. Notably, for intrinsic bias, we introduce a novel metric, AssocMAD, to assess the disparities of LLMs across multiple demographic groups. Additionally, we leverage counterfactual intervention to evaluate extrinsic bias in a task of clinical diagnosis prediction. Our experiments across popular and medically adapted LLMs, particularly from the Mistral and LLaMA families, unveil prevalent behaviors with both intrinsic and extrinsic bias. This work underscores the critical need to mitigate clinical bias and sets a new standard for future evaluations of LLMs' clinical bias.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2310387054",
                    "name": "Yubo Zhang"
                },
                {
                    "authorId": "2191714074",
                    "name": "Shudi Hou"
                },
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "2256611613",
                    "name": "Wei Wang"
                },
                {
                    "authorId": "2311993454",
                    "name": "Muhao Chen"
                },
                {
                    "authorId": "2310387763",
                    "name": "Jieyu Zhao"
                }
            ]
        },
        {
            "paperId": "d302a2e405d13917f8f740880c6d6bf4fe3f8298",
            "title": "CliBench: Multifaceted Evaluation of Large Language Models in Clinical Decisions on Diagnoses, Procedures, Lab Tests Orders and Prescriptions",
            "abstract": "The integration of Artificial Intelligence (AI), especially Large Language Models (LLMs), into the clinical diagnosis process offers significant potential to improve the efficiency and accessibility of medical care. While LLMs have shown some promise in the medical domain, their application in clinical diagnosis remains underexplored, especially in real-world clinical practice, where highly sophisticated, patient-specific decisions need to be made. Current evaluations of LLMs in this field are often narrow in scope, focusing on specific diseases or specialties and employing simplified diagnostic tasks. To bridge this gap, we introduce CliBench, a novel benchmark developed from the MIMIC IV dataset, offering a comprehensive and realistic assessment of LLMs' capabilities in clinical diagnosis. This benchmark not only covers diagnoses from a diverse range of medical cases across various specialties but also incorporates tasks of clinical significance: treatment procedure identification, lab test ordering and medication prescriptions. Supported by structured output ontologies, CliBench enables a precise and multi-granular evaluation, offering an in-depth understanding of LLM's capability on diverse clinical tasks of desired granularity. We conduct a zero-shot evaluation of leading LLMs to assess their proficiency in clinical decision-making. Our preliminary results shed light on the potential and limitations of current LLMs in clinical settings, providing valuable insights for future advancements in LLM-powered healthcare.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "2306897813",
                    "name": "Chenchen Ye"
                },
                {
                    "authorId": "2117822989",
                    "name": "Yu Yan"
                },
                {
                    "authorId": "2224092326",
                    "name": "Xiaoxuan Wang"
                },
                {
                    "authorId": "2254277920",
                    "name": "Peipei Ping"
                },
                {
                    "authorId": "2307042380",
                    "name": "Timothy S Chang"
                },
                {
                    "authorId": "2267330481",
                    "name": "Wei Wang"
                }
            ]
        },
        {
            "paperId": "fdb3864b5c8914adedfce0245648604191e6e38e",
            "title": "MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding",
            "abstract": "We introduce MuirBench, a comprehensive benchmark that focuses on robust multi-image understanding capabilities of multimodal LLMs. MuirBench consists of 12 diverse multi-image tasks (e.g., scene understanding, ordering) that involve 10 categories of multi-image relations (e.g., multiview, temporal relations). Comprising 11,264 images and 2,600 multiple-choice questions, MuirBench is created in a pairwise manner, where each standard instance is paired with an unanswerable variant that has minimal semantic differences, in order for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our results reveal that even the best-performing models like GPT-4o and Gemini Pro find it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy. Open-source multimodal LLMs trained on single images can hardly generalize to multi-image questions, hovering below 33.3% in accuracy. These results highlight the importance of MuirBench in encouraging the community to develop multimodal LLMs that can look beyond a single image, suggesting potential pathways for future improvements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267154244",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "2078360",
                    "name": "Xingyu Fu"
                },
                {
                    "authorId": "2110302673",
                    "name": "James Y. Huang"
                },
                {
                    "authorId": "2306133127",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2267021590",
                    "name": "Qin Liu"
                },
                {
                    "authorId": "2257096300",
                    "name": "Xiaogeng Liu"
                },
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "2257007579",
                    "name": "Nan Xu"
                },
                {
                    "authorId": "2203076",
                    "name": "Wenxuan Zhou"
                },
                {
                    "authorId": "145086492",
                    "name": "Kai Zhang"
                },
                {
                    "authorId": "2284682152",
                    "name": "Tianyi Yan"
                },
                {
                    "authorId": "2266978162",
                    "name": "W. Mo"
                },
                {
                    "authorId": "2306480671",
                    "name": "Hsiang-Hui Liu"
                },
                {
                    "authorId": "2887562",
                    "name": "Pan Lu"
                },
                {
                    "authorId": "2287283699",
                    "name": "Chunyuan Li"
                },
                {
                    "authorId": "2256992327",
                    "name": "Chaowei Xiao"
                },
                {
                    "authorId": "2278984743",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "2297187000",
                    "name": "Dan Roth"
                },
                {
                    "authorId": "2267010012",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                }
            ]
        },
        {
            "paperId": "43cbfe896a3b3e5194f43a2161f40e29463ff29d",
            "title": "Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach (preprint)",
            "abstract": "Susceptibility to misinformation describes the degree of belief in unverifiable claims, a latent aspect of individuals' mental processes that is not observable. Existing susceptibility studies heavily rely on self-reported beliefs, which can be subject to bias, expensive to collect, and challenging to scale for downstream applications. To address these limitations, in this work, we propose a computational approach to model users' latent susceptibility levels. As shown in previous research, susceptibility is influenced by various factors (e.g., demographic factors, political ideology), and directly influences people's reposting behavior on social media. To represent the underlying mental process, our susceptibility modeling incorporates these factors as inputs, guided by the supervision of people's sharing behavior. Using COVID-19 as a testbed domain, our experiments demonstrate a significant alignment between the susceptibility scores estimated by our computational modeling and human judgments, confirming the effectiveness of this latent modeling approach. Furthermore, we apply our model to annotate susceptibility scores on a large-scale dataset and analyze the relationships between susceptibility with various factors. Our analysis reveals that political leanings and psychological factors exhibit varying degrees of association with susceptibility to COVID-19 misinformation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108082280",
                    "name": "Yanchen Liu"
                },
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "2253469011",
                    "name": "Wenna Qin"
                },
                {
                    "authorId": "2253463535",
                    "name": "Azure Zhou"
                },
                {
                    "authorId": "47739850",
                    "name": "Jiaao Chen"
                },
                {
                    "authorId": "2266973626",
                    "name": "Weiyan Shi"
                },
                {
                    "authorId": "2256611613",
                    "name": "Wei Wang"
                },
                {
                    "authorId": "2263629011",
                    "name": "Diyi Yang"
                }
            ]
        },
        {
            "paperId": "82fe948f18ca0138d035f553286c5e4b712dbdbe",
            "title": "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models",
            "abstract": "We investigate security concerns of the emergent instruction tuning paradigm, that models are trained on crowdsourced datasets with task instructions to achieve superior performance. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions (~1000 tokens) and control model behavior through data poisoning, without even the need to modify data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets. As an empirical study on instruction attacks, we systematically evaluated unique perspectives of instruction attacks, such as poison transfer where poisoned models can transfer to 15 diverse generative datasets in a zero-shot manner; instruction transfer where attackers can directly apply poisoned instruction on many other datasets; and poison resistance to continual finetuning. Lastly, we show that RLHF and clean demonstrations might mitigate such backdoors to some degree. These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110519123",
                    "name": "Jiashu Xu"
                },
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "47939052",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "2723309",
                    "name": "Chaowei Xiao"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                }
            ]
        },
        {
            "paperId": "ad468ac7902c936b73bd4e1400f3af62a0444bb7",
            "title": "STAR: Boosting Low-Resource Information Extraction by Structure-to-Text Data Generation with Large Language Models",
            "abstract": "Information extraction tasks such as event extraction require an in-depth understanding of the output structure and sub-task dependencies. They heavily rely on task-specific training data in the form of (passage, target structure) pairs to obtain reasonable performance. However, obtaining such data through human annotation is costly, leading to a pressing need for low-resource information extraction approaches that require minimal human labeling for real-world applications. Fine-tuning supervised models with synthesized training data would be a generalizable method, but the existing data generation methods either still rely on large-scale ground-truth data or cannot be applied to complicated IE tasks due to their poor performance. To address these challenges, we propose STAR, a data generation method that leverages Large Language Models (LLMs) to synthesize data instances given limited seed demonstrations, thereby boosting low-resource information extraction performance. Our approach involves generating target structures (Y) followed by generating passages (X), all accomplished with the aid of LLMs. We design fine-grained step-by-step instructions to obtain the initial data instances. We further reduce errors and improve data quality through self-reflection error identification and self-refinement with iterative revision. Our experiments show that the data generated by STAR significantly improve the performance of low-resource event extraction and relation extraction tasks, even surpassing the effectiveness of human-curated data. Human assessment of the data quality shows STAR-generated data exhibit higher passage quality and better align with the task definitions compared with the human-curated data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "2108170326",
                    "name": "Xiaoxu Wang"
                },
                {
                    "authorId": "2008339028",
                    "name": "Po-Nien Kung"
                },
                {
                    "authorId": "1970636",
                    "name": "P. Brantingham"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                },
                {
                    "authorId": "145200778",
                    "name": "Wei Wang"
                }
            ]
        },
        {
            "paperId": "bfa6d9c1ad7ba2c4f5b1806021f3e1999d0e5d7e",
            "title": "MIDDAG: Where Does Our News Go? Investigating Information Diffusion via Community-Level Information Pathways",
            "abstract": "We present MIDDAG, an intuitive, interactive system that visualizes the information propagation paths on social media triggered by COVID-19-related news articles accompanied by comprehensive insights including user/community susceptibility level, as well as events and popular opinions raised by the crowd while propagating the information. Besides discovering information flow patterns among users, we construct communities among users and develop the propagation forecasting capability, enabling tracing and understanding of how information is disseminated at a higher level. A demo video and more are available at https://info-pathways.github.io.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "2223976510",
                    "name": "Alexander K. Taylor"
                },
                {
                    "authorId": "2253657042",
                    "name": "Nuan Wen"
                },
                {
                    "authorId": "2108082280",
                    "name": "Yanchen Liu"
                },
                {
                    "authorId": "2008339028",
                    "name": "Po-Nien Kung"
                },
                {
                    "authorId": "2253469011",
                    "name": "Wenna Qin"
                },
                {
                    "authorId": "2253451145",
                    "name": "Shicheng Wen"
                },
                {
                    "authorId": "2253463535",
                    "name": "Azure Zhou"
                },
                {
                    "authorId": "2254124345",
                    "name": "Diyi Yang"
                },
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "2253599901",
                    "name": "Nanyun Peng"
                },
                {
                    "authorId": "2256611613",
                    "name": "Wei Wang"
                }
            ]
        }
    ]
}