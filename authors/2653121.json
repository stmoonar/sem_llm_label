{
    "authorId": "2653121",
    "papers": [
        {
            "paperId": "2c4b9e3c85876c71ea14229b8d3b59f2bc3cdf99",
            "title": "MIRAI: Evaluating LLM Agents for Event Forecasting",
            "abstract": "Recent advancements in Large Language Models (LLMs) have empowered LLM agents to autonomously collect world information, over which to conduct reasoning to solve complex problems. Given this capability, increasing interests have been put into employing LLM agents for predicting international events, which can influence decision-making and shape policy development on an international scale. Despite such a growing interest, there is a lack of a rigorous benchmark of LLM agents' forecasting capability and reliability. To address this gap, we introduce MIRAI, a novel benchmark designed to systematically evaluate LLM agents as temporal forecasters in the context of international events. Our benchmark features an agentic environment with tools for accessing an extensive database of historical, structured events and textual news articles. We refine the GDELT event database with careful cleaning and parsing to curate a series of relational prediction tasks with varying forecasting horizons, assessing LLM agents' abilities from short-term to long-term forecasting. We further implement APIs to enable LLM agents to utilize different tools via a code-based interface. In summary, MIRAI comprehensively evaluates the agents' capabilities in three dimensions: 1) autonomously source and integrate critical information from large global databases; 2) write codes using domain-specific APIs and libraries for tool-use; and 3) jointly reason over historical knowledge from diverse formats and time to accurately predict future events. Through comprehensive benchmarking, we aim to establish a reliable framework for assessing the capabilities of LLM agents in forecasting international events, thereby contributing to the development of more accurate and trustworthy models for international relation analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2306897813",
                    "name": "Chenchen Ye"
                },
                {
                    "authorId": "2309190965",
                    "name": "Ziniu Hu"
                },
                {
                    "authorId": "2303979338",
                    "name": "Yihe Deng"
                },
                {
                    "authorId": "2309201786",
                    "name": "Zijie Huang"
                },
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "2303919444",
                    "name": "Wei Wang"
                }
            ]
        },
        {
            "paperId": "a6c7bcb5c43b98abb153c3fa3da80f3b525e6eac",
            "title": "Molecular Contrastive Pretraining with Collaborative Featurizations",
            "abstract": "Molecular pretraining, which learns molecular representations over massive unlabeled data, has become a prominent paradigm to solve a variety of tasks in computational chemistry and drug discovery. Recently, prosperous progress has been made in molecular pretraining with different molecular featurizations, including 1D SMILES strings, 2D graphs, and 3D geometries. However, the role of molecular featurizations with their corresponding neural architectures in molecular pretraining remains largely unexamined. In this paper, through two case studies\u2500chirality classification and aromatic ring counting\u2500we first demonstrate that different featurization techniques convey chemical information differently. In light of this observation, we propose a simple and effective MOlecular pretraining framework with COllaborative featurizations (MOCO). MOCO comprehensively leverages multiple featurizations that complement each other and outperforms existing state-of-the-art models that solely rely on one or two featurizations on a wide range of molecular property prediction tasks.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "2109123910",
                    "name": "Dingshuo Chen"
                },
                {
                    "authorId": "2249855934",
                    "name": "Yuanqi Du"
                },
                {
                    "authorId": "2282904722",
                    "name": "Yingze Wang"
                },
                {
                    "authorId": "48873756",
                    "name": "Q. Liu"
                },
                {
                    "authorId": "50425438",
                    "name": "Shu Wu"
                }
            ]
        },
        {
            "paperId": "b26f66c9bb59811c7d67cdab91a0d65d15364836",
            "title": "An Evaluation of Large Language Models in Bioinformatics Research",
            "abstract": "Large language models (LLMs) such as ChatGPT have gained considerable interest across diverse research communities. Their notable ability for text completion and generation has inaugurated a novel paradigm for language-interfaced problem solving. However, the potential and efficacy of these models in bioinformatics remain incompletely explored. In this work, we study the performance LLMs on a wide spectrum of crucial bioinformatics tasks. These tasks include the identification of potential coding regions, extraction of named entities for genes and proteins, detection of antimicrobial and anti-cancer peptides, molecular optimization, and resolution of educational bioinformatics problems. Our findings indicate that, given appropriate prompts, LLMs like GPT variants can successfully handle most of these tasks. In addition, we provide a thorough analysis of their limitations in the context of complicated bioinformatics tasks. In conclusion, we believe that this work can provide new perspectives and motivate future research in the field of LLMs applications, AI for Science and bioinformatics.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "2284946149",
                    "name": "Hengchuang Yin"
                },
                {
                    "authorId": "2257361455",
                    "name": "Zhonghui Gu"
                },
                {
                    "authorId": "2285010258",
                    "name": "Fanhao Wang"
                },
                {
                    "authorId": "2284872090",
                    "name": "Yiparemu Abuduhaibaier"
                },
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "2284869299",
                    "name": "Xinming Tu"
                },
                {
                    "authorId": "2238119871",
                    "name": "Xian-Sheng Hua"
                },
                {
                    "authorId": "2220669584",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2260436235",
                    "name": "Yizhou Sun"
                }
            ]
        },
        {
            "paperId": "b85c299e556c1b0b3d66787b31a25d54e7528283",
            "title": "Efficient Evolutionary Search Over Chemical Space with Large Language Models",
            "abstract": "Molecular discovery, when formulated as an optimization problem, presents significant computational challenges because optimization objectives can be non-differentiable. Evolutionary Algorithms (EAs), often used to optimize black-box objectives in molecular discovery, traverse chemical space by performing random mutations and crossovers, leading to a large number of expensive objective evaluations. In this work, we ameliorate this shortcoming by incorporating chemistry-aware Large Language Models (LLMs) into EAs. Namely, we redesign crossover and mutation operations in EAs using LLMs trained on large corpora of chemical information. We perform extensive empirical studies on both commercial and open-source models on multiple tasks involving property optimization, molecular rediscovery, and structure-based drug design, demonstrating that the joint usage of LLMs with EAs yields superior performance over all baseline models across single- and multi-objective settings. We demonstrate that our algorithm improves both the quality of the final solution and convergence speed, thereby reducing the number of required objective evaluations. Our code is available at http://github.com/zoom-wang112358/MOLLEO",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2266420540",
                    "name": "Haorui Wang"
                },
                {
                    "authorId": "2176083515",
                    "name": "Marta Skreta"
                },
                {
                    "authorId": "92791502",
                    "name": "C. Ser"
                },
                {
                    "authorId": "2309242877",
                    "name": "Wenhao Gao"
                },
                {
                    "authorId": "2865034",
                    "name": "Lingkai Kong"
                },
                {
                    "authorId": "2308098758",
                    "name": "Felix Streith-Kalthoff"
                },
                {
                    "authorId": "2300174373",
                    "name": "Chenru Duan"
                },
                {
                    "authorId": "8103389",
                    "name": "Yuchen Zhuang"
                },
                {
                    "authorId": "2259265562",
                    "name": "Yue Yu"
                },
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "2257386088",
                    "name": "Yuanqi Du"
                },
                {
                    "authorId": "2261813847",
                    "name": "Al\u00e1n Aspuru-Guzik"
                },
                {
                    "authorId": "9044665",
                    "name": "Kirill Neklyudov"
                },
                {
                    "authorId": "2305503379",
                    "name": "Chao Zhang"
                }
            ]
        },
        {
            "paperId": "d276a83b9bd5aaf75c6d921841959118e0d59870",
            "title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers",
            "abstract": "Developing effective biomedical retrieval models is important for excelling at knowledge-intensive biomedical tasks but still challenging due to the deficiency of sufficient publicly annotated biomedical data and computational resources. We present BMRetriever, a series of dense retrievers for enhancing biomedical retrieval via unsupervised pre-training on large biomedical corpora, followed by instruction fine-tuning on a combination of labeled datasets and synthetic pairs. Experiments on 5 biomedical tasks across 11 datasets verify BMRetriever's efficacy on various biomedical applications. BMRetriever also exhibits strong parameter efficiency, with the 410M variant outperforming baselines up to 11.7 times larger, and the 2B variant matching the performance of models with over 5B parameters. The training data and model checkpoints are released at \\url{https://huggingface.co/BMRetriever} to ensure transparency, reproducibility, and application to new domains.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "2265148831",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "2263890944",
                    "name": "Wenqi Shi"
                },
                {
                    "authorId": "2218865512",
                    "name": "Yue Yu"
                },
                {
                    "authorId": "8103389",
                    "name": "Yuchen Zhuang"
                },
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "2237844925",
                    "name": "M. D. Wang"
                },
                {
                    "authorId": "2263536473",
                    "name": "Joyce C. Ho"
                },
                {
                    "authorId": "2256776233",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "2237940940",
                    "name": "Carl Yang"
                }
            ]
        },
        {
            "paperId": "08221c085f89b7b69e629e04b9cbba4c76b8c983",
            "title": "Unsupervised Graph Representation Learning with Cluster-aware Self-training and Refining",
            "abstract": "Unsupervised graph representation learning aims to learn low-dimensional node embeddings without supervision while preserving graph topological structures and node attributive features. Previous Graph Neural Networks (GNN) require a large number of labeled nodes, which may not be accessible in real-world applications. To this end, we present a novel unsupervised graph neural network model with Cluster-aware Self-training and Refining (CLEAR). Specifically, in the proposed CLEAR model, we perform clustering on the node embeddings and update the model parameters by predicting the cluster assignments. To avoid degenerate solutions of clustering, we formulate the graph clustering problem as an optimal transport problem and leverage a balanced clustering strategy. Moreover, we observe that graphs often contain inter-class edges, which mislead the GNN model to aggregate noisy information from neighborhood nodes. Therefore, we propose to refine the graph topology by strengthening intra-class edges and reducing node connections between different classes based on cluster labels, which better preserves cluster structures in the embedding space. We conduct comprehensive experiments on two benchmark tasks using real-world datasets. The results demonstrate the superior performance of the proposed model over baseline methods. Notably, our model gains over 7% improvements in terms of accuracy on node clustering over state-of-the-arts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "47103911",
                    "name": "Yichen Xu"
                },
                {
                    "authorId": "2155385667",
                    "name": "Feng Yu"
                },
                {
                    "authorId": "48873756",
                    "name": "Q. Liu"
                },
                {
                    "authorId": "50425438",
                    "name": "Shu Wu"
                }
            ]
        },
        {
            "paperId": "45a4e7d72dc559d5a582c89ad02c9f145463e84f",
            "title": "M2Hub: Unlocking the Potential of Machine Learning for Materials Discovery",
            "abstract": "We introduce M$^2$Hub, a toolkit for advancing machine learning in materials discovery. Machine learning has achieved remarkable progress in modeling molecular structures, especially biomolecules for drug discovery. However, the development of machine learning approaches for modeling materials structures lag behind, which is partly due to the lack of an integrated platform that enables access to diverse tasks for materials discovery. To bridge this gap, M$^2$Hub will enable easy access to materials discovery tasks, datasets, machine learning methods, evaluations, and benchmark results that cover the entire workflow. Specifically, the first release of M$^2$Hub focuses on three key stages in materials discovery: virtual screening, inverse design, and molecular simulation, including 9 datasets that covers 6 types of materials with 56 tasks across 8 types of material properties. We further provide 2 synthetic datasets for the purpose of generative tasks on materials. In addition to random data splits, we also provide 3 additional data partitions to reflect the real-world materials discovery scenarios. State-of-the-art machine learning methods (including those are suitable for materials structures but never compared in the literature) are benchmarked on representative tasks. Our codes and library are publicly available at https://github.com/yuanqidu/M2Hub.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "93584228",
                    "name": "Yuanqi Du"
                },
                {
                    "authorId": "2107962435",
                    "name": "Yingheng Wang"
                },
                {
                    "authorId": "2185656535",
                    "name": "Yin-Hua Huang"
                },
                {
                    "authorId": "2177387927",
                    "name": "Jianan Canal Li"
                },
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "49902007",
                    "name": "T. Xie"
                },
                {
                    "authorId": "35473448",
                    "name": "Chenru Duan"
                },
                {
                    "authorId": "2377945",
                    "name": "J. Gregoire"
                },
                {
                    "authorId": "2082417220",
                    "name": "Carla P. Gomes"
                }
            ]
        },
        {
            "paperId": "4993258852711c4e3d0011325ac3db680eae84f4",
            "title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
            "abstract": "Most of the existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that the current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2224092326",
                    "name": "Xiaoxuan Wang"
                },
                {
                    "authorId": "3407296",
                    "name": "Ziniu Hu"
                },
                {
                    "authorId": "2887562",
                    "name": "Pan Lu"
                },
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "47540245",
                    "name": "Jieyu Zhang"
                },
                {
                    "authorId": "2224018745",
                    "name": "Satyen Subramaniam"
                },
                {
                    "authorId": "2224017080",
                    "name": "Arjun R. Loomba"
                },
                {
                    "authorId": "2145408511",
                    "name": "Shichang Zhang"
                },
                {
                    "authorId": "2109461904",
                    "name": "Yizhou Sun"
                },
                {
                    "authorId": "2158624285",
                    "name": "Wei Wang"
                }
            ]
        },
        {
            "paperId": "4ac14a3d12e027056e57c5a05b0414b6b0ff7002",
            "title": "Deep Dag Learning of Effective Brain Connectivity for FMRI Analysis",
            "abstract": "Functional magnetic resonance imaging (fMRI) has become one of the most common imaging modalities for brain function analysis. Recently, graph neural networks (GNN) have been adopted for fMRI analysis with superior performance. Unfortunately, traditional functional brain networks are mainly constructed based on similarities among region of interests (ROIs), which are noisy and can lead to inferior results for GNN models. To better adapt GNNs for fMRI analysis, we propose DABNet, a Deep DAG learning framework based on Brain Networks for fMRI analysis. DABNet adopts a brain network generator module, which harnesses the DAG learning approach to transform the raw time-series into effective brain connectivities. Experiments on two fMRI datasets demonstrate the efficacy of DABNet. The generated brain networks also highlight the prediction-related brain regions and thus provide interpretations for predictions.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1633124736",
                    "name": "Yue Yu"
                },
                {
                    "authorId": "2052319438",
                    "name": "Xuan Kan"
                },
                {
                    "authorId": "2112821580",
                    "name": "Hejie Cui"
                },
                {
                    "authorId": "47462790",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "8499589",
                    "name": "Yujia Zheng"
                },
                {
                    "authorId": "19214393",
                    "name": "Xiangchen Song"
                },
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                },
                {
                    "authorId": "41034714",
                    "name": "Razieh Nabi"
                },
                {
                    "authorId": "2153202261",
                    "name": "Ying Guo"
                },
                {
                    "authorId": "40422511",
                    "name": "C. Zhang"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                }
            ]
        },
        {
            "paperId": "58145d3d9379e9c013c56bd83553128aaea59429",
            "title": "Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks",
            "abstract": "Molecular Representation Learning (MRL) has proven impactful in numerous biochemical applications such as drug discovery and enzyme design. While Graph Neural Networks (GNNs) are effective at learning molecular representations from a 2D molecular graph or a single 3D structure, existing works often overlook the flexible nature of molecules, which continuously interconvert across conformations via chemical bond rotations and minor vibrational perturbations. To better account for molecular flexibility, some recent works formulate MRL as an ensemble learning problem, focusing on explicitly learning from a set of conformer structures. However, most of these studies have limited datasets, tasks, and models. In this work, we introduce the first MoleculAR Conformer Ensemble Learning (MARCEL) benchmark to thoroughly evaluate the potential of learning on conformer ensembles and suggest promising research directions. MARCEL includes four datasets covering diverse molecule- and reaction-level properties of chemically diverse molecules including organocatalysts and transition-metal catalysts, extending beyond the scope of common GNN benchmarks that are confined to drug-like molecules. In addition, we conduct a comprehensive empirical study, which benchmarks representative 1D, 2D, and 3D molecular representation learning models, along with two strategies that explicitly incorporate conformer ensembles into 3D MRL models. Our findings reveal that direct learning from an accessible conformer space can improve performance on a variety of tasks and models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "2253658207",
                    "name": "Jeehyun Hwang"
                },
                {
                    "authorId": "2249762898",
                    "name": "Keir Adams"
                },
                {
                    "authorId": "2249924165",
                    "name": "Zhen Liu"
                },
                {
                    "authorId": "2133509770",
                    "name": "B. Nan"
                },
                {
                    "authorId": "1499248915",
                    "name": "Brock A. Stenfors"
                },
                {
                    "authorId": "2249855934",
                    "name": "Yuanqi Du"
                },
                {
                    "authorId": "2249762513",
                    "name": "Jatin Chauhan"
                },
                {
                    "authorId": "2249763890",
                    "name": "Olaf Wiest"
                },
                {
                    "authorId": "2385206",
                    "name": "O. Isayev"
                },
                {
                    "authorId": "13027820",
                    "name": "Connor W. Coley"
                },
                {
                    "authorId": "2244171114",
                    "name": "Yizhou Sun"
                },
                {
                    "authorId": "2257130406",
                    "name": "Wei Wang"
                }
            ]
        }
    ]
}