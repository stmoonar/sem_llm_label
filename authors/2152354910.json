{
    "authorId": "2152354910",
    "papers": [
        {
            "paperId": "3e712cddb347c464ac810dbc7d61a8e1a04978b0",
            "title": "FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering",
            "abstract": "Knowledge base question answering (KBQA) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most KBQA models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce FlexKBQA by utilizing Large Language Models (LLMs) as program translators for addressing the challenges inherent in the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms to sample diverse programs, such as SPARQL queries, from the knowledge base, which are subsequently converted into natural language questions via LLMs. This synthetic dataset facilitates training a specialized lightweight model for the KB. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, FlexKBQA introduces an executionguided self-training method to iterative leverage unlabeled user questions. Furthermore, we explore harnessing the inherent reasoning capability of LLMs to enhance the entire framework. Consequently, FlexKBQA delivers substantial flexibility, encompassing data annotation, deployment, and being domain agnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we observe that under the few-shot even the more challenging zero-shot scenarios, FlexKBQA achieves impressive results with a few annotations, surpassing all previous baselines and even approaching the performance of supervised models, achieving a remarkable 93% performance relative to the fully-supervised models. We posit that FlexKBQA represents a significant advancement towards exploring better integration of large and lightweight models. Code is available at https://github.com/leezythu/FlexKBQA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155354022",
                    "name": "Zhenyu Li"
                },
                {
                    "authorId": "2272781120",
                    "name": "Sunqi Fan"
                },
                {
                    "authorId": "2022231256",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "2116521868",
                    "name": "Xiuxing Li"
                },
                {
                    "authorId": "2150461809",
                    "name": "Zhichao Duan"
                },
                {
                    "authorId": "2057587396",
                    "name": "Bo Dong"
                },
                {
                    "authorId": "2152354910",
                    "name": "Ning Liu"
                },
                {
                    "authorId": "2115642141",
                    "name": "Jianyong Wang"
                }
            ]
        },
        {
            "paperId": "98cf72b7d263e86a41cb7075cd134ff96a885856",
            "title": "Learning Interpretable Rules for Scalable Data Representation and Classification",
            "abstract": "Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. A novel design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Exhaustive experiments on ten small and four large data sets show that RRL outperforms the competitive interpretable approaches and can be easily adjusted to obtain a trade-off between classification accuracy and model complexity for different scenarios.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2261359416",
                    "name": "Zhuo Wang"
                },
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "2152354910",
                    "name": "Ning Liu"
                },
                {
                    "authorId": "2254333075",
                    "name": "Jianyong Wang"
                }
            ]
        },
        {
            "paperId": "c7dbd6c80ae941daf1de81439b0d1f992da130ab",
            "title": "Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today",
            "abstract": "Recent investigations show that large language models (LLMs), specifically GPT-4, not only have remarkable capabilities in common Natural Language Processing (NLP) tasks but also exhibit human-level performance on various professional and academic benchmarks. However, whether GPT-4 can be directly used in practical applications and replace traditional artificial intelligence (AI) tools in specialized domains requires further experimental validation. In this paper, we explore the potential of LLMs such as GPT-4 to outperform traditional AI tools in dementia diagnosis. Comprehensive comparisons between GPT-4 and traditional AI tools are conducted to examine their diagnostic accuracy in a clinical setting. Experimental results on two real clinical datasets show that, although LLMs like GPT-4 demonstrate potential for future advancements in dementia diagnosis, they currently do not surpass the performance of traditional AI tools. The interpretability and faithfulness of GPT-4 are also evaluated by comparison with real doctors. We discuss the limitations of GPT-4 in its current state and propose future research directions to enhance GPT-4 in dementia diagnosis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218836424",
                    "name": "Zhuo Wang"
                },
                {
                    "authorId": "2209392929",
                    "name": "R. Li"
                },
                {
                    "authorId": "50660465",
                    "name": "Bowen Dong"
                },
                {
                    "authorId": "31737970",
                    "name": "Jie Wang"
                },
                {
                    "authorId": "2116521868",
                    "name": "Xiuxing Li"
                },
                {
                    "authorId": "2152354910",
                    "name": "Ning Liu"
                },
                {
                    "authorId": "39245374",
                    "name": "C. Mao"
                },
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "5851496",
                    "name": "L. Dong"
                },
                {
                    "authorId": "2115556978",
                    "name": "Jing Gao"
                },
                {
                    "authorId": "2115642141",
                    "name": "Jianyong Wang"
                }
            ]
        },
        {
            "paperId": "dde3f99f8ca4ed11076d84c4ae2bce1a77de19ac",
            "title": "Effective Few-Shot Named Entity Linking by Meta-Learning",
            "abstract": "Entity linking aims to link ambiguous mentions to their corresponding entities in a knowledge base, which is significant and fundamental for various downstream applications, e.g., knowledge base completion, question answering, and information extraction. While great efforts have been devoted to this task, most of these studies follow the assumption that large-scale labeled data is available. However, when the labeled data is insufficient for specific domains due to labor-intensive annotation work, the performance of existing algorithms will suffer an intolerable decline. In this paper, we endeavor to solve the problem of few-shot entity linking, which only requires a minimal amount of in-domain labeled data and is more practical in real situations. Specifically, we firstly propose a novel weak supervision strategy to generate non-trivial synthetic entity-mention pairs based on mention rewriting. Since the quality of the synthetic data has a critical impact on effective model training, we further design a meta-learning mechanism to assign different weights to each synthetic entity-mention pair automatically. Through this way, we can profoundly exploit rich and precious semantic information to derive a well-trained entity linking model under the few-shot setting. The experiments on real-world datasets show that the proposed method can extensively improve the state-of-the-art few-shot entity linking model and achieve impressive performance when only a small amount of labeled data is available. Moreover, we also demonstrate the outstanding ability of the model's transferability. Our code and models will be open-sourced.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116521868",
                    "name": "Xiuxing Li"
                },
                {
                    "authorId": "2155354022",
                    "name": "Zhenyu Li"
                },
                {
                    "authorId": "2621696",
                    "name": "Zhengyan Zhang"
                },
                {
                    "authorId": "2152354910",
                    "name": "Ning Liu"
                },
                {
                    "authorId": "34825854",
                    "name": "Haitao Yuan"
                },
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": null,
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2115642141",
                    "name": "Jianyong Wang"
                }
            ]
        },
        {
            "paperId": "d4818e0b979b860e9b2dbb74bb3f4758ab56693e",
            "title": "Jointly Modeling Fact Triples and Text Information for Knowledge Base Completion",
            "abstract": "Knowledge bases become essential resources for many data mining and information retrieval tasks, but they remain far from complete. Knowledge base completion has attracted extensive research efforts from researchers and prac-titioners in diverse areas, which aims to infer missing facts from existing ones in a knowledge base. Quantities of knowledge base completion methods have been developed by regarding each relation as a translation from head entity to tail entity. However, existing methods merely concentrate on fact triples in the knowledge base or co-occurrence of words in the text, while supplementary semantic information expressed via related entities in the text has not been fully exploited. Meanwhile, the representation ability of current methods encounters bottlenecks due to the structure sparseness of knowledge base. In this paper, we propose a novel knowledge base representation learning method by taking advantage of the rich semantic information expressed via related entities in the textual corpus to expand the semantic structure of knowledge base. In this way, our model can break through the limitation of structure sparseness and promote the performance of knowledge base completion. Extensive experiments on two real-world datasets show that the proposed method successfully addresses the above issues and significantly outperforms the state-of-the-art methods on the benchmark task of link prediction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116521868",
                    "name": "Xiuxing Li"
                },
                {
                    "authorId": "2155354022",
                    "name": "Zhenyu Li"
                },
                {
                    "authorId": "2150461809",
                    "name": "Zhichao Duan"
                },
                {
                    "authorId": "2292419240",
                    "name": "Jiacheng Xu"
                },
                {
                    "authorId": "2152354910",
                    "name": "Ning Liu"
                },
                {
                    "authorId": "2115642141",
                    "name": "Jianyong Wang"
                }
            ]
        },
        {
            "paperId": "d69e2356d469139eced650bce41ab2073cf559f8",
            "title": "Scalable Rule-Based Representation Learning for Interpretable Classification",
            "abstract": "Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. An improved design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Exhaustive experiments on nine small and four large data sets show that RRL outperforms the competitive interpretable approaches and can be easily adjusted to obtain a trade-off between classification accuracy and model complexity for different scenarios. Our code is available at: https://github.com/12wang3/rrl.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "101279813",
                    "name": "Zhuo Wang"
                },
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "2152354910",
                    "name": "Ning Liu"
                },
                {
                    "authorId": "2447408",
                    "name": "Jianyong Wang"
                }
            ]
        },
        {
            "paperId": "dba9d6ace6871fffaf5d7b0d437231c2fb01e54f",
            "title": "Bridging the Language Gap: Knowledge Injected Multilingual Question Answering",
            "abstract": "Question Answering (QA) is the task of automati-cally answering questions posed by humans in natural languages. There are different settings to answer a question, such as abstractive, extractive, boolean, and multiple-choice QA. As a popular topic in natural language processing tasks, extractive question answering task (extractive QA) has gained extensive attention in the past few years. With the continuous evolvement of the world, generalized cross-lingual transfer (G-XLT), where question and answer context are in different languages, poses some unique challenges over cross-lingual transfer (XLT), where question and answer context are in the same language. With the boost of corresponding development of related benchmarks, many works have been done to improve the performance of various language QA tasks. However, only a few works are dedicated to the G-XLT task. In this work, we propose a generalized cross-lingual transfer framework to enhance the model's ability to understand different languages. Specifically, we first assemble triples from different languages to form multilingual knowledge. Since the lack of knowledge between different languages greatly limits models' reasoning ability, we further design a knowledge injection strategy via leveraging link prediction techniques to enrich the model storage of multilingual knowledge. In this way, we can profoundly exploit rich semantic knowledge. Experiment results on real-world datasets MLQA demonstrate that the proposed method can improve the performance by a large margin, outperforming the baseline method by 13.18%/12.00% F1/EM on average.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150461809",
                    "name": "Zhichao Duan"
                },
                {
                    "authorId": "2116521868",
                    "name": "Xiuxing Li"
                },
                {
                    "authorId": "2621696",
                    "name": "Zhengyan Zhang"
                },
                {
                    "authorId": "2155354022",
                    "name": "Zhenyu Li"
                },
                {
                    "authorId": "2152354910",
                    "name": "Ning Liu"
                },
                {
                    "authorId": "2115642141",
                    "name": "Jianyong Wang"
                }
            ]
        },
        {
            "paperId": "2ad13e93811d66b2de9b4a6058321c585d6be44d",
            "title": "Transparent Classification with Multilayer Logical Perceptrons and Random Binarization",
            "abstract": "Models with transparent inner structure and high classification performance are required to reduce potential risk and provide trust for users in domains like health care, finance, security, etc. However, existing models are hard to simultaneously satisfy the above two properties. In this paper, we propose a new hierarchical rule-based model for classification tasks, named Concept Rule Sets (CRS), which has both a strong expressive ability and a transparent inner structure. To address the challenge of efficiently learning the non-differentiable CRS model, we propose a novel neural network architecture, Multilayer Logical Perceptron (MLLP), which is a continuous version of CRS. Using MLLP and the Random Binarization (RB) method we proposed, we can search the discrete solution of CRS in continuous space using gradient descent and ensure the discrete CRS acts almost the same as the corresponding continuous MLLP. Experiments on 12 public data sets show that CRS outperforms the state-of-the-art approaches and the complexity of the learned CRS is close to the simple decision tree.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Zhuo Wang"
                },
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "2152354910",
                    "name": "Ning Liu"
                },
                {
                    "authorId": "2447408",
                    "name": "Jianyong Wang"
                }
            ]
        },
        {
            "paperId": "e5a9a094a42fab1f3b3ed74b89c739674d9d11a4",
            "title": "Knowledge-Aware Deep Dual Networks for Text-Based Mortality Prediction",
            "abstract": "Mortality prediction is one of the essential tasks in medical data mining and is significant for inferring clinical outcomes. With a large number of medical notes collected from hospitals, there is an urgent need for developing effective models for predicting mortality based on them. In contrast to structured electronic health records, medical notes are unstructured texts written by experienced caregivers and contain more complicated information about patients, posing more challenges for modeling. Most previous studies rely on tedious hand-crafted features or generating indirect features based on some statistical models such as topic modeling, which might incur information loss for later model training. Recently, some deep models have been proposed to unify the stages of feature construction and model training. However, domain concept knowledge has been neglected, which is important to gain a better understanding of medical notes. To address the above issues, we propose novel Knowledge-aware Deep Dual Networks (K-DDN) for the text-based mortality prediction task. Specifically, a simple deep dual network is first proposed to fuse the representations of medical knowledge and raw text for prediction. Afterward, we incorporate a co-attention mechanism into the basic model, guiding the knowledge and text representation learning with the help of each other. Experimental results on two publicly real-world datasets show the proposed deep dual networks outperform state-of-the-art methods and the co-attention mechanism can further improve the performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152354910",
                    "name": "Ning Liu"
                },
                {
                    "authorId": "2887562",
                    "name": "Pan Lu"
                },
                {
                    "authorId": null,
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "2447408",
                    "name": "Jianyong Wang"
                }
            ]
        }
    ]
}