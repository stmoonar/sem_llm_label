{
    "authorId": "66358686",
    "papers": [
        {
            "paperId": "1f5d49bc63c2f5396ebfe83b39c6ab5d622bc36c",
            "title": "LASO: Language-Guided Affordance Segmentation on 3D Object",
            "abstract": "Segmenting affordance in 3D data is key for bridging perception and action in robots. Existing efforts mostly focus on the visual side and overlook the affordance knowledge from a semantic aspect. This oversight not only limits their generalization to unseen objects, but more importantly, hinders their synergy with large language models (LLMs) which are excellent task planners that can decompose an overarching command into agent-actionable instructions. With this regard, we propose a novel task, Language-guided Affordance Segmentation on 3D Object (LASO), which challenges a model to segment a 3D object's part relevant to a given affordance question. To facilitate the task, we contribute a dataset comprising 19,751 point-question pairs, covering 8434 object shapes and 870 expert-crafted questions. As a pioneer solution, we further propose PointRefer, which highlights an adaptive fusion module to identify target affordance regions at different scales. To ensure a text-aware segmentation, we adopt a set of affordance queries conditioned on linguistic cues to generate dynamic kernels. These kernels are further used to convolute with point features and generate a segmentation mask. Comprehensive experiments and analyses validate PointRefer's effectiveness. With these efforts, We hope that LASO can steer the direction of 3D affordance, guiding it towards enhanced integration with the evolving capabilities of LLMs. Code and data are available at https://github.com/yl3800/LASO.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135358934",
                    "name": "Yicong Li"
                },
                {
                    "authorId": "2321571104",
                    "name": "Na Zhao"
                },
                {
                    "authorId": "66358686",
                    "name": "Junbin Xiao"
                },
                {
                    "authorId": "2224656169",
                    "name": "Chun Feng"
                },
                {
                    "authorId": "2321572172",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "2304746354",
                    "name": "Tat-seng Chua"
                }
            ]
        },
        {
            "paperId": "2a63b499c6fa52d06379d9b8edf229d743be0266",
            "title": "VideoQA in the Era of LLMs: An Empirical Study",
            "abstract": "Video Large Language Models (Video-LLMs) are flourishing and has advanced many video-language tasks. As a golden testbed, Video Question Answering (VideoQA) plays pivotal role in Video-LLM developing. This work conducts a timely and comprehensive study of Video-LLMs' behavior in VideoQA, aiming to elucidate their success and failure modes, and provide insights towards more human-like video understanding and question answering. Our analyses demonstrate that Video-LLMs excel in VideoQA; they can correlate contextual cues and generate plausible responses to questions about varied video contents. However, models falter in handling video temporality, both in reasoning about temporal content ordering and grounding QA-relevant temporal moments. Moreover, the models behave unintuitively - they are unresponsive to adversarial video perturbations while being sensitive to simple variations of candidate answers and questions. Also, they do not necessarily generalize better. The findings demonstrate Video-LLMs' QA capability in standard condition yet highlight their severe deficiency in robustness and interpretability, suggesting the urgent need on rationales in Video-LLM developing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66358686",
                    "name": "Junbin Xiao"
                },
                {
                    "authorId": "2315302469",
                    "name": "Nanxin Huang"
                },
                {
                    "authorId": "2315295623",
                    "name": "Hangyu Qin"
                },
                {
                    "authorId": "2315485445",
                    "name": "Dongyang Li"
                },
                {
                    "authorId": "2135358934",
                    "name": "Yicong Li"
                },
                {
                    "authorId": "31734386",
                    "name": "Fengbin Zhu"
                },
                {
                    "authorId": "2315303735",
                    "name": "Zhulin Tao"
                },
                {
                    "authorId": "2315484507",
                    "name": "Jianxing Yu"
                },
                {
                    "authorId": "2315451665",
                    "name": "Liang Lin"
                },
                {
                    "authorId": "2304746354",
                    "name": "Tat-seng Chua"
                },
                {
                    "authorId": "2237811928",
                    "name": "Angela Yao"
                }
            ]
        },
        {
            "paperId": "43285aa29b0a951fac79cc550749b54a02fdb792",
            "title": "Scene-Text Grounding for Text-Based Video Question Answering",
            "abstract": "Existing efforts in text-based video question answering (TextVideoQA) are criticized for their opaque decisionmaking and heavy reliance on scene-text recognition. In this paper, we propose to study Grounded TextVideoQA by forcing models to answer questions and spatio-temporally localize the relevant scene-text regions, thus decoupling QA from scenetext recognition and promoting research towards interpretable QA. The task has three-fold significance. First, it encourages scene-text evidence versus other short-cuts for answer predictions. Second, it directly accepts scene-text regions as visual answers, thus circumventing the problem of ineffective answer evaluation by stringent string matching. Third, it isolates the challenges inherited in VideoQA and scene-text recognition. This enables the diagnosis of the root causes for failure predictions, e.g., wrong QA or wrong scene-text recognition? To achieve Grounded TextVideoQA, we propose the T2S-QA model that highlights a disentangled temporal-to-spatial contrastive learning strategy for weakly-supervised scene-text grounding and grounded TextVideoQA. To facilitate evaluation, we construct a new dataset ViTXT-GQA which features 52K scene-text bounding boxes within 2.2K temporal segments related to 2K questions and 729 videos. With ViTXT-GQA, we perform extensive experiments and demonstrate the severe limitations of existing techniques in Grounded TextVideoQA. While T2S-QA achieves superior results, the large performance gap with human leaves ample space for improvement. Our further analysis of oracle scene-text inputs posits that the major challenge is scene-text recognition. To advance the research of Grounded TextVideoQA, our dataset and code are at \\url{https://github.com/zhousheng97/ViTXT-GQA.git}",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2238264934",
                    "name": "Sheng Zhou"
                },
                {
                    "authorId": "66358686",
                    "name": "Junbin Xiao"
                },
                {
                    "authorId": "2282481318",
                    "name": "Xun Yang"
                },
                {
                    "authorId": "47382681",
                    "name": "Peipei Song"
                },
                {
                    "authorId": null,
                    "name": "Dan Guo"
                },
                {
                    "authorId": "2237811928",
                    "name": "Angela Yao"
                },
                {
                    "authorId": "2292410331",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2304746354",
                    "name": "Tat-seng Chua"
                }
            ]
        },
        {
            "paperId": "87b4a8f4b417de31765660edcbdbdb9769011d3c",
            "title": "Abductive Ego-View Accident Video Understanding for Safe Driving Perception",
            "abstract": "We present MM-AU, a novel dataset for Multi-Modal Accident video Understanding. MM-AU contains 11,727 in-the-wild ego-view accident videos, each with temporally aligned text descriptions. We annotate over 2.23 mil-lion object boxes and 58,650 pairs of video-based accident reasons, covering 58 accident categories. MM-AU supports various accident understanding tasks, particularly multimodal video diffusion to understand accident cause-effect chains for safe driving. With MM-AU, we present an Abductive accident Video unders tanding framework for Safe Driving perception (AdVersa-SD). AdVersa-SD performs video diffusion via an Object-Centric Video Diffusion (OAVD) method which is driven by an abductive CLIP model. This model involves a contrastive interaction loss to learn the pair co-occurrence of normal, near-accident, accident frames with the corresponding text descriptions, such as accident reasons, prevention advice, and accident categories. OAVD enforces the object region learning while fixing the content of the original frame background in video generation, to find the dominant objects for certain accidents. Extensive experiments verify the abductive ability of AdVersa-SD and the superiority of OAVD against the state-of-the-art diffusion models. Additionally, we provide care-ful benchmark evaluations for object detection and accident reason answering since AdVersa-SD relies on precise object and accident reason information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2199033163",
                    "name": "Jianwu Fang"
                },
                {
                    "authorId": "2289620362",
                    "name": "Lei-lei Li"
                },
                {
                    "authorId": "2289955184",
                    "name": "Junfei Zhou"
                },
                {
                    "authorId": "66358686",
                    "name": "Junbin Xiao"
                },
                {
                    "authorId": "2268703418",
                    "name": "Hongkai Yu"
                },
                {
                    "authorId": "2289612016",
                    "name": "Chen Lv"
                },
                {
                    "authorId": "2189422900",
                    "name": "Jianru Xue"
                },
                {
                    "authorId": "2304746354",
                    "name": "Tat-seng Chua"
                }
            ]
        },
        {
            "paperId": "d64fc87a69bbf53331c74270444c14aedc14dbae",
            "title": "Question-Answering Dense Video Events",
            "abstract": "Multimodal Large Language Models (MLLMs) have shown excellent performance in question-answering of single-event videos. In this paper, we present question-answering dense video events, a novel task that requires answering and grounding the dense-event questions in long videos, thus challenging MLLMs to faithfully comprehend and reason about multiple events occurring over extended time periods. To facilitate the study, we construct DeVE-QA - a dataset featuring 78K questions about 26K events on 10.6K long videos. We then benchmark and show that existing MLLMs excelling at single-event QA struggle to perform well in DeVE-QA. For improvement, we propose DeVi, a novel training-free MLLM approach that highlights a hierarchical captioning module, a temporal event memory module, and a self-consistency checking module to respectively detect, contextualize and memorize, and ground dense-events in long videos for question answering. Extensive experiments show that DeVi is superior at answering dense-event questions and grounding relevant video moments. Compared with existing MLLMs, it achieves a remarkable increase of 4.1 percent and 3.7 percent for G(round)QA accuracy on DeVE-QA and NExT-GQA respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2315295623",
                    "name": "Hangyu Qin"
                },
                {
                    "authorId": "66358686",
                    "name": "Junbin Xiao"
                },
                {
                    "authorId": "2237811928",
                    "name": "Angela Yao"
                }
            ]
        },
        {
            "paperId": "ef6d3fbd97ba38974d31d0a8789a159772c73ed1",
            "title": "Video-Language Understanding: A Survey from Model Architecture, Model Training, and Data Perspectives",
            "abstract": "Humans use multiple senses to comprehend the environment. Vision and language are two of the most vital senses since they allow us to easily communicate our thoughts and perceive the world around us. There has been a lot of interest in creating video-language understanding systems with human-like senses since a video-language pair can mimic both our linguistic medium and visual environment with temporal dynamics. In this survey, we review the key tasks of these systems and highlight the associated challenges. Based on the challenges, we summarize their methods from model architecture, model training, and data perspectives. We also conduct performance comparison among the methods, and discuss promising directions for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261748294",
                    "name": "Thong Nguyen"
                },
                {
                    "authorId": "2305614095",
                    "name": "Yi Bin"
                },
                {
                    "authorId": "66358686",
                    "name": "Junbin Xiao"
                },
                {
                    "authorId": "2305612341",
                    "name": "Leigang Qu"
                },
                {
                    "authorId": "2135358934",
                    "name": "Yicong Li"
                },
                {
                    "authorId": "2305625221",
                    "name": "Jay Zhangjie Wu"
                },
                {
                    "authorId": "2174812220",
                    "name": "Cong-Duy Nguyen"
                },
                {
                    "authorId": "2258761748",
                    "name": "See-Kiong Ng"
                },
                {
                    "authorId": "26336902",
                    "name": "Anh Tuan Luu"
                }
            ]
        },
        {
            "paperId": "186c13602325a4b8c67b158576e0cc043274ff5a",
            "title": "Deconfounded Multimodal Learning for Spatio-temporal Video Grounding",
            "abstract": "The task of spatio-temporal video grounding involves identifying the spatial and temporal regions in a video that correspond to the objects or actions described in a given textual description. However, current models used for spatio-temporal video grounding often rely heavily on spatio-temporal priors to make the predictions. As a result, they may suffer from spurious correlations and lack the ability to generalize well to new or diverse scenarios. To overcome this limitation, we introduce a deconfounded multimodal learning framework, which utilizes a structural causal model to treat dataset biases as a confounder and subsequently remove their confounding effect. Through this framework, we can perform causal intervention on the multimodal input and derive an unbiased estimation formula through the do-calculus technique. In order to tackle the challenge of diverse and often unobservable confounders, we further propose a novel retrieval-based approach with a causal mask mechanism. The proposed method leverages analogical reasoning to facilitate deconfounded learning and mitigate dataset biases, enabling unbiased spatio-temporal prediction without explicitly modeling the confounding factors. Extensive experiments on two challenging benchmarks have well verified the effectiveness and rationality of our proposed solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2130162840",
                    "name": "Jiawei Wang"
                },
                {
                    "authorId": "2261954625",
                    "name": "Zhanchang Ma"
                },
                {
                    "authorId": "2147412314",
                    "name": "Da Cao"
                },
                {
                    "authorId": "2258285591",
                    "name": "Yuquan Le"
                },
                {
                    "authorId": "66358686",
                    "name": "Junbin Xiao"
                },
                {
                    "authorId": "143779329",
                    "name": "Tat-seng Chua"
                }
            ]
        },
        {
            "paperId": "2504035356a92cf2c8ad2beafd361719ac8aa6da",
            "title": "SoarGraph: Numerical Reasoning over Financial Table-Text Data via Semantic-Oriented Hierarchical Graphs",
            "abstract": "Towards the intelligent understanding of table-text data in the finance domain, previous research explores numerical reasoning over table-text content with Question Answering (QA) tasks. A general framework is to extract supporting evidence from the table and text and then perform numerical reasoning over extracted evidence for inferring the answer. However, existing models are vulnerable to missing supporting evidence, which limits their performance. In this work, we propose a novel Semantic-Oriented Hierarchical Graph (SoarGraph) that models the semantic relationships and dependencies among the different elements (e.g., question, table cells, text paragraphs, quantities, and dates) using hierarchical graphs to facilitate supporting evidence extraction and enhance numerical reasoning capability. We conduct our experiments on two popular benchmarks, FinQA and TAT-QA datasets, and the results show that our SoarGraph significantly outperforms all strong baselines, demonstrating remarkable effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31734386",
                    "name": "Fengbin Zhu"
                },
                {
                    "authorId": "2118769749",
                    "name": "Moxin Li"
                },
                {
                    "authorId": "66358686",
                    "name": "Junbin Xiao"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2144448019",
                    "name": "Chao Wang"
                },
                {
                    "authorId": "143779329",
                    "name": "Tat-seng Chua"
                }
            ]
        },
        {
            "paperId": "4a91153f52869aa709f778e5e21814e237543542",
            "title": "Discovering Spatio-Temporal Rationales for Video Question Answering",
            "abstract": "This paper strives to solve complex video question answering (VideoQA) which features long video containing multiple objects and events at different time. To tackle the challenge, we highlight the importance of identifying question-critical temporal moments and spatial objects from the vast amount of video content. Towards this, we propose a Spatio-Temporal Rationalization (STR), a differentiable selection module that adaptively collects question-critical moments and objects using cross-modal interaction. The discovered video moments and objects are then served as grounded rationales to support answer reasoning. Based on STR, we further propose TranSTR, a Transformerstyle neural network architecture that takes STR as the core and additionally underscores a novel answer interaction mechanism to coordinate STR for answer decoding. Experiments on four datasets show that TranSTR achieves new state-of-the-art (SoTA). Especially, on NExT-QA and Causal-VidQA which feature complex VideoQA, it significantly surpasses the previous SoTA by 5.8% and 6.8%, respectively. We then conduct extensive studies to verify the importance of STR as well as the proposed answer interaction mechanism. With the success of TranSTR and our comprehensive analysis, we hope this work can spark more future efforts in complex VideoQA. Code will be released at https://github.com/yl3800/TranSTR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135358934",
                    "name": "Yicong Li"
                },
                {
                    "authorId": "66358686",
                    "name": "Junbin Xiao"
                },
                {
                    "authorId": "2224656169",
                    "name": "Chun Feng"
                },
                {
                    "authorId": "98285513",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "143779329",
                    "name": "Tat-seng Chua"
                }
            ]
        },
        {
            "paperId": "5b43ab59cbd57c3986fa9c9aa6f8c308ee393ef0",
            "title": "Contrastive Video Question Answering via Video Graph Transformer",
            "abstract": "We propose to perform video question answering (VideoQA) in a Contrastive manner via a Video Graph Transformer model (CoVGT). CoVGT's uniqueness and superiority are three-fold: 1) It proposes a dynamic graph transformer module which encodes video by explicitly capturing the visual objects, their relations and dynamics, for complex spatio-temporal reasoning. 2) It designs separate video and text transformers for contrastive learning between the video and text to perform QA, instead of multi-modal transformer for answer classification. Fine-grained video-text communication is done by additional cross-modal interaction modules. 3) It is optimized by the joint fully- and self-supervised contrastive objectives between the correct and incorrect answers, as well as the relevant and irrelevant questions respectively. With superior video encoding and QA solution, we show that CoVGT can achieve much better performances than previous arts on video reasoning tasks. Its performances even surpass those models that are pretrained with millions of external data. We further show that CoVGT can also benefit from cross-modal pretraining, yet with orders of magnitude smaller data. The results demonstrate the effectiveness and superiority of CoVGT, and additionally reveal its potential for more data-efficient pretraining.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66358686",
                    "name": "Junbin Xiao"
                },
                {
                    "authorId": "2153245275",
                    "name": "Pan Zhou"
                },
                {
                    "authorId": "144031869",
                    "name": "Angela Yao"
                },
                {
                    "authorId": "2135358934",
                    "name": "Yicong Li"
                },
                {
                    "authorId": "48043335",
                    "name": "Richang Hong"
                },
                {
                    "authorId": "2111618103",
                    "name": "Shuicheng Yan"
                },
                {
                    "authorId": "143779329",
                    "name": "Tat-seng Chua"
                }
            ]
        }
    ]
}