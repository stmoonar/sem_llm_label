{
    "authorId": "2150349871",
    "papers": [
        {
            "paperId": "72c498b2a388d2c02e7f8eb9416a9d708b28b7c1",
            "title": "MODL: Multilearner Online Deep Learning",
            "abstract": "Online deep learning solves the problem of learning from streams of data, reconciling two opposing objectives: learn fast and learn deep. Existing work focuses almost exclusively on exploring pure deep learning solutions, which are much better suited to handle the\"deep\"than the\"fast\"part of the online learning equation. In our work, we propose a different paradigm, based on a hybrid multilearner approach. First, we develop a fast online logistic regression learner. This learner does not rely on backpropagation. Instead, it uses closed form recursive updates of model parameters, handling the fast learning part of the online learning problem. We then analyze the existing online deep learning theory and show that the widespread ODL approach, currently operating at complexity $O(L^2)$ in terms of the number of layers $L$, can be equivalently implemented in $O(L)$ complexity. This further leads us to the cascaded multilearner design, in which multiple shallow and deep learners are co-trained to solve the online learning problem in a cooperative, synergistic fashion. We show that this approach achieves state-of-the-art results on common online learning datasets, while also being able to handle missing features gracefully. Our code is publicly available at https://github.com/AntonValk/MODL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1419478649",
                    "name": "Antonios Valkanas"
                },
                {
                    "authorId": "3313242",
                    "name": "Boris N. Oreshkin"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        },
        {
            "paperId": "8c659df0ec487a5e4f42d1f46ea93227a47e5fce",
            "title": "Personalized Negative Reservoir for Incremental Learning in Recommender Systems",
            "abstract": "Recommender systems have become an integral part of online platforms. Every day the volume of training data is expanding and the number of user interactions is constantly increasing. The exploration of larger and more expressive models has become a necessary pursuit to improve user experience. However, this progression carries with it an increased computational burden. In commercial settings, once a recommendation system model has been trained and deployed it typically needs to be updated frequently as new client data arrive. Cumulatively, the mounting volume of data is guaranteed to eventually make full batch retraining of the model from scratch computationally infeasible. Naively fine-tuning solely on the new data runs into the well-documented problem of catastrophic forgetting. Despite the fact that negative sampling is a crucial part of training with implicit feedback, no specialized technique exists that is tailored to the incremental learning framework. In this work, we take the first step to propose, a personalized negative reservoir strategy which is used to obtain negative samples for the standard triplet loss. This technique balances alleviation of forgetting with plasticity by encouraging the model to remember stable user preferences and selectively forget when user interests change. We derive the mathematical formulation of a negative sampler to populate and update the reservoir. We integrate our design in three SOTA and commonly used incremental recommendation models. We show that these concrete realizations of our negative reservoir framework achieve state-of-the-art results in standard benchmarks, on multiple standard top-k evaluation metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1419478649",
                    "name": "Antonios Valkanas"
                },
                {
                    "authorId": "2290188725",
                    "name": "Yuening Wang"
                },
                {
                    "authorId": "2242303315",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        },
        {
            "paperId": "dc7ac4cd64a55db1e91ad8cf0ddbc55c9f178b57",
            "title": "CKGConv: General Graph Convolution with Continuous Kernels",
            "abstract": "The existing definitions of graph convolution, either from spatial or spectral perspectives, are inflexible and not unified. Defining a general convolution operator in the graph domain is challenging due to the lack of canonical coordinates, the presence of irregular structures, and the properties of graph symmetries. In this work, we propose a novel and general graph convolution framework by parameterizing the kernels as continuous functions of pseudo-coordinates derived via graph positional encoding. We name this Continuous Kernel Graph Convolution (CKGConv). Theoretically, we demonstrate that CKGConv is flexible and expressive. CKGConv encompasses many existing graph convolutions, and exhibits a stronger expressiveness, as powerful as graph transformers in terms of distinguishing non-isomorphic graphs. Empirically, we show that CKGConv-based Networks outperform existing graph convolutional networks and perform comparably to the best graph transformers across a variety of graph datasets. The code and models are publicly available at https://github.com/networkslab/CKGConv.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1892081076",
                    "name": "Liheng Ma"
                },
                {
                    "authorId": "38939190",
                    "name": "Soumyasundar Pal"
                },
                {
                    "authorId": "2265585667",
                    "name": "Yitian Zhang"
                },
                {
                    "authorId": "2297828274",
                    "name": "Jiaming Zhou"
                },
                {
                    "authorId": "2242303315",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        },
        {
            "paperId": "017bc4b451726ea90d132a20452b99e708d9c510",
            "title": "Multi-resolution Time-Series Transformer for Long-term Forecasting",
            "abstract": "The performance of transformers for time-series forecasting has improved significantly. Recent architectures learn complex temporal patterns by segmenting a time-series into patches and using the patches as tokens. The patch size controls the ability of transformers to learn the temporal patterns at different frequencies: shorter patches are effective for learning localized, high-frequency patterns, whereas mining long-term seasonalities and trends requires longer patches. Inspired by this observation, we propose a novel framework, Multi-resolution Time-Series Transformer (MTST), which consists of a multi-branch architecture for simultaneous modeling of diverse temporal patterns at different resolutions. In contrast to many existing time-series transformers, we employ relative positional encoding, which is better suited for extracting periodic components at different scales. Extensive experiments on several real-world datasets demonstrate the effectiveness of MTST in comparison to state-of-the-art forecasting techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265585667",
                    "name": "Yitian Zhang"
                },
                {
                    "authorId": "1892081076",
                    "name": "Liheng Ma"
                },
                {
                    "authorId": "38939190",
                    "name": "Soumyasundar Pal"
                },
                {
                    "authorId": "2242303315",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        },
        {
            "paperId": "26ebeeb1b9172df34ad21f1000bb6f3c374a222e",
            "title": "Structure Aware Incremental Learning with Personalized Imitation Weights for Recommender Systems",
            "abstract": "Recommender systems now consume large-scale data and play a significant role in improving user experience. Graph Neural Networks (GNNs) have emerged as one of the most effective recommender system models because they model the rich relational information. The ever-growing volume of data can make training GNNs prohibitively expensive. To address this, previous attempts propose to train the GNN models incrementally as new data blocks arrive. \nFeature and structure knowledge distillation techniques have been explored to allow the GNN model to train in a fast incremental fashion while alleviating the catastrophic forgetting problem. \nHowever, preserving the same amount of the historical information for all users is sub-optimal since it fails to take into account the dynamics of each user's change of preferences. \nFor the users whose interests shift substantially, retaining too much of the old knowledge can overly constrain the model, preventing it from quickly adapting to the users\u2019 novel interests. \nIn contrast, for users who have static preferences, model performance can benefit greatly from preserving as much of the user's long-term preferences as possible.\nIn this work, we propose a novel training strategy that adaptively learns personalized imitation weights for each user to balance the contribution from the recent data and the amount of knowledge to be distilled from previous time periods.\nWe demonstrate the effectiveness of learning imitation weights via a comparison on five diverse datasets for three state-of-art structure distillation based recommender systems. The performance shows consistent improvement over competitive incremental learning techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108711613",
                    "name": "Yuening Wang"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "1419478649",
                    "name": "Antonios Valkanas"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "74142381",
                    "name": "Chen Ma"
                },
                {
                    "authorId": "2069718816",
                    "name": "Jianye Hao"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        },
        {
            "paperId": "4d1c260cc34b1df902f2024896ef7c1db400fe20",
            "title": "Motion In-Betweening via Deep <inline-formula><tex-math notation=\"LaTeX\">$\\Delta$</tex-math><alternatives><mml:math><mml:mi>\u0394</mml:mi></mml:math><inline-graphic xlink:href=\"oreshkin-ieq1-3309107.gif\"/></alternatives></inline-formula>-Interpolator",
            "abstract": "We show that the task of synthesizing human motion conditioned on a set of key frames can be solved more accurately and effectively if a deep learning based interpolator operates in the delta mode using the spherical linear interpolator as a baseline. We empirically demonstrate the strength of our approach on publicly available datasets achieving state-of-the-art performance. We further generalize these results by showing that the <inline-formula><tex-math notation=\"LaTeX\">$\\Delta$</tex-math><alternatives><mml:math><mml:mi>\u0394</mml:mi></mml:math><inline-graphic xlink:href=\"oreshkin-ieq3-3309107.gif\"/></alternatives></inline-formula>-regime is viable with respect to the reference of the last known frame (also known as the zero-velocity model). This supports the more general conclusion that operating in the reference frame local to input frames is more accurate and robust than in the global (world) reference frame advocated in previous work.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3313242",
                    "name": "Boris N. Oreshkin"
                },
                {
                    "authorId": "1419478649",
                    "name": "Antonios Valkanas"
                },
                {
                    "authorId": "2806687",
                    "name": "F\u00e9lix G. Harvey"
                },
                {
                    "authorId": "2150349811",
                    "name": "L. M'enard"
                },
                {
                    "authorId": "2342894",
                    "name": "Florent Bocquelet"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        },
        {
            "paperId": "58810ee6ea9f9d40816419a2f0ec3ed3c45a8458",
            "title": "Spectral Augmentations for Graph Contrastive Learning",
            "abstract": "Contrastive learning has emerged as a premier method for learning representations with or without supervision. Recent studies have shown its utility in graph representation learning for pre-training. Despite successes, the understanding of how to design effective graph augmentations that can capture structural properties common to many different types of downstream graphs remains incomplete. We propose a set of well-motivated graph transformation operations derived via graph spectral analysis to provide a bank of candidates when constructing augmentations for a graph contrastive objective, enabling contrastive learning to capture useful structural representation from pre-training graph datasets. We first present a spectral graph cropping augmentation that involves filtering nodes by applying thresholds to the eigenvalues of the leading Laplacian eigenvectors. Our second novel augmentation reorders the graph frequency components in a structural Laplacian-derived position graph embedding. Further, we introduce a method that leads to improved views of local subgraphs by performing alignment via global random walk embeddings. Our experimental results indicate consistent improvements in out-of-domain graph data transfer compared to state-of-the-art graph contrastive learning methods, shedding light on how to design a graph learner that is able to learn structural properties common to diverse graph types.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "81407699",
                    "name": "Amur Ghose"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "2069718816",
                    "name": "Jianye Hao"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        },
        {
            "paperId": "67dc4878a30b4204705966311345bd655479f98a",
            "title": "Substituting Data Annotation with Balanced Updates and Collective Loss in Multi-label Text Classification",
            "abstract": "Multi-label text classification (MLTC) is the task of assigning multiple labels to a given text, and has a wide range of application domains. Most existing approaches require an enormous amount of annotated data to learn a classifier and/or a set of well-defined constraints on the label space structure, such as hierarchical relations which may be complicated to provide as the number of labels increases. In this paper, we study the MLTC problem in annotation-free and scarce-annotation settings in which the magnitude of available supervision signals is linear to the number of labels. Our method follows three steps, (1) mapping input text into a set of preliminary label likelihoods by natural language inference using a pre-trained language model, (2) calculating a signed label dependency graph by label descriptions, and (3) updating the preliminary label likelihoods with message passing along the label dependency graph, driven with a collective loss function that injects the information of expected label frequency and average multi-label cardinality of predictions. The experiments show that the proposed framework achieves effective performance under low supervision settings with almost imperceptible computational and memory overheads added to the usage of pre-trained language model outperforming its initial performance by 70\\% in terms of example-based F1 score.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067361514",
                    "name": "Muberra Ozmen"
                },
                {
                    "authorId": "2245395221",
                    "name": "Joseph Cotnareanu"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        },
        {
            "paperId": "816e1dba651c5e37627d825cf79e1edc69c17cd9",
            "title": "Population Monte Carlo With Normalizing Flow",
            "abstract": "Adaptive importance sampling (AIS) methods provide a useful alternative to Markov Chain Monte Carlo (MCMC) algorithms for performing inference of intractable distributions. Population Monte Carlo (PMC) algorithms constitute a family of AIS approaches which adapt the proposal distributions iteratively to improve the approximation of the target distribution. Recent work in this area primarily focuses on ameliorating the proposal adaptation procedure for high-dimensional applications. However, most of the AIS algorithms use simple proposal distributions for sampling, which might be inadequate in exploring target distributions with intricate geometries. In this work, we construct expressive proposal distributions in the AIS framework using normalizing flow, an appealing approach for modeling complex distributions. We use an iterative parameter update rule to enhance the approximation of the target distribution. Numerical experiments show that in high-dimensional settings, the proposed algorithm offers significantly improved performance compared to the existing techniques.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38939190",
                    "name": "Soumyasundar Pal"
                },
                {
                    "authorId": "1419478649",
                    "name": "Antonios Valkanas"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        },
        {
            "paperId": "8ae47f3e238304f33c8cfe5f24c054339dbc4080",
            "title": "Diffusing Gaussian Mixtures for Generating Categorical Data",
            "abstract": "Learning a categorical distribution comes with its own set of challenges. A successful approach taken by state-of-the-art works is to cast the problem in a continuous domain to take advantage of the impressive performance of the generative models for continuous data. Amongst them are the recently emerging diffusion probabilistic models, which have the observed advantage of generating high-quality samples. Recent advances for categorical generative models have focused on log likelihood improvements. In this work, we propose a generative model for categorical data based on diffusion models with a focus on high-quality sample generation, and propose sampled-based evaluation methods. The efficacy of our method stems from performing diffusion in the continuous domain while having its parameterization informed by the structure of the categorical nature of the target distribution. Our method of evaluation highlights the capabilities and limitations of different generative models for generating categorical data, and includes experiments on synthetic and real-world protein datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388386548",
                    "name": "Florence Regol"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        }
    ]
}