{
    "authorId": "2042750482",
    "papers": [
        {
            "paperId": "6a5ab4a74a9f567eda276a6e5a276aa89e61e690",
            "title": "Safe Reinforcement Learning via Observation Shielding",
            "abstract": "Reinforcement Learning (RL) algorithms have shown success in scaling up to large problems. However, deploying those algorithms in real-world applications remains challenging due to their vulnerability to adversarial perturbations. Existing RL robustness methods against adversarial attacks are weak to large perturbations - a scenario that cannot be ruled out for RL adversarial threats, as is the case for deep neural networks in classification tasks. This paper proposes a method called observation-shielding RL (OSRL) to increase the robustness of RL against large perturbations using predictive models and threat detection. Instead of changing the RL algorithms with robustness regularization or retrain them with adversarial perturbations, we depart considerably from previous approaches and develop an add-on safety feature for existing RL algorithms during runtime. OSRL builds on the idea of model predictive shielding, where an observation predictive model is used to override the perturbed observations as needed to ensure safety. Extensive experiments on various MuJoco 1 environments (Ant, Hoope, InvertedPendulum, Reacher) environment demonstrate that our proposed OSRL is safer and more efficient than state-of-the-art robustness methods under large perturbations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2042750482",
                    "name": "Joe McCalmon"
                },
                {
                    "authorId": "2111150708",
                    "name": "Tongtong Liu"
                },
                {
                    "authorId": "2207250593",
                    "name": "Reid Goldsmith"
                },
                {
                    "authorId": "2207266743",
                    "name": "Andrew Cyhaniuk"
                },
                {
                    "authorId": "9628020",
                    "name": "Talal Halabi"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                }
            ]
        },
        {
            "paperId": "3892a9bee7c9f0314a513c0933a588e1e069722b",
            "title": "CAPS: Comprehensible Abstract Policy Summaries for Explaining Reinforcement Learning Agents",
            "abstract": "As reinforcement learning (RL) continues to improve and be applied in situations alongside humans, the need to explain the learned behaviors of RL agents to end-users becomes more important. Strategies for explaining the reasoning behind an agent\u2019s policy, called policy-level explanations , can lead to important insights about both the task and the agent\u2019s behaviors. Following this line of research, in this work, we propose a novel approach, named as CAPS , that summarizes an agent\u2019s policy in the form of a directed graph with natural language descriptions. A decision tree based clustering method is utilized to abstract the state space of the task into fewer, condensed states which makes the policy graphs more digestible to end-users. This abstraction allows the users to control the size of the policy graph to achieve their desired balance between comprehensi-bility and accuracy. In addition, we develop a heuristic optimization method to find the most explainable graph policy and present it to the users. Finally, we use the user-defined predicates to enrich the abstract states with semantic meaning. We test our approach on 5 RL tasks, using both deterministic and stochastic policies, and show that our method is: (1) agnostic to the algorithms used to train the policies, and (2) comparable in accuracy and superior in explanation capabilities to existing baselines. Especially, when provided with our explanation graph, end-users are able to accurately interpret policies of trained RL agents 80% of the time, compared to 10% when provided with the next best baseline. We make our code and datasets available to ensure the reproducibility of our research",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2042750482",
                    "name": "Joe McCalmon"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                },
                {
                    "authorId": "2150817685",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "e196e0c89cc6a2824750160a0264b7a5c6866574",
            "title": "LSTM-Based Anomalous Behavior Detection in Multi-Agent Reinforcement Learning",
            "abstract": "Multi-Agent Reinforcement Learning (MARL) extends individual reinforcement learning to enable a team of agents to collaboratively determine the global optimal policy that maximizes the sum of their local accumulated rewards. It has been recently deployed in multiple application domains such as edge computing, wireless networks, and Cyber-Physical Systems. Nonetheless, the security of MARL and its potential exposure to cyberattacks have not yet been fully investigated. This paper examines one of the most serious vulnerabilities in MARL algorithms: the compromised agent. This newly-engineered adversarial vulnerability is exploited when a malicious user compromises an agent to directly control its actions, and subsequently pushes its cooperative agents to act off-policy. We present a novel stacked-LSTM ensemble approach to detect such an attack. The results show that our anomalous behavior detection system significantly outperforms five baselines from the literature.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2181836823",
                    "name": "Cameron Lischke"
                },
                {
                    "authorId": "2111150708",
                    "name": "Tongtong Liu"
                },
                {
                    "authorId": "2042750482",
                    "name": "Joe McCalmon"
                },
                {
                    "authorId": "10707785",
                    "name": "Md Asifur Rahman"
                },
                {
                    "authorId": "9628020",
                    "name": "Talal Halabi"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                }
            ]
        },
        {
            "paperId": "5d5bfcd4066d0b644271bef7e78ec887753b703e",
            "title": "A Symbolic-AI Approach for UAV Exploration Tasks",
            "abstract": "Performing autonomous exploration and exploitation is essential for un- manned aerial vehicles (UAVs) operating in unknown environments. Often, such missions involve first building a map of the environment via pure exploration and subsequently exploiting it for specific downstream tasks. But, conducting separate exploration and exploitation steps is not always feasible in practice. In this paper, we develop a novel exploration approach enabling exploration and exploitation in a single step for an area-of-interest (AoI) search task. The basic idea is to employ a probabilistic information gain map, called a belief map, as a prior to guide the exploration trajectory, while efficiently reducing false positive information in the process. The approach is composed of three layers. The first is an information potential layer to decide the exploration direction for the UAV. Next, the proximity layer exploits detected AoI by exploring their proximal areas. The last layer, a forced movement layer, is responsible for enabling the UAV to escape local maxima caused by the previous layers. We tested the performance of our approach in two different tasks relative to two exploration methods published in the literature. The results demonstrate that our proposed approach is capable of navigating through randomly generated environments and covering more AoI in fewer time steps compared to the baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108247709",
                    "name": "Yixin Zhang"
                },
                {
                    "authorId": "2042750482",
                    "name": "Joe McCalmon"
                },
                {
                    "authorId": "2042699781",
                    "name": "Ashley Peake"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                },
                {
                    "authorId": "2873829",
                    "name": "V. P. Pauca"
                }
            ]
        },
        {
            "paperId": "683039b2598c89eb09edbfb27dc272c859342391",
            "title": "Deep Reinforcement Learning for Adaptive Exploration of Unknown Environments",
            "abstract": "Autonomous exploration is an essential task for unmanned aerial vehicles (UAVs) operating in unknown environments. Often, UAVs on these missions must first build a map of the environment via pure exploration and subsequently use (i.e. exploit) the generated map for downstream navigation tasks. Accomplishing these navigation tasks in two separate steps is not always possible and can potentially be disadvantageous for UAVs deployed in outdoor and dynamically changing environments. Current exploration approaches typically use a priori human-generated maps or heuristics such as frontier-based exploration. Other approaches use learning but focus only on learning policies for specific tasks and use sample inefficient random exploration or make impractical assumptions about full map availability. In this paper, we develop an adaptive exploration approach that allows for a trade off between exploration and exploitation in a single step using Deep Reinforcement Learning (DRL). We specifically focus on UAVs searching for areas of interest (AoIs) in an unknown environment. The proposed approach uses a map segmentation technique to decompose the environment map into smaller, tractable maps. DDQN and A2C algorithms are extended with a stack of LSTM layers and trained to generate optimal policies for the exploration and exploitation tasks, respectively. Then, an information gain function is repeatedly computed to determine the optimal trade-off between them. We tested our approach in 3 different tasks against 4 baselines. The results demonstrate that our proposed approach is capable of navigating through randomly generated environments and covering more AoI in less time compared to the baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2042699781",
                    "name": "Ashley Peake"
                },
                {
                    "authorId": "2042750482",
                    "name": "Joe McCalmon"
                },
                {
                    "authorId": "2108247709",
                    "name": "Yixin Zhang"
                },
                {
                    "authorId": "2087983259",
                    "name": "Daniel Myers"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                },
                {
                    "authorId": "2873829",
                    "name": "V. P. Pauca"
                }
            ]
        },
        {
            "paperId": "6b84d3b87179a7299923f8bb20a806ade2574796",
            "title": "Exploration of Unknown Environments Using Deep Reinforcement Learning",
            "abstract": "My research presents a method for efficient exploration of an outdoor, unknown area, which aims to achieve precise coverage of regions of interest within that area. While this method for autonomous exploration was designed for autonomous controllers in unmanned aerial vehicles (UAVs), the concepts apply to any vehicle which uses autonomous navigation. We consider an environment with areas of interest of various sizes littered throughout, and a reinforcement learning agent which is tasked with discovering and mapping these areas in an efficient manner.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2042750482",
                    "name": "Joe McCalmon"
                }
            ]
        },
        {
            "paperId": "6b4b4b16c1a5f6a07351db229fa7dba14ee00e64",
            "title": "Multi-Agent Reinforcement Learning for Cooperative Adaptive Cruise Control",
            "abstract": "A growing trend in the field of autonomous vehicles is the use of platooning. The design of control algorithms for platoons is challenging considering that coordination among vehicles is obtained through diverse communication channels. Currently, Adaptive Cruise Control (ACC) is used in individual vehicles to regulate certain driving functions. ACC can be extended to leverage inter-vehicle communication, creating a tightly coupled vehicle stream in the form of a platoon. This extension, Cooperative Adaptive Cruise Control (CACC), typically assumes full communication among vehicles. In this paper, we develop a deep reinforcement learning based CACC that allows platooning vehicles to learn a robust communication protocol alongside their coordination policies. LSTM is used to implement ACC for each vehicle and is trained using policy gradient. To coordinate driving, the vehicle's LSTM adapts to exchange relevant information with the other vehicles, creating the CACC. We simulate two platoons of 3 and 5 vehicles, respectively. We test our CACC with the learned communication protocol against full and inhibited communication baselines with and without a jamming attack. We also train our approach with local and global reward systems. Results suggest that models with individual rewards and the learned communication protocol achieve higher performance and faster convergence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2042699781",
                    "name": "Ashley Peake"
                },
                {
                    "authorId": "2042750482",
                    "name": "Joe McCalmon"
                },
                {
                    "authorId": "2042700303",
                    "name": "Benjamin Raiford"
                },
                {
                    "authorId": "2111150708",
                    "name": "Tongtong Liu"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                }
            ]
        },
        {
            "paperId": "f43462c5e8e84f87067f29aa552406650095c01f",
            "title": "Wilderness Search and Rescue Missions using Deep Reinforcement Learning",
            "abstract": "Wilderness Search and Rescue (WiSAR) requires navigating large regions - often in rugged, remote areas - searching for missing people or animals. Because of the large regions and potentially limited mobility of ground vehicles, WiSAR missions are frequently carried out with the help of Unmanned Aerial Vehicles (UAVs). However, the ability to autonomously execute WiSAR remains an unsolved challenge. In this paper, we take advantage of Deep Reinforcement Learning (DRL) to develop an autonomous WiSAR controller for UAVs. We improve the learning and understanding of a UAV agent to explore a partially observable environment in search of a victim trapped in the wild. The proposed approach breaks up this difficult problem into 4 sub-tasks: tractable mapping of the environment in small regions, region selection, target search, and region exploration. Quad-Tree is utilized offline to decompose the environment map into smaller, tractable maps. Then, an efficient cost function is repeatedly computed to determine the best target region to search in each iteration of the process. Recurrent-DDQN and A2C algorithms are trained to generate optimal policies for the target search and regions exploration tasks, respectively. We tested our approach against a baseline of a hard-coded policy of navigating the map in a zigzag fashion and another baseline of using the same sub-tasks but instead of using the DRL algorithms, randomly selecting an action at each time step. The results demonstrate that our proposed approach is capable of navigating through 25 randomly generated environments and finding the missing victim faster than the baselines by 46%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2042699781",
                    "name": "Ashley Peake"
                },
                {
                    "authorId": "2042750482",
                    "name": "Joe McCalmon"
                },
                {
                    "authorId": "2108247709",
                    "name": "Yixin Zhang"
                },
                {
                    "authorId": "2042700303",
                    "name": "Benjamin Raiford"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                }
            ]
        }
    ]
}