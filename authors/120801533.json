{
    "authorId": "120801533",
    "papers": [
        {
            "paperId": "5316598d39df1bf47240eb4a8dc0f4770ec3fa72",
            "title": "OpenHEXAI: An Open-Source Framework for Human-Centered Evaluation of Explainable Machine Learning",
            "abstract": "Recently, there has been a surge of explainable AI (XAI) methods driven by the need for understanding machine learning model behaviors in high-stakes scenarios. However, properly evaluating the effectiveness of the XAI methods inevitably requires the involvement of human subjects, and conducting human-centered benchmarks is challenging in a number of ways: designing and implementing user studies is complex; numerous design choices in the design space of user study lead to problems of reproducibility; and running user studies can be challenging and even daunting for machine learning researchers. To address these challenges, this paper presents OpenHEXAI, an open-source framework for human-centered evaluation of XAI methods. OpenHEXAI features (1) a collection of diverse benchmark datasets, pre-trained models, and post hoc explanation methods; (2) an easy-to-use web application for user study; (3) comprehensive evaluation metrics for the effectiveness of post hoc explanation methods in the context of human-AI decision making tasks; (4) best practice recommendations of experiment documentation; and (5) convenient tools for power analysis and cost estimation. OpenHEAXI is the first large-scale infrastructural effort to facilitate human-centered benchmarks of XAI methods. It simplifies the design and implementation of user studies for XAI methods, thus allowing researchers and practitioners to focus on the scientific questions. Additionally, it enhances reproducibility through standardized designs. Based on OpenHEXAI, we further conduct a systematic benchmark of four state-of-the-art post hoc explanation methods and compare their impacts on human-AI decision making tasks in terms of accuracy, fairness, as well as users' trust and understanding of the machine learning model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261475382",
                    "name": "Jiaqi Ma"
                },
                {
                    "authorId": "120801533",
                    "name": "Vivian Lai"
                },
                {
                    "authorId": "49889218",
                    "name": "Yiming Zhang"
                },
                {
                    "authorId": "2131059444",
                    "name": "Chacha Chen"
                },
                {
                    "authorId": "2290718096",
                    "name": "Paul Hamilton"
                },
                {
                    "authorId": "1725415927",
                    "name": "Davor Ljubenkov"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                },
                {
                    "authorId": "2269165090",
                    "name": "Chenhao Tan"
                }
            ]
        },
        {
            "paperId": "19d276ff6389bedd3d369673adcfa9a169cff934",
            "title": "Adversarial Collaborative Filtering for Free",
            "abstract": "Collaborative Filtering (CF) has been successfully used to help users discover the items of interest. Nevertheless, existing CF methods suffer from noisy data issue, which negatively impacts the quality of recommendation. To tackle this problem, many prior studies leverage adversarial learning to regularize the representations of users/items, which improves both generalizability and robustness. Those methods often learn adversarial perturbations and model parameters under min-max optimization framework. However, there still have two major drawbacks: 1) Existing methods lack theoretical guarantees of why adding perturbations improve the model generalizability and robustness; 2) Solving min-max optimization is time-consuming. In addition to updating the model parameters, each iteration requires additional computations to update the perturbations, making them not scalable for industry-scale datasets. In this paper, we present Sharpness-aware Collaborative Filtering (SharpCF), a simple yet effective method that conducts adversarial training without extra computational cost over the base optimizer. To achieve this goal, we first revisit the existing adversarial collaborative filtering and discuss its connection with recent Sharpness-aware Minimization. This analysis shows that adversarial training actually seeks model parameters that lie in neighborhoods around the optimal model parameters having uniformly low loss values, resulting in better generalizability. To reduce the computational overhead, SharpCF introduces a novel trajectory loss to measure the alignment between current weights and past weights. Experimental results on real-world datasets demonstrate that our SharpCF achieves superior performance with almost zero additional computational cost comparing to adversarial training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1504511015",
                    "name": "Huiyuan Chen"
                },
                {
                    "authorId": "2185014510",
                    "name": "Xiaoting Li"
                },
                {
                    "authorId": "120801533",
                    "name": "Vivian Lai"
                },
                {
                    "authorId": "3056465",
                    "name": "Chin-Chia Michael Yeh"
                },
                {
                    "authorId": "1944223736",
                    "name": "Yujie Fan"
                },
                {
                    "authorId": "2185013996",
                    "name": "Yan Zheng"
                },
                {
                    "authorId": "40308435",
                    "name": "Mahashweta Das"
                },
                {
                    "authorId": "2257352920",
                    "name": "Hao Yang"
                }
            ]
        },
        {
            "paperId": "a30d5f2f10cef8af4efd4f929dfe2ce90c8b3010",
            "title": "Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory",
            "abstract": "We address a fundamental challenge in Natural Language Generation (NLG) model evaluation -- the design and evaluation of evaluation metrics. Recognizing the limitations of existing automatic metrics and noises from how current human evaluation was conducted, we propose MetricEval, a framework informed by measurement theory, the foundation of educational test design, for conceptualizing and evaluating the reliability and validity of NLG evaluation metrics. The framework formalizes the source of measurement error and offers statistical tools for evaluating evaluation metrics based on empirical data. With our framework, one can quantify the uncertainty of the metrics to better interpret the result. To exemplify the use of our framework in practice, we analyzed a set of evaluation metrics for summarization and identified issues related to conflated validity structure in human-eval and reliability in LLM-based metrics. Through MetricEval, we aim to promote the design, evaluation, and interpretation of valid and reliable metrics to advance robust and effective NLG models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9605732",
                    "name": "Ziang Xiao"
                },
                {
                    "authorId": "116270523",
                    "name": "Susu Zhang"
                },
                {
                    "authorId": "120801533",
                    "name": "Vivian Lai"
                },
                {
                    "authorId": "144921048",
                    "name": "Q. Liao"
                }
            ]
        },
        {
            "paperId": "bc0ce44cdd7799fab76404c40f02a73c9a2056a3",
            "title": "Enhancing Transformers without Self-supervised Learning: A Loss Landscape Perspective in Sequential Recommendation",
            "abstract": "Transformer and its variants are a powerful class of architectures for sequential recommendation, owing to their ability of capturing a user\u2019s dynamic interests from their past interactions. Despite their success, Transformer-based models often require the optimization of a large number of parameters, making them difficult to train from sparse data in sequential recommendation. To address the problem of data sparsity, previous studies have utilized self-supervised learning to enhance Transformers, such as pre-training embeddings from item attributes or contrastive data augmentations. However, these approaches encounter several training issues, including initialization sensitivity, manual data augmentations, and large batch-size memory bottlenecks. In this work, we investigate Transformers from the perspective of loss geometry, aiming to enhance the models\u2019 data efficiency and generalization in sequential recommendation. We observe that Transformers (e.g., SASRec) can converge to extremely sharp local minima if not adequately regularized. Inspired by the recent Sharpness-Aware Minimization (SAM), we propose SAMRec, which significantly improves the accuracy and robustness of sequential recommendation. SAMRec performs comparably to state-of-the-art self-supervised Transformers, such as S3Rec and CL4SRec, without the need for pre-training or strong data augmentations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120801533",
                    "name": "Vivian Lai"
                },
                {
                    "authorId": "1504511015",
                    "name": "Huiyuan Chen"
                },
                {
                    "authorId": "3056465",
                    "name": "Chin-Chia Michael Yeh"
                },
                {
                    "authorId": "2153556921",
                    "name": "Minghua Xu"
                },
                {
                    "authorId": "2232929519",
                    "name": "Yiwei Cai"
                },
                {
                    "authorId": "2145058012",
                    "name": "Hao Yang"
                }
            ]
        },
        {
            "paperId": "e2de545750c859b9f7f62919e2b94f9e92ad9147",
            "title": "Selective Explanations: Leveraging Human Input to Align Explainable AI",
            "abstract": "While a vast collection of explainable AI (XAI) algorithms has been developed in recent years, they have been criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective ---a fundamental property of human explanations---by selectively presenting a subset of model reasoning based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small dataset. This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three paradigms based on our proposed framework: in Study 1, we ask the participants to provide critique-based or open-ended input to generate selective explanations (self-input). In Study 2, we show the participants selective explanations based on input from a panel of similar users (annotator input). Our experiments demonstrate the promise of selective explanations in reducing over-reliance on AI and improving collaborative decision making and subjective perceptions of the AI system, but also paint a nuanced picture that attributes some of these positive effects to the opportunity to provide one's own input to augment AI explanations. Overall, our work proposes a novel XAI framework inspired by human communication behaviors and demonstrates its potential to encourage future work to make AI explanations more human-compatible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120801533",
                    "name": "Vivian Lai"
                },
                {
                    "authorId": "49889218",
                    "name": "Yiming Zhang"
                },
                {
                    "authorId": "2131059444",
                    "name": "Chacha Chen"
                },
                {
                    "authorId": "144921048",
                    "name": "Q. Liao"
                },
                {
                    "authorId": "2111727675",
                    "name": "Chenhao Tan"
                }
            ]
        },
        {
            "paperId": "f46050b4ae991a0c047f9930c8de7c706e61c197",
            "title": "Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies",
            "abstract": "AI systems are adopted in numerous domains due to their increasingly strong predictive performance. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time-consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our work highlights the need to develop common frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other\u2019s work and produce generalizable scientific knowledge. We also hope this work will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120801533",
                    "name": "Vivian Lai"
                },
                {
                    "authorId": "2131059444",
                    "name": "Chacha Chen"
                },
                {
                    "authorId": "1405364873",
                    "name": "Alison Smith-Renner"
                },
                {
                    "authorId": "144921048",
                    "name": "Q. Liao"
                },
                {
                    "authorId": "2111727675",
                    "name": "Chenhao Tan"
                }
            ]
        },
        {
            "paperId": "29c29d19a1d6fc62b19251547ce9df351e49763e",
            "title": "Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation",
            "abstract": "Despite impressive performance in many benchmark datasets, AI models can still make mistakes, especially among out-of-distribution examples. It remains an open question how such imperfect models can be used effectively in collaboration with humans. Prior work has focused on AI assistance that helps people make individual high-stakes decisions, which is not scalable for a large amount of relatively low-stakes decisions, e.g., moderating social media comments. Instead, we propose conditional delegation as an alternative paradigm for human-AI collaboration where humans create rules to indicate trustworthy regions of a model. Using content moderation as a testbed, we develop novel interfaces to assist humans in creating conditional delegation rules and conduct a randomized experiment with two datasets to simulate in-distribution and out-of-distribution scenarios. Our study demonstrates the promise of conditional delegation in improving model performance and provides insights into design for this novel paradigm, including the effect of AI explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120801533",
                    "name": "Vivian Lai"
                },
                {
                    "authorId": "40502796",
                    "name": "Samuel Carton"
                },
                {
                    "authorId": "2163452640",
                    "name": "Rajat Bhatnagar"
                },
                {
                    "authorId": "2268772828",
                    "name": "Vera Liao"
                },
                {
                    "authorId": "2108127520",
                    "name": "Yunfeng Zhang"
                },
                {
                    "authorId": "2269165090",
                    "name": "Chenhao Tan"
                },
                {
                    "authorId": "144921048",
                    "name": "Q. Liao"
                }
            ]
        },
        {
            "paperId": "9b6f3694d30701f88d9ed017375cfdba7cb3a1ea",
            "title": "Towards Robust and Semantically Organised Latent Representations for Unsupervised Text Style Transfer",
            "abstract": "Recent studies show that auto-encoder based approaches successfully perform language generation, smooth sentence interpolation, and style transfer over unseen attributes using unlabelled datasets in a zero-shot manner. The latent space geometry of such models is organised well enough to perform on datasets where the style is \u201ccoarse-grained\u201d i.e. a small fraction of words alone in a sentence are enough to determine the overall style label. A recent study uses a discrete token-based perturbation approach to map \u201csimilar\u201d sentences (\u201csimilar\u201d defined by low Levenshtein distance/ high word overlap) close by in latent space. This definition of \u201csimilarity\u201d does not look into the underlying nuances of the constituent words while mapping latent space neighbourhoods and therefore fails to recognise sentences with different style-based semantics while mapping latent neighbourhoods. We introduce EPAAEs (Embedding Perturbed Adversarial AutoEncoders) which completes this perturbation model, by adding a finely adjustable noise component on the continuous embeddings space. We empirically show that this (a) produces a better organised latent space that clusters stylistically similar sentences together, (b) performs best on a diverse set of text style transfer tasks than its counterparts, and (c) is capable of fine-grained control of Style Transfer strength. We also extend the text style transfer tasks to NLI datasets and show that these more complex definitions of style are learned best by EPAAE. To the best of our knowledge, extending style transfer to NLI tasks has not been explored before.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120801533",
                    "name": "Vivian Lai"
                },
                {
                    "authorId": "1405364873",
                    "name": "Alison Smith-Renner"
                },
                {
                    "authorId": "2119057915",
                    "name": "Ke Zhang"
                },
                {
                    "authorId": "117397243",
                    "name": "Ruijia Cheng"
                },
                {
                    "authorId": "2170222829",
                    "name": "Wenjuan Zhang"
                },
                {
                    "authorId": "2086973507",
                    "name": "Joel R. Tetreault"
                },
                {
                    "authorId": "144633617",
                    "name": "A. Jaimes"
                }
            ]
        },
        {
            "paperId": "449ff2404a7b11fa3c47cd228fa5469b00ffef20",
            "title": "Towards a Science of Human-AI Decision Making: A Survey of Empirical Studies",
            "abstract": "As AI systems demonstrate increasingly strong predictive performance, their adoption has grown in numerous domains. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI models and AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our survey highlights the need to develop common frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other's work and produce generalizable scientific knowledge. We also hope this survey will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120801533",
                    "name": "Vivian Lai"
                },
                {
                    "authorId": "2131059444",
                    "name": "Chacha Chen"
                },
                {
                    "authorId": "144921048",
                    "name": "Q. Liao"
                },
                {
                    "authorId": "1405364873",
                    "name": "Alison Smith-Renner"
                },
                {
                    "authorId": "2111727675",
                    "name": "Chenhao Tan"
                }
            ]
        },
        {
            "paperId": "74bcea217e5bd1d04ee0315abaf7e29a25eb418b",
            "title": "Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making",
            "abstract": "Although AI holds promise for improving human decision making in societally critical domains, it remains an open question how human-AI teams can reliably outperform AI alone and human alone in challenging prediction tasks (also known as complementary performance). We explore two directions to understand the gaps in achieving complementary performance. First, we argue that the typical experimental setup limits the potential of human-AI teams. To account for lower AI performance out-of-distribution than in-distribution because of distribution shift, we design experiments with different distribution types and investigate human performance for both in-distribution and out-of-distribution examples. Second, we develop novel interfaces to support interactive explanations so that humans can actively engage with AI assistance. Using virtual pilot studies and large-scale randomized experiments across three tasks, we demonstrate a clear difference between in-distribution and out-of-distribution, and observe mixed results for interactive explanations: while interactive explanations improve human perception of AI assistance's usefulness, they may reinforce human biases and lead to limited performance improvement. Overall, our work points out critical challenges and future directions towards enhancing human performance with AI assistance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2140162948",
                    "name": "Han Liu"
                },
                {
                    "authorId": "120801533",
                    "name": "Vivian Lai"
                },
                {
                    "authorId": "40348583",
                    "name": "Chenhao Tan"
                }
            ]
        }
    ]
}