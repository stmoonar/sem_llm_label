{
    "authorId": "2149231521",
    "papers": [
        {
            "paperId": "f525bcdb3870ea15528dd450b59db64586509205",
            "title": "Can We Learn Question, Answer, and Distractors All from an Image? A New Task for Multiple-choice Visual Question Answering",
            "abstract": "Multiple-choice visual question answering (MC VQA) requires an answer picked from a list of distractors, based on a question and an image. This research has attracted wide interest from the fields of visual question answering, visual question generation, and visual distractor generation. However, these fields still stay in their own territories, and how to jointly generate meaningful questions, correct answers, and challenging distractors remains unexplored. In this paper, we introduce a novel task, Visual Question-Answer-Distractors Generation (VQADG), which can bridge this research gap as well as take as a cornerstone to promote existing VQA models. Specific to the VQADG task, we present a novel framework consisting of a vision-and-language model to encode the given image and generate QADs jointly, and contrastive learning to ensure the consistency of the generated question, answer, and distractors. Empirical evaluations on the benchmark dataset validate the performance of our model in the VQADG task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119365890",
                    "name": "Wenjian Ding"
                },
                {
                    "authorId": "2301908675",
                    "name": "Yao Zhang"
                },
                {
                    "authorId": "2257572522",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "2261673463",
                    "name": "Adam Jatowt"
                },
                {
                    "authorId": "2149231521",
                    "name": "Zhenglu Yang"
                }
            ]
        },
        {
            "paperId": "35ffe006979cde43fdfba519b73d9dc1642c4469",
            "title": "Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue",
            "abstract": "Accurate knowledge selection is critical in knowledge-grounded dialogue systems. Towards a closer look at it, we offer a novel perspective to organize existing literature, i.e., knowledge selection coupled with, after, and before generation. We focus on the third under-explored category of study, which can not only select knowledge accurately in advance, but has the advantage to reduce the learning, adjustment, and interpretation burden of subsequent response generation models, especially LLMs. We propose GATE, a generator-agnostic knowledge selection method, to prepare knowledge for subsequent response generation models by selecting context-related knowledge among different knowledge structures and variable knowledge requirements. Experimental results demonstrate the superiority of GATE, and indicate that knowledge selection before generation is a lightweight yet effective way to facilitate LLMs (e.g., ChatGPT) to generate more informative responses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257353760",
                    "name": "Lang Qin"
                },
                {
                    "authorId": "1611287648",
                    "name": "Yao Zhang"
                },
                {
                    "authorId": "46822058",
                    "name": "Hongru Liang"
                },
                {
                    "authorId": "2257572522",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "2149231521",
                    "name": "Zhenglu Yang"
                }
            ]
        },
        {
            "paperId": "3e4cfcce2a001104032f9e66d9cb06d72bb044d2",
            "title": "ACROSS: An Alignment-based Framework for Low-Resource Many-to-One Cross-Lingual Summarization",
            "abstract": ".",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2158012487",
                    "name": "Peiyao Li"
                },
                {
                    "authorId": "2148904527",
                    "name": "Zhengkun Zhang"
                },
                {
                    "authorId": "2152809962",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "2154884181",
                    "name": "Liang Li"
                },
                {
                    "authorId": "1774986",
                    "name": "A. Jatowt"
                },
                {
                    "authorId": "2149231521",
                    "name": "Zhenglu Yang"
                }
            ]
        },
        {
            "paperId": "1a5a9d7fdfed0e5d3f5c1b00d6356361b8c57941",
            "title": "HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks",
            "abstract": "The workflow of pretraining and fine-tuning has emerged as a popular paradigm for solving various NLP and V&L (Vision-and-Language) downstream tasks. With the capacity of pretrained models growing rapidly, how to perform parameter-efficient fine-tuning has become fairly important for quick transfer learning and deployment. In this paper, we design a novel unified parameter-efficient transfer learning framework that works effectively on both pure language and V&L tasks. In particular, we use a shared hypernetwork that takes trainable hyper-embeddings as input, and outputs weights for fine-tuning different small modules in a pretrained language model, such as tuning the parameters inserted into multi-head attention blocks (i.e., prefix-tuning) and feed-forward blocks (i.e., adapter-tuning). We define a set of embeddings (e.g., layer, block, task and visual embeddings) as the key components to calculate hyper-embeddings, which thus can support both pure language and V&L tasks. Our proposed framework adds fewer trainable parameters in multi-task learning while achieving superior performances and transfer ability compared to state-of-the-art methods. Empirical results on the GLUE benchmark and multiple V&L tasks confirm the effectiveness of our framework on both textual and visual modalities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2148904527",
                    "name": "Zhengkun Zhang"
                },
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "1840054391",
                    "name": "Xiaojun Meng"
                },
                {
                    "authorId": "2136912252",
                    "name": "Yasheng Wang"
                },
                {
                    "authorId": "2125066358",
                    "name": "Yadao Wang"
                },
                {
                    "authorId": "2110310493",
                    "name": "Xin Jiang"
                },
                {
                    "authorId": "30738758",
                    "name": "Qun Liu"
                },
                {
                    "authorId": "2149231521",
                    "name": "Zhenglu Yang"
                }
            ]
        },
        {
            "paperId": "20fde81e7ef98511c000237459a0751cd9e832ba",
            "title": "Interacting with Non-Cooperative User: A New Paradigm for Proactive Dialogue Policy",
            "abstract": "Proactive dialogue system is able to lead the conversation to a goal topic and has advantaged potential in bargain, persuasion, and negotiation. Current corpus-based learning manner limits its practical application in real-world scenarios. To this end, we contribute to advancing the study of the proactive dialogue policy to a more natural and challenging setting, i.e., interacting dynamically with users. Further, we call attention to the non-cooperative user behavior - the user talks about off-path topics when he/she is not satisfied with the previous topics introduced by the agent. We argue that the targets of reaching the goal topic quickly and maintaining a high user satisfaction are not always converged, because the topics close to the goal and the topics user preferred may not be the same. Towards this issue, we propose a new solution named I-Pro that can learn Proactive policy in the Interactive setting. Specifically, we learn the trade-off via a learned goal weight, which consists of four factors (dialogue turn, goal completion difficulty, user satisfaction estimation, and cooperative degree). The experimental results demonstrate I-Pro significantly outperforms baselines in terms of effectiveness and interpretability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39165620",
                    "name": "Wenqiang Lei"
                },
                {
                    "authorId": "1611287648",
                    "name": "Yao Zhang"
                },
                {
                    "authorId": "66947198",
                    "name": "Feifan Song"
                },
                {
                    "authorId": "46822058",
                    "name": "Hongru Liang"
                },
                {
                    "authorId": "3340223",
                    "name": "Jiaxin Mao"
                },
                {
                    "authorId": "2075420316",
                    "name": "Jiancheng Lv"
                },
                {
                    "authorId": "2149231521",
                    "name": "Zhenglu Yang"
                },
                {
                    "authorId": "143779329",
                    "name": "Tat-seng Chua"
                }
            ]
        },
        {
            "paperId": "5d0419f282aa8ad7c98c1f28876323645a7407d6",
            "title": "Multi-Party Empathetic Dialogue Generation: A New Task for Dialog Systems",
            "abstract": "Empathetic dialogue assembles emotion understanding, feeling projection, and appropriate response generation. Existing work for empathetic dialogue generation concentrates on the two-party conversation scenario. Multi-party dialogues, however, are pervasive in reality. Furthermore, emotion and sensibility are typically confused; a refined empathy analysis is needed for comprehending fragile and nuanced human feelings. We address these issues by proposing a novel task called Multi-Party Empathetic Dialogue Generation in this study. Additionally, a Static-Dynamic model for Multi-Party Empathetic Dialogue Generation, SDMPED, is introduced as a baseline by exploring the static sensibility and dynamic emotion for the multi-party empathetic dialogue learning, the aspects that help SDMPED achieve the state-of-the-art performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2478924",
                    "name": "Ling-Yu Zhu"
                },
                {
                    "authorId": "2148904527",
                    "name": "Zhengkun Zhang"
                },
                {
                    "authorId": "2152809962",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "2120853132",
                    "name": "Hongbin Wang"
                },
                {
                    "authorId": "2119019798",
                    "name": "Haiying Wu"
                },
                {
                    "authorId": "2149231521",
                    "name": "Zhenglu Yang"
                }
            ]
        },
        {
            "paperId": "882d631efdf718b140863ec498d185805cdcaa3e",
            "title": "Modeling Temporal-Modal Entity Graph for Procedural Multimodal Machine Comprehension",
            "abstract": "Procedural Multimodal Documents (PMDs) organize textual instructions and corresponding images step by step. Comprehending PMDs and inducing their representations for the downstream reasoning tasks is designated as Procedural MultiModal Machine Comprehension (M3C). In this study, we approach Procedural M3C at a fine-grained level (compared with existing explorations at a document or sentence level), that is, entity. With delicate consideration, we model entity both in its temporal and cross-modal relation and propose a novel Temporal-Modal Entity Graph (TMEG). Specifically, graph structure is formulated to capture textual and visual entities and trace their temporal-modal evolution. In addition, a graph aggregation module is introduced to conduct graph encoding and reasoning. Comprehensive experiments across three Procedural M3C tasks are conducted on a traditional dataset RecipeQA and our new dataset CraftQA, which can better evaluate the generalization of TMEG.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111012883",
                    "name": "Huibin Zhang"
                },
                {
                    "authorId": "2148904527",
                    "name": "Zhengkun Zhang"
                },
                {
                    "authorId": "1611287648",
                    "name": "Yao Zhang"
                },
                {
                    "authorId": "2152809962",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "2154875428",
                    "name": "Yufan Li"
                },
                {
                    "authorId": "2142142903",
                    "name": "Ning Jiang"
                },
                {
                    "authorId": "2007774059",
                    "name": "Xinde Wei"
                },
                {
                    "authorId": "2149231521",
                    "name": "Zhenglu Yang"
                }
            ]
        },
        {
            "paperId": "2eba956d63835c474955767ea458b3729472ade4",
            "title": "Self-Supervised Learning for speech recognition with Intermediate layer supervision",
            "abstract": "Recently, pioneer work finds that speech pre-trained models can solve full-stack speech processing tasks, because the model utilizes bottom layers to learn speaker-related information and top layers to encode content-related information. Since the network capacity is limited, we believe the speech recognition performance could be further improved if the model is dedicated to audio content information learning. To this end, we propose Intermediate Layer Supervision for Self-Supervised Learning (ILS-SSL), which forces the model to concentrate on content information as much as possible by adding an additional SSL loss on the intermediate layers. Experiments on LibriSpeech test-other set show that our method outperforms HuBERT significantly, which achieves a 23.5%/11.6% relative word error rate reduction in the w/o language model setting for base/large models. Detailed analysis shows the bottom layers of our model have a better correlation with phonetic units, which is consistent with our intuition and explains the success of our method for ASR.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "8206308",
                    "name": "Chengyi Wang"
                },
                {
                    "authorId": "49176273",
                    "name": "Yu Wu"
                },
                {
                    "authorId": "2107970655",
                    "name": "Sanyuan Chen"
                },
                {
                    "authorId": "2107983441",
                    "name": "Shujie Liu"
                },
                {
                    "authorId": "152319568",
                    "name": "Jinyu Li"
                },
                {
                    "authorId": "48549092",
                    "name": "Yao Qian"
                },
                {
                    "authorId": "2149231521",
                    "name": "Zhenglu Yang"
                }
            ]
        },
        {
            "paperId": "4511c0448206f3170d2bf6e087d05700c3326fb2",
            "title": "Fact-Tree Reasoning for N-ary Question Answering over Knowledge Graphs",
            "abstract": "Current Question Answering over Knowledge Graphs (KGQA) task mainly focuses on performing answer reasoning upon KGs with binary facts. However, it neglects the n-ary facts, which contain more than two entities. In this work, we highlight a more challenging but under-explored task: n-ary KGQA, i.e., answering n-ary facts questions upon n-ary KGs. Nevertheless, the multi-hop reasoning framework popular in binary KGQA task is not directly applicable on n-ary KGQA. We propose two feasible improvements: 1) upgrade the basic reasoning unit from entity or relation to fact, and 2) upgrade the reasoning structure from chain to tree. Therefore, we propose a novel fact-tree reasoning framework, FacTree, which integrates the above two upgrades. FacTree transforms the question into a fact tree and performs iterative fact reasoning on the fact tree to infer the correct answer. Experimental results on the n-ary KGQA dataset we constructed and two binary KGQA benchmarks demonstrate the effectiveness of FacTree compared with state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1611287648",
                    "name": "Yao Zhang"
                },
                {
                    "authorId": "2158012487",
                    "name": "Peiyao Li"
                },
                {
                    "authorId": "46822058",
                    "name": "Hongru Liang"
                },
                {
                    "authorId": "1774986",
                    "name": "A. Jatowt"
                },
                {
                    "authorId": "2149231521",
                    "name": "Zhenglu Yang"
                }
            ]
        },
        {
            "paperId": "b55a03afc6f8e6b27db626a79ff4db00b3881086",
            "title": "News Content Completion with Location-Aware Image Selection",
            "abstract": "News, as one of the fundamental social media types, typically contains both texts and images. Image selection, which involves choosing appropriate images according to some specified contexts, is crucial for formulating good news. However, it presents two challenges: where to place images and which images to use. The difficulties associated with this where-which problem lie in the fact that news typically contains linguistically rich text that delivers complex information and more than one image. In this paper, we propose a novel end-to-end two-stage framework to address these issues comprehensively. In the first stage, we identify key information in news by using location embeddings, which represent the local contextual information of each candidate location for image insertion. Then, in the second stage, we thoroughly examine the candidate images and select the most context-related ones to insert into each location identified in the first stage. We also introduce three insertion strategies to formulate different scenarios influencing the image selection procedure. Extensive experiments demonstrate the consistent superiority of the proposed framework in image selection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2148904527",
                    "name": "Zhengkun Zhang"
                },
                {
                    "authorId": "2152809962",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "1774986",
                    "name": "A. Jatowt"
                },
                {
                    "authorId": "2118237750",
                    "name": "Zhe Sun"
                },
                {
                    "authorId": "3075033",
                    "name": "Shao-Ping Lu"
                },
                {
                    "authorId": "2149231521",
                    "name": "Zhenglu Yang"
                }
            ]
        }
    ]
}