{
    "authorId": "153191671",
    "papers": [
        {
            "paperId": "fa0c7c2673437e09e81bab15514ee719697fac0d",
            "title": "Using Pre-Trained Language Models for Abstractive DBPEDIA Summarization: A Comparative Study",
            "abstract": ". Purpose : This study addresses the limitations of current short abstracts of DB-PEDIA entities, which often lack a comprehensive overview due to their creating method (i.e., selecting the \ufb01rst two-three sentences from the full DB PEDIA abstracts). Methodology : We leverage pre-trained language models to generate abstractive summaries of DB PEDIA abstracts in six languages (English, French, German, Italian, Spanish, and Dutch). We performed several experiments to assess the quality of generated summaries by language models. In particular, we evaluated the generated summaries using human judgments and automated metrics (Self-ROUGE and BERTScore). Additionally, we studied the correlation between human judgments and automated metrics in evaluating the generated summaries under different aspects: informativeness, coherence, conciseness, and \ufb02uency. Findings : Pre-trained language models generate summaries more concise and informative than existing short abstracts. Speci\ufb01cally, BART-based models effectively overcome the limitations of DB PEDIA short abstracts, especially for longer ones. Moreover, we show that BERTScore and ROUGE-1 are reliable metrics for assessing the informativeness and coherence of the generated summaries with respect to the full DB PEDIA abstracts. We also \ufb01nd a negative correlation between conciseness and human ratings. Furthermore, \ufb02uency evaluation remains challenging without human judgment. Value : This study has signi\ufb01cant implications for various applications in machine learning and natural language processing that rely on DB PEDIA resources. By providing succinct and comprehensive summaries, our approach enhances the quality of DB PEDIA abstracts and contributes to the semantic web community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2422519",
                    "name": "Hamada M. Zahera"
                },
                {
                    "authorId": "1516914336",
                    "name": "Fedor Vitiugin"
                },
                {
                    "authorId": "37273320",
                    "name": "M. A. Sherif"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "1712107",
                    "name": "A. N. Ngomo"
                }
            ]
        },
        {
            "paperId": "13d2dd81d308ac8575c3380dbeb673d77732d56a",
            "title": "Assessing the Impact of Music Recommendation Diversity on Listeners: A Longitudinal Study",
            "abstract": "We present the results of a 12-week longitudinal user study wherein the participants, 110 subjects from Southern Europe, received on a daily basis Electronic Music (EM) diversified recommendations. By analyzing their explicit and implicit feedback, we show that exposure to specific levels of music recommendation diversity may be responsible for long-term impacts on listeners\u2019 attitudes. In particular, we highlight the function of diversity in increasing the openness in listening to EM, a music genre not particularly known or liked by the participants previous to their participation in the study. Moreover, we demonstrate that recommendations may help listeners in removing positive and negative attachments towards EM, deconstructing pre-existing implicit associations but also stereotypes associated with this music. In addition, our results show the significant influence that recommendation diversity has in generating curiosity in listeners.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "74248595",
                    "name": "Lorenzo Porcaro"
                },
                {
                    "authorId": "2065565656",
                    "name": "Emilia G'omez"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "2487792a18f1f0d358ae5026e5d874b7260795e2",
            "title": "SMDRM: A Platform to Analyze Social Media for Disaster Risk Management in Near Real Time",
            "abstract": "Social media has been described as a mechanism for understanding a situation using information spread across many minds, i.e., a form of distributed cognition (Hutchins 1995). Gaining situational awareness in a disaster is critical and time-sensitive. Social media provides a vast data source that might help improve response in the early hours and days of a crisis. However, social media platforms may not provide information in a manner that is useful for crisis responders. SMDRM is a software platform that streamlines the processing of text and images extracted from Twitter in near real-time during a specific event.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50245405",
                    "name": "Valerio Lorini"
                },
                {
                    "authorId": "114389385",
                    "name": "E. Panizio"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "4afa91b57a5fdbd834e881ce129928cb457c3105",
            "title": "Cross-Lingual Query-Based Summarization of Crisis-Related Social Media: An Abstractive Approach Using Transformers",
            "abstract": "Relevant and timely information collected from social media during crises can be an invaluable resource for emergency management. However, extracting this information remains a challenging task, particularly when dealing with social media postings in multiple languages. This work proposes a cross-lingual method for retrieving and summarizing crisis-relevant information from social media postings. We describe a uniform way of expressing various information needs through structured queries and a way of creating summaries answering those information needs. The method is based on multilingual transformers embeddings. Queries are written in one of the languages supported by the embeddings, and the extracted sentences can be in any of the other languages supported. Abstractive summaries are created by transformers. The evaluation, done by crowdsourcing evaluators and emergency management experts, and carried out on collections extracted from Twitter during five large-scale disasters spanning ten languages, shows the flexibility of our approach. The generated summaries are regarded as more focused, structured, and coherent than existing state-of-the-art methods, and experts compare them favorably against summaries created by existing, state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1516914336",
                    "name": "Fedor Vitiugin"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "9af73b0f168e8e331a649bc90a4a28e43a04b9cf",
            "title": "The Coverage of Sexual Violence in Spanish News Media",
            "abstract": "The present study analyzes news articles about sexual violence published by online news media in Spain. Our goal is to get insights about the way in which sexual violence is covered and described, with a focus on biases described by previous research. We begin by collecting about 120,000 messages on Twitter (\u201ctweets\u201d) posted during 2020 by 13 of the most popular online news outlets in Spain. Next, we use a supervised classifier to detect tweets that are likely to contain links to news articles related to sexual violence, finding nearly 500 of them. We group these into clusters of articles that are likely to refer to the same event, and extract a series of descriptive elements using regular expressions. Finally, we compare these descriptors with official statistics about sexual violence in Spain. Our conclusions find biases that are well aligned with those described in the literature about the coverage of sexual violence in the news, indicating that this type of automated analysis can help uncover these biases. For instance, news media covers sexual assault cases much more often than sexual harassment cases, despite the latter being more frequent. More worryingly, crimes happening at home are under-represented in the media, and crimes happening in leisure spaces are over-represented. In general, rather than presenting a balanced view of different types of sexual violence, media outlets perpetuate and reinforce harmful preconceptions and myths.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065269182",
                    "name": "M. Budan"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "bb6322d0b4bebfd483e0b91e7ae60e1e800e2cbf",
            "title": "SciLander: Mapping the Scientific News Landscape",
            "abstract": "The COVID-19 pandemic has fueled the spread of misinformation on social media and the Web as a whole.\nThe phenomenon dubbed `infodemic' has taken the challenges of information veracity and trust to new heights by massively introducing seemingly scientific and technical elements into misleading content.\nDespite the existing body of work on modeling and predicting misinformation, the coverage of very complex scientific topics with inherent uncertainty and an evolving set of findings, such as COVID-19, provides many new challenges that are not easily solved by existing tools. \nTo address these issues, we introduce SciLander, a method for learning representations of news sources reporting on science-based topics.\nWe extract four heterogeneous indicators for the sources; two generic indicators that capture (1) the copying of news stories between sources, and (2) the use of the same terms to mean different things (semantic shift), and two scientific indicators that capture (1) the usage of jargon and (2) the stance towards specific citations.\nWe use these indicators as signals of source agreement, sampling pairs of positive (similar) and negative (dissimilar) samples, and combine them in a unified framework to train unsupervised news source embeddings with a triplet margin loss objective.\nWe evaluate our method on a novel COVID-19 dataset containing nearly 1M news articles from 500 sources spanning a period of 18 months since the beginning of the pandemic in 2020.\nOur results show that the features learned by our model outperform state-of-the-art baseline methods on the task of news veracity classification.\nFurthermore, a clustering analysis suggests that the learned representations encode information about the reliability, political leaning, and partisanship bias of these sources.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "51034107",
                    "name": "Mauricio G. Gruppi"
                },
                {
                    "authorId": "3314639",
                    "name": "Panayiotis Smeros"
                },
                {
                    "authorId": "3139418",
                    "name": "Sibel Adali"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "1751802",
                    "name": "K. Aberer"
                }
            ]
        },
        {
            "paperId": "1667d6041078a25ada9a5a0a3c4191337368bec9",
            "title": "Uneven Coverage of Natural Disasters in Wikipedia: the Case of Flood",
            "abstract": "The usage of non-authoritative data for disaster management presents the opportunity of accessing timely information that might not be available through other means, as well as the challenge of dealing with several layers of biases. Wikipedia, a collaboratively-produced encyclopedia, includes in-depth information about many natural and human-made disasters, and its editors are particularly good at adding information in real-time as a crisis unfolds. In this study, we focus on the English version of Wikipedia, that is by far the most comprehensive version of this encyclopedia. Wikipedia tends to have good coverage of disasters, particularly those having a large number of fatalities. However, we also show that a tendency to cover events in wealthy countries and not cover events in poorer ones permeates Wikipedia as a source for disaster-related information. By performing careful automatic content analysis at a large scale, we show how the coverage of floods in Wikipedia is skewed towards rich, English-speaking countries, in particular the US and Canada. We also note how coverage of floods in countries with the lowest income, as well as countries in South America, is substantially lower than the coverage of floods in middle-income countries. These results have implications for systems using Wikipedia or similar collaborative media platforms as an information source for detecting emergencies or for gathering valuable information for disaster response.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50245405",
                    "name": "Valerio Lorini"
                },
                {
                    "authorId": "2099715241",
                    "name": "Javier Rando"
                },
                {
                    "authorId": "1388387493",
                    "name": "Diego S\u00e1ez-Trumper"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "45646111b9ac030111b37a4146cf11b0d3204d6e",
            "title": "Social Media Alerts can Improve, but not Replace Hydrological Models for Forecasting Floods",
            "abstract": "Social media can be used for disaster risk reduction as a complement to traditional information sources, and the literature has suggested numerous ways to achieve this. In the case of floods, for instance, data collection from social media can be triggered by a severe weather forecast and/or a flood prediction. By way of contrast, in this paper we explore the possibility of having an entirely independent flood monitoring system which is based completely on social media, and which is completely self-activated. This independence and self-activation would bring increased robustness, as the system would not depend on other mechanisms for forecasting. We observe that social media can indeed help in the early detection of some flood events that would otherwise not be detected until later, albeit at the cost of many false positives. Overall, our experiments suggest that social media signals should only be used to complement existing monitoring systems, and we provide various explanations to support this argument.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50245405",
                    "name": "Valerio Lorini"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "2082076617",
                    "name": "D. Nappo"
                },
                {
                    "authorId": "69058764",
                    "name": "F. Dottori"
                },
                {
                    "authorId": "144911052",
                    "name": "P. Salamon"
                }
            ]
        },
        {
            "paperId": "52bb5a1f23b9ca111355c114dcf7b57abd3da0af",
            "title": "Comparison of Social Media in English and Russian During Emergencies and Mass Convergence Events",
            "abstract": "Twitter is used for spreading information during crisis events. In this paper, we \ufb01rst retrieve event-related information posted in English and Russian during six disasters and sports events that received wide media coverage in both languages, using an adaptive information \ufb01ltering method for automating the collection of about 100 000 messages. We then compare the contents of these messages in terms of 17 informational and linguistic features using a di\ufb00erence in di\ufb00erences approach. Our results suggest that posts in each language are focused on di\ufb00erent types of information. For instance, almost 50% of the popular people mentioned in these messages appear exclusively in either the English messages or the Russian messages, but not both. Our results also suggest di\ufb00erences in the adoption of platform mechanics during crises between Russian-speaking and English-speaking users. This has important implications for data collection during crises, which is almost always focused on a single language.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1516914336",
                    "name": "Fedor Vitiugin"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "6eb341c5cfebd8f4401ebb764c3fb322ab982464",
            "title": "Modeling Islamist Extremist Communications on Social Media using Contextual Dimensions",
            "abstract": "Terror attacks have been linked in part to online extremist content. Online conversations are cloaked in religious ambiguity, with deceptive intentions, often twisted from mainstream meaning to serve a malevolent ideology. Although tens of thousands of Islamist extremism supporters consume such content, they are a small fraction relative to peaceful Muslims. The efforts to contain the ever-evolving extremism on social media platforms have remained inadequate and mostly ineffective. Divergent extremist and mainstream contexts challenge machine interpretation, with a particular threat to the precision of classification algorithms. Radicalization is a subtle long-running persuasive process that occurs over time. Our context-aware computational approach to the analysis of extremist content on Twitter breaks down this persuasion process into building blocks that acknowledge inherent ambiguity and sparsity that likely challenge both manual and automated classification. Based on prior empirical and qualitative research in social sciences, particularly political science, we model this process using a combination of three contextual dimensions -- religion, ideology, and hate -- each elucidating a degree of radicalization and highlighting independent features to render them computationally accessible. We utilize domain-specific knowledge resources for each of these contextual dimensions such as Qur'an for religion, the books of extremist ideologues and preachers for political ideology and a social media hate speech corpus for hate. The significant sensitivity of the Islamist extremist ideology and its local and global security implications require reliable algorithms for modelling such communications on Twitter. Our study makes three contributions to reliable analysis: (i) Development of a computational approach rooted in the contextual dimensions of religion, ideology, and hate, which reflects strategies employed by online Islamist extremist groups, (ii) An in-depth analysis of relevant tweet datasets with respect to these dimensions to exclude likely mislabeled users, and (iii) A framework for understanding online radicalization as a process to assist counter-programming. Given the potentially significant social impact, we evaluate the performance of our algorithms to minimize mislabeling, where our context-aware approach outperforms a competitive baseline by 10.2% in precision, thereby enhancing the potential of such tools for use in human review.",
            "fieldsOfStudy": [
                "Computer Science",
                "Sociology"
            ],
            "authors": [
                {
                    "authorId": "2753547",
                    "name": "Ugur Kursuncu"
                },
                {
                    "authorId": "3072757",
                    "name": "Manas Gaur"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "82194910",
                    "name": "Amanuel Alambo"
                },
                {
                    "authorId": "3195814",
                    "name": "K. Thirunarayan"
                },
                {
                    "authorId": "2890773",
                    "name": "V. Shalin"
                },
                {
                    "authorId": "9626624",
                    "name": "Dilshod Achilov"
                },
                {
                    "authorId": "1750148",
                    "name": "I. Arpinar"
                },
                {
                    "authorId": "144463965",
                    "name": "A. Sheth"
                }
            ]
        },
        {
            "paperId": "758a29b97b416ef718daa8b4e6be672e13af1d2e",
            "title": "Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries",
            "abstract": "Social data in digital form\u2014including user-generated content, expressed or implicit relations between people, and behavioral traces\u2014are at the core of popular applications and platforms, driving the research agenda of many researchers. The promises of social data are many, including understanding \u201cwhat the world thinks\u201d about a social issue, brand, celebrity, or other entity, as well as enabling better decision-making in a variety of fields including public policy, healthcare, and economics. Many academics and practitioners have warned against the na\u00efve usage of social data. There are biases and inaccuracies occurring at the source of the data, but also introduced during processing. There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked. This paper recognizes the rigor with which these issues are addressed by different researchers varies across a wide range. We identify a variety of menaces in the practices around social data use, and organize them in a framework that helps to identify them. \u201cFor your own sanity, you have to remember that not all problems can be solved. Not all problems can be solved, but all problems can be illuminated.\u201d \u2013Ursula Franklin1",
            "fieldsOfStudy": [
                "Sociology",
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39824354",
                    "name": "Alexandra Olteanu"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "145472333",
                    "name": "Fernando Diaz"
                },
                {
                    "authorId": "3528206",
                    "name": "Emre K\u0131c\u0131man"
                }
            ]
        },
        {
            "paperId": "d65841794c3e22cc214cec7751713afe5ae3031f",
            "title": "Introduction to the Special Issue on AI for Disaster Management and Resilience",
            "abstract": "The articles in this special section provide a forum on the burgeoning topic of artificial intelligence (AI) for disaster management and resilience.",
            "fieldsOfStudy": [
                "Sociology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "119916328",
                    "name": "Yu-Ru Lin"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "2012701156",
                    "name": "Jie Yin"
                }
            ]
        },
        {
            "paperId": "ec2300918fb451c1a58925925ba355fa7d7bc3a5",
            "title": "Integrating Social Media into a Pan-European Flood Awareness System: A Multilingual Approach",
            "abstract": "This paper describes a prototype system that integrates social media analysis into the European Flood Awareness System (EFAS). This integration allows the collection of social media data to be automatically triggered by flood risk warnings determined by a hydro-meteorological model. Then, we adopt a multi-lingual approach to find flood-related messages by employing two state-of-the-art methodologies: language-agnostic word embeddings and language-aligned word embeddings. Both approaches can be used to bootstrap a classifier of social media messages for a new language with little or no labeled data. Finally, we describe a method for selecting relevant and representative messages and displaying them back in the interface of EFAS.",
            "fieldsOfStudy": [
                "Computer Science",
                "Political Science"
            ],
            "authors": [
                {
                    "authorId": "50245405",
                    "name": "Valerio Lorini"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "69058764",
                    "name": "F. Dottori"
                },
                {
                    "authorId": "31925229",
                    "name": "M. Kalas"
                },
                {
                    "authorId": "2082076617",
                    "name": "D. Nappo"
                },
                {
                    "authorId": "144911052",
                    "name": "P. Salamon"
                }
            ]
        },
        {
            "paperId": "7d6c9d8b5596f90049c0e266aee1750a001335eb",
            "title": "Processing Social Media Messages in Mass Emergency: Survey Summary",
            "abstract": "Millions of people use social media to share information during disasters and mass emergencies. Information available on social media, particularly in the early hours of an event when few other sources are available, can be extremely valuable for emergency responders and decision makers, helping them gain situational awareness and plan relief efforts. Processing social media content to obtain such information involves solving multiple challenges, including parsing brief and informal messages, handling information overload, and prioritizing different types of information. These challenges can be mapped to information processing operations such as filtering, classifying, ranking, aggregating, extracting, and summarizing. This work highlights these challenges and presents state of the art computational techniques to deal with social media messages, focusing on their application to crisis scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "145472333",
                    "name": "Fernando Diaz"
                },
                {
                    "authorId": "3681090",
                    "name": "Sarah Vieweg"
                }
            ]
        },
        {
            "paperId": "7f8dedc8c956a57ffb5016dc545c8084d8800b67",
            "title": "The 5th International Workshop on Social Web for Disaster Management(SWDM'18): Collective Sensing, Trust, and Resilience in Global Crises",
            "abstract": "During large-scale emergencies such as natural and man-made disasters, a massive amount of information is posted by the public in social media. Collecting, aggregating, and presenting this information to stakeholders can be extremely challenging, particularly if an understanding of the \"big picture\u00bb\u00bb is sought. This international workshop, the fifth in the series, is a key venue for researchers and practitioners to discuss research challenges and technical issues around the usage of social media in disaster management. Workshop\u00bbs website: https://sites.google.com/site/swdm2018/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34821764",
                    "name": "Y. Lin"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "2965064",
                    "name": "Jie Yin"
                }
            ]
        },
        {
            "paperId": "8cfa39cb13777b67b63ce7ba82e35cee16b70b87",
            "title": "The Effect of Extremist Violence on Hateful Speech Online",
            "abstract": "\n \n User-generated content online is shaped by many factors, including endogenous elements such as platform affordances and norms, as well as exogenous elements, in particular significant events. These impact what users say, how they say it, and when they say it. In this paper, we focus on quantifying the impact of violent events on various types of hate speech, from offensive and derogatory to intimidation and explicit calls for violence. We anchor this study in a series of attacks involving Arabs and Muslims as perpetrators or victims, occurring in Western countries, that have been covered extensively by news media. These attacks have fueled intense policy debates around immigration in various fora, including online media, which have been marred by racist prejudice and hateful speech. The focus of our research is to model the effect of the attacks on the volume and type of hateful speech on two social media platforms, Twitter and Reddit. Among other findings, we observe that extremist violence tends to lead to an increase in online hate speech, particularly on messages directly advocating violence. Our research has implications for the way in which hate speech online is monitored and suggests ways in which it could be fought.\n \n",
            "fieldsOfStudy": [
                "Computer Science",
                "Sociology",
                "Political Science"
            ],
            "authors": [
                {
                    "authorId": "39824354",
                    "name": "Alexandra Olteanu"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "48857747",
                    "name": "Jeremy Boy"
                },
                {
                    "authorId": "1712865",
                    "name": "Kush R. Varshney"
                }
            ]
        },
        {
            "paperId": "97a32b03bc0073bbd6b65249a7d227bfa6e37db5",
            "title": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management",
            "abstract": "Since 1992, the ACM International Conference on Information and Knowledge Management (CIKM) has brought together leading researchers and developers from the knowledge management, information retrieval, and data management communities to discuss cutting-edge research on advanced knowledge and information systems. We are pleased to present the 26th edition of CIKM on 6-10 November, 2017, at the Pan Pacific Singapore hotel, with the special theme of Smart Cities, Smart Nations. \n \nThis year our attendees will enjoy four keynote speakers: Rajeev Rastogi (Amazon), Qiang Yang (HKUST), Rada Mihalcea (Michigan), and K Ananth Krishnan (Tata Consultancy Services). In 6-7 parallel sessions, our program includes presentations of 171 full research papers, 119 short research papers, and 30 demonstrations of new research advances. The program's focus this year can be seen at a glance in the word cloud at right, constructed from the titles of all accepted research papers. Also on offer are eight tutorials on timely research topics, and six collocated workshops on topics ranging from history to transportation, biomedicine to bias. \n \nWe are excited about our greatly expanded data analytics competition this year, the CIKM AnalytiCup. During the past nine months, over 1500 teams from all over the world have vied to win over $60,000 in AnalytiCup prizes and travel money by solving real-world analytics problems posed by our corporate sponsors Alibaba/Shenzhen Meteorological Bureau, DataSpark, and Lazada. A fourth competition, a weekend-long hackathon sponsored by DHL, takes place immediately before the conference. The finalists from all four competitions come together on 6 November for a final showdown in front of corporate judges. Solution summaries from finalist teams in the first three competitions can be found in these proceedings. \n \nAlso new this year are several other events aimed directly at practitioners. During the main conference, we are offering hands-on tutorials on the hot topics of scalable deep learning and scalable data science. The Case Studies track, intended to highlight the experiences and lessons learned by early adopters, debuts this year with 23 studies of technology adoption in interesting applications. And immediately before the main conference, CIKMconnect brings together students and industry for posters, technical discussions, recruiting events, and networking. \n \nIt takes a village to produce a major conference! Our program committee chairs, senior PC and PC members valiantly and gracefully handled a record total number of submissions: 855 full research papers, 419 short research papers, 80 demos, and 103 case studies. Each submission was reviewed by three program committee members, each a recognized expert in the field, and an independent committee selected the full paper awards recipients.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709901",
                    "name": "Ee-Peng Lim"
                },
                {
                    "authorId": "145136558",
                    "name": "M. Winslett"
                },
                {
                    "authorId": "144721996",
                    "name": "M. Sanderson"
                },
                {
                    "authorId": "1699363",
                    "name": "A. Fu"
                },
                {
                    "authorId": "1738536",
                    "name": "Jimeng Sun"
                },
                {
                    "authorId": "71766254",
                    "name": "Shane Culpepper"
                },
                {
                    "authorId": "145373095",
                    "name": "Eric Lo"
                },
                {
                    "authorId": "152316651",
                    "name": "Joyce Ho"
                },
                {
                    "authorId": "145985025",
                    "name": "D. Donato"
                },
                {
                    "authorId": "144947410",
                    "name": "R. Agrawal"
                },
                {
                    "authorId": "145473095",
                    "name": "Yu Zheng"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "1735962",
                    "name": "Aixin Sun"
                },
                {
                    "authorId": "1684751",
                    "name": "V. Tseng"
                },
                {
                    "authorId": "2829009",
                    "name": "Chenliang Li"
                }
            ]
        },
        {
            "paperId": "f05339cf19b31cf3a3dfe6f9d7a3315dfba57c52",
            "title": "Summarization Approach From Microblog During Disaster Events",
            "abstract": "During bulk convergence events such as natural disasters, microblogging platforms like Twitter are broadly used by affected people to post situational awareness messages. As soon as natural disaster events happen, users are willing to know more about them. Twitter is a great source that can be exploited for obtaining such fine-grained arranged information for fresh natural disaster events. These crisis-related messages disperse among multiple categories like infrastructure damage, information about bomb blast, missing, injured, and dead people etc. The challenge here is to create summary from disaster related tweets and filter the short spam url containing tweets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "113657372",
                    "name": "Pooja B. Kawade"
                },
                {
                    "authorId": "1414634568",
                    "name": "N. N.Pise"
                },
                {
                    "authorId": "47326709",
                    "name": "P. Kulkarni"
                },
                {
                    "authorId": "2042376",
                    "name": "Koustav Rudra"
                },
                {
                    "authorId": "7359504",
                    "name": "Subham Ghosh"
                },
                {
                    "authorId": "4213990",
                    "name": "Niloy Ganguly"
                },
                {
                    "authorId": "2232128391",
                    "name": "Siddhartha Banerjee"
                },
                {
                    "authorId": "2570223",
                    "name": "A. Bruns"
                },
                {
                    "authorId": "2231445628",
                    "name": "Yxian"
                },
                {
                    "authorId": "2264427239",
                    "name": "A. Olariu"
                },
                {
                    "authorId": "35373727",
                    "name": "Sandeep Panem"
                },
                {
                    "authorId": "46722320",
                    "name": "Manish Gupta"
                },
                {
                    "authorId": "145205784",
                    "name": "Vasudeva Varma"
                },
                {
                    "authorId": "2231446010",
                    "name": "Structured"
                },
                {
                    "authorId": "2108330403",
                    "name": "Zhenhua Wang"
                },
                {
                    "authorId": "144128216",
                    "name": "Lidan Shou"
                },
                {
                    "authorId": "2152953644",
                    "name": "Ke Chen"
                },
                {
                    "authorId": "2146662153",
                    "name": "Gang Chen"
                },
                {
                    "authorId": "32584303",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "145472333",
                    "name": "Fernando Diaz"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "2113393444",
                    "name": "Chao Shen"
                },
                {
                    "authorId": "2118168646",
                    "name": "Fei Liu"
                },
                {
                    "authorId": "2231461270",
                    "name": "Ji Lucas"
                },
                {
                    "authorId": "2057788",
                    "name": "M. Osborne"
                },
                {
                    "authorId": "153760564",
                    "name": "E. Cano"
                },
                {
                    "authorId": "2231459133",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "144122487",
                    "name": "R. Power"
                },
                {
                    "authorId": "35204326",
                    "name": "B. Robinson"
                },
                {
                    "authorId": "144256929",
                    "name": "J. Colton"
                },
                {
                    "authorId": "1492161567",
                    "name": "Pengyi Zhang"
                },
                {
                    "authorId": "2145763295",
                    "name": "Chao Chen"
                },
                {
                    "authorId": "27672597",
                    "name": "Jinchao Zhang"
                },
                {
                    "authorId": "2231659475",
                    "name": "W. Zhou"
                },
                {
                    "authorId": "2131176486",
                    "name": "Shi-xiang Liu"
                },
                {
                    "authorId": "70402545",
                    "name": "Yang Xiang"
                }
            ]
        },
        {
            "paperId": "f9f8c6bad954aaac4d8c5c8bfb533a9aa3428532",
            "title": "Story\u2010focused reading in online news and its potential for user engagement",
            "abstract": "We study the news reading behavior of several hundred thousand users on 65 highly visited news sites. We focus on a specific phenomenon: users reading several articles related to a particular news development, which we call story\u2010focused reading. Our goal is to understand the effect of story\u2010focused reading on user engagement and how news sites can support this phenomenon. We found that most users focus on stories that interest them and that even casual news readers engage in story\u2010focused reading. During story\u2010focused reading, users spend more time reading and a larger number of news sites are involved. In addition, readers employ different strategies to find articles related to a story. We also analyze how news sites promote story\u2010focused reading by looking at how they link their articles to related content published by them, or by other sources. The results show that providing links to related content leads to a higher engagement of the users, and that this is the case even for links to external sites. We also show that the performance of links can be affected by their type, their position, and how many of them are present within an article.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "16598985",
                    "name": "Janette Lehmann"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "1684032",
                    "name": "M. Lalmas"
                },
                {
                    "authorId": "1389957009",
                    "name": "R. Baeza-Yates"
                }
            ]
        },
        {
            "paperId": "0c2b8c24ee758a21b4b7c3d5e5be1972953464f0",
            "title": "The Fourth International Workshop on Social Web for Disaster Management (SWDM 2016)",
            "abstract": "The proliferation of social media platforms together with the wide adoption of smartphone devices has transformed how we communicate and share news. During large-scale emergencies, such as natural disasters or armed attacks, victims, responders, and volunteers increasingly use social media to post situation updates and to request and offer help. The use of social media for emergency and disaster response has been a prominent application of information and knowledge management techniques in recent years. There are a number of challenges associated with near real-time processing of vast volumes of information in a way that makes sense for people directly affected, for volunteer organizations, and for official emergency response agencies. As massive amount of messages posted by users are transformed into semi-structured records via information extraction and natural language processing techniques, there is a growing need for developing advanced techniques to aggregate this large-scale data to gain an understanding of the ``big picture'' of an emergency, and to detect and predict how a disaster could develop. This workshop seeks to provide a platform for the exchange of ideas, identification of important problems, and discovery of possible synergies. It will enable interesting discussions and encouraged collaboration between various disciplines, and information and knowledge management approaches is the core of this workshop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "145472333",
                    "name": "Fernando Diaz"
                },
                {
                    "authorId": "34821764",
                    "name": "Y. Lin"
                },
                {
                    "authorId": "2965064",
                    "name": "Jie Yin"
                }
            ]
        },
        {
            "paperId": "9d85a643e65bea868ed6067bf4edf437dc7eef7d",
            "title": "A Robust Framework for Classifying Evolving Document Streams in an Expert-Machine-Crowd Setting",
            "abstract": "An emerging challenge in the online classification of social media data streams is to keep the categories used for classification up-to-date. In this paper, we propose an innovative framework based on an Expert-Machine-Crowd (EMC) triad to help categorize items by continuously identifying novel concepts in heterogeneous data streams often riddled with outliers. We unify constrained clustering and outlier detection by formulating a novel optimization problem: COD-Means. We design an algorithm to solve the COD-Means problem and show that COD-Means will not only help detect novel categories but also seamlessly discover human annotation errors and improve the overall quality of the categorization process. Experiments on diverse real data sets demonstrate that our approach is both effective and efficient.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32584303",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "50793091",
                    "name": "S. Chawla"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "d1d9e742cef8ef63fe888f83996149e612174210",
            "title": "Overview of the Special Issue on Trust and Veracity of Information in Social Media",
            "abstract": "From a business and government point of view, there is an increasing need to interpret and act upon information from large-volume media, such as Twitter, Facebook and Web news. However, knowledge gathered from such online sources comes with a major caveat\u2014it cannot always be trusted, nor is it always factual or of high quality. Rumors tend to spread rapidly through social networks, and their veracity is hard to establish in a timely fashion. For instance, during an earthquake in Chile, rumors spread through Twitter that a volcano became active and there was a tsunami warning in Valparaiso [Castillo et al. 2013]. Later, these reports were found to be false. Another example concerns \u201castroturf campaigns\u201d\u2014a malicious use of Twitter and other social media during election campaigns to provide fake support of a message or project by grassroots participants, while at the same time hiding the original campaign sponsors (usually elite groups or their lobbies). Researchers have identified numerous sources of untrusted content, and through them they found that several online communities interact with narratives stemming from conspiracy theories [Bessi et al. 2015]. A 2012",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144178604",
                    "name": "S. Papadopoulos"
                },
                {
                    "authorId": "1723649",
                    "name": "Kalina Bontcheva"
                },
                {
                    "authorId": "2953357",
                    "name": "E. Jaho"
                },
                {
                    "authorId": "144307089",
                    "name": "M. Lupu"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "f8cd9e3da23d01ee7487be29ca251b6ad20889af",
            "title": "Combining Human Computing and Machine Learning to Make Sense of Big (Aerial) Data for Disaster Response",
            "abstract": "Aerial imagery captured via unmanned aerial vehicles (UAVs) is playing an increasingly important role in disaster response. Unlike satellite imagery, aerial imagery can be captured and processed within hours rather than days. In addition, the spatial resolution of aerial imagery is an order of magnitude higher than the imagery produced by the most sophisticated commercial satellites today. Both the United States Federal Emergency Management Agency (FEMA) and the European Commission's Joint Research Center (JRC) have noted that aerial imagery will inevitably present a big data challenge. The purpose of this article is to get ahead of this future challenge by proposing a hybrid crowdsourcing and real-time machine learning solution to rapidly process large volumes of aerial data for disaster response in a time-sensitive manner. Crowdsourcing can be used to annotate features of interest in aerial images (such as damaged shelters and roads blocked by debris). These human-annotated features can then be used to train a supervised machine learning system to learn to recognize such features in new unseen images. In this article, we describe how this hybrid solution for image analysis can be implemented as a module (i.e., Aerial Clicker) to extend an existing platform called Artificial Intelligence for Disaster Response (AIDR), which has already been deployed to classify microblog messages during disasters using its Text Clicker module and in response to Cyclone Pam, a category 5 cyclone that devastated Vanuatu in March 2015. The hybrid solution we present can be applied to both aerial and satellite imagery and has applications beyond disaster response such as wildlife protection, human rights, and archeological exploration. As a proof of concept, we recently piloted this solution using very high-resolution aerial photographs of a wildlife reserve in Namibia to support rangers with their wildlife conservation efforts (SAVMAP project, http://lasig.epfl.ch/savmap ). The results suggest that the platform we have developed to combine crowdsourcing and machine learning to make sense of large volumes of aerial images can be used for disaster response.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "49317293",
                    "name": "P. Meier"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "2977931",
                    "name": "D. Tuia"
                },
                {
                    "authorId": "2070318733",
                    "name": "Nicolas Rey"
                },
                {
                    "authorId": "1382155500",
                    "name": "Julien Briant"
                },
                {
                    "authorId": "3348860",
                    "name": "P. Rudd"
                },
                {
                    "authorId": "144152549",
                    "name": "F. Reinhard"
                },
                {
                    "authorId": "144939013",
                    "name": "Matthew Parkan"
                },
                {
                    "authorId": "2436529",
                    "name": "S. Joost"
                }
            ]
        },
        {
            "paperId": "f97369154badbe4d12870b1627c056aa25b50be5",
            "title": "Vagueness: Natural Language and Semantics",
            "abstract": "During the 2015 Nepal earthquake, a 26-year-old Indian lawyer and activist posted the following on Twitter: Media must report about d alleged 20k RSS chaps off 2 #Nepal.here's a pic coz d 1 @ShainaNC shared isn't true.. ;) Meaning: media must report about allegations that twenty thousand volunteers from India's Rashtriya Swayamsevak Sangh (RSS) had joined the relief efforts in Nepal, as falsely claimed on Twitter by Shaina NC (a member of the Bharatiya Janata Party, a political group close to the RSS). This message mixes shortened words (\u201cd\u201d for \u201cthe,\u201d \u201c2\u201d for \u201cto,\u201d \u201ccoz\u201d for \u201cbecause,\u201d \u201cpic\u201d for \u201cpicture\u201d), ambiguous abbreviations (\u201cRSS,\u201d which may mean a number of things), British slang (\u201cchaps\u201d), platform-specific codes (such as the hashtag #Nepal and the user mention @ShainaNC ), punctuation/capitalization issues (lack of spacing between #Nepal and here , usage of two dots instead of an ellipsis), and sarcasm expressed through a \u201cwink\u201d emoticon (\u201c)\u201d). In general, understanding a message in social media requires contextual information to compensate for fragmented, ambiguous \u2013 in otherwords, vague \u2013 text that is open to more than one interpretation. This chapter is about Natural Language Processing (NLP), which encompasses computational methods created for dealing with human language. NLP methods incorporating statistical machine learning elements were developed in the 1980s and 1990s using mostly profesionally written texts, such as newspaper articles. Since the late 1990s and the 2000s, these methods have been extended to deal first with Web content, and in the late 2000s and early 2010s, with social media messages and short text messages sent from mobile phones (SMS). Many modern NLP methods are based on machine learning. The next section ( \u00a7 3.1) describes the text of social media messages. Then, we outline basic NLP methods such as tokenization, stemming, part-of-speech tagging, and dependency parsing ( \u00a7 3.2), as well as sentiment analysis/opinion mining ( \u00a7 3.3). Next, we describe how to locate references to entities such as people and organizations ( \u00a7 3.4), and, particularly, places ( \u00a7 3.5). Finally, we refer to methods for extracting structured data from unstructured text ( \u00a7 3.6), and for adding semantics to messages ( \u00a7 3.7). Social Media Is Conversational In general on the Internet \u201cwe find language that is fragmentary, laden with typographical errors, often bereft of punctuation, and sometimes downright incoherent\u201d (Baron, 2003).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "ff564eaf13da72c6ae8075b8c3a6d6c49f19936c",
            "title": "Enabling Digital Health by Automatic Classification of Short Messages",
            "abstract": "In response to the growing HIV/AIDS and other health-related issues, UNICEF through their U-Report platform receives thousands of messages (SMS) every day to provide prevention strategies, health case advice, and counseling support to vulnerable population. Due to a rapid increase in U-Report usage (up to 300% in last 3 years), plus approximately 1,000 new registrations each day, the volume of messages has thus continued to increase, which made it impossible for the team at UNICEF to process them in a timely manner. In this paper, we present a platform designed to perform automatic classification of short messages (SMS) in real-time to help UNICEF categorize and prioritize health-related messages as they arrive. We employ a hybrid approach, which combines human and machine intelligence that seeks to resolve the information overload issue by introducing processing of large-scale data at high-speed while maintaining a high classification accuracy. The system has recently been tested in conjunction with UNICEF in Zambia to classify short messages received via the U-Report platform on various health related issues. The system is designed to enable UNICEF make sense of a large volume of short messages in a timely manner. In terms of evaluation, we report design choices, challenges, and performance of the system observed during the deployment to validate its effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32584303",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "49317293",
                    "name": "P. Meier"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "1411222533",
                    "name": "Andre Lesa"
                },
                {
                    "authorId": "1401308012",
                    "name": "M. Garc\u00eda-Herranz"
                }
            ]
        },
        {
            "paperId": "21b086a3b78467fc4bc9c4135fbf06b03ad21ac4",
            "title": "The Case for Readability of Crisis Communications in Social Media",
            "abstract": "The readability of text documents has been studied from a linguistic perspective long before people began to regularly communicate via Internet technologies. Typically, such studies look at books or articles containing many paragraphs and pages. However, the readability of short messages comprising a few sentences, common on today's social networking sites and microblogging services, has received less attention from researchers working on \"readability\". Emergency management specialists, crisis response practitioners, and scholars have long recognized that clear communication is essential during crises. To the best of our knowledge, the work we present here is the first to study the readability of crisis communications posted on Twitter-by governments, non-governmental organizations, and mainstream media. The data we analyze is comprised of hundreds of tweets posted during 15 different crises in English-speaking countries, which happened between 2012 and 2013. We describe factors which negatively affect comprehension, and consider how understanding can be improved. Based on our analysis and observations, we conclude with several recommendations for how to write brief crisis messages on social media that are clear and easy to understand.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216191",
                    "name": "Irina Temnikova"
                },
                {
                    "authorId": "3681090",
                    "name": "Sarah Vieweg"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "396036235e241fcb595b59a64ae87d939bcbccc4",
            "title": "Towards a Data-driven Approach to Identify Crisis-Related Topics in Social Media Streams",
            "abstract": "While categorizing any type of user-generated content online is a challenging problem, categorizing social media messages during a crisis situation adds an additional layer of complexity, due to the volume and variability of information, and to the fact that these messages must be classified as soon as they arrive. Current approaches involve the use of automatic classification, human classification, or a mixture of both. In these types of approaches, there are several reasons to keep the number of information categories small and updated, which we examine in this article. This means at the onset of a crisis an expert must select a handful of information categories into which information will be categorized. The next step, as the crisis unfolds, is to dynamically change the initial set as new information is posted online. In this paper, we propose an effective way to dynamically extract emerging, potentially interesting, new categories from social media data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32584303",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "58ecfdc0161283266ec0bd853b56ec913c6af42b",
            "title": "What to Expect When the Unexpected Happens: Social Media Communications Across Crises",
            "abstract": "The use of social media to communicate timely information during crisis situations has become a common practice in recent years. In particular, the one-to-many nature of Twitter has created an opportunity for stakeholders to disseminate crisis-relevant messages, and to access vast amounts of information they may not otherwise have. Our goal is to understand what affected populations, response agencies and other stakeholders can expect-and not expect-from these data in various types of disaster situations. Anecdotal evidence suggests that different types of crises elicit different reactions from Twitter users, but we have yet to see whether this is in fact the case. In this paper, we investigate several crises-including natural hazards and human-induced disasters-in a systematic manner and with a consistent methodology. This leads to insights about the prevalence of different information types and sources across a variety of crisis situations.",
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "authors": [
                {
                    "authorId": "39824354",
                    "name": "Alexandra Olteanu"
                },
                {
                    "authorId": "3681090",
                    "name": "Sarah Vieweg"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "825d51c902c939e4ad78dc768cd580f9796dc8e1",
            "title": "EMTerms 1.0: A Terminological Resource for Crisis Tweets",
            "abstract": "We present the first release of EMTerms (Emergency Management Terms), the largest crisis-related terminological resource to date, containing over 7,000 terms used in Twitter to describe various crises. This resource can be used by practitioners to search for relevant messages in Twitter during crises, and by computer scientists to develop new automatic methods for crises in Twitter. The terms have been collected from a seed set of terms manually annotated by a linguist and an emergency manager from tweets broadcast during 4 crisis events. A Conditional Random Fields (CRF) method was then applied to tweets from 35 crisis events, in order to expand the set of terms while overcoming the difficulty of getting more emergency managers\u2019 annotations. The terms are classified into 23 information-specific categories, by using a combination of expert annotations and crowdsourcing. This article presents the detailed terminology extraction methodology, as well as final results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216191",
                    "name": "Irina Temnikova"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "3681090",
                    "name": "Sarah Vieweg"
                }
            ]
        },
        {
            "paperId": "8e1ad45cb441399758fdc45896c63be25f1f964f",
            "title": "Comparing Events Coverage in Online News and Social Media: The Case of Climate Change",
            "abstract": "\n \n Social media is becoming more and more integrated in the distribution and consumption of news. How is news in social media different from mainstream news? This paper presents a comparative analysis covering a span of 17 months and hundreds of news events, using a method that combines automatic and manual annotations. We focus on climate change, a topic that is frequently present in the news through a number of arguments, from current practices and causes (e.g. fracking, CO2 emissions) to consequences and solutions (e.g. extreme weather, electric cars). The coverage that these different aspects receive is often dependent on how they are framed---typically by mainstream media. Yet, evidence suggests an existing gap between what the news media publishes online and what the general public shares in social media. Through the analysis of a series of events, including awareness campaigns, natural disasters, governmental meetings and publications, among others, we uncover differences in terms of the triggers, actions, and news values that are prevalent in both types of media. This methodology can be extended to other important topics present in the news.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39824354",
                    "name": "Alexandra Olteanu"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "2943892",
                    "name": "N. Diakopoulos"
                },
                {
                    "authorId": "1751802",
                    "name": "K. Aberer"
                }
            ]
        },
        {
            "paperId": "021a03abbe88c92a6f6b7c49f41420c25798f6e4",
            "title": "Developing Mobile Linked Data Applications",
            "abstract": "With the rapid advancement of mobile technologies, users are generating and consuming a significant amount of data on their handheld devices. However, the lack of Linked Data tools for these devices has left much of the data unstructured and difficult to reuse and integrate with other datasets. We will demonstrate an application development framework that enables the easy development of mobile apps that generate and consume Linked Data. We also provide a set of easy-to-deploy Web Services to supplement functionality for mobile apps focused on crowdsourcing. We motivate our work by describing a real-world application of this framework, which is a disaster relief application that streams crowd-sourced reports in real time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1693195",
                    "name": "O. Seneviratne"
                },
                {
                    "authorId": "2538510",
                    "name": "E. Patton"
                },
                {
                    "authorId": "9317291",
                    "name": "Daniela Miao"
                },
                {
                    "authorId": "4007305",
                    "name": "Fuming Shih"
                },
                {
                    "authorId": "2108712122",
                    "name": "Weihua Li"
                },
                {
                    "authorId": "1735243",
                    "name": "Lalana Kagal"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "0b663b6ffa9639c70fa9bb24cf59cfc25486c1fe",
            "title": "Ranking item features by mining online user-item interactions",
            "abstract": "We assume a database of items in which each item is described by a set of attributes, some of which could be multi-valued. We refer to each of the distinct attribute values as a feature. We also assume that we have information about the interactions (such as visits or likes) between a set of users and those items. In our paper, we would like to rank the features of an item using user-item interactions. For instance, if the items are movies, features could be actors, directors or genres, and user-item interaction could be user liking the movie. These information could be used to identify the most important actors for each movie. While users are drawn to an item due to a subset of its features, a user-item interaction only provides an expression of user preference over the entire item, and not its component features. We design algorithms to rank the features of an item depending on whether interaction information is available at aggregated or individual level granularity and extend them to rank composite features (set of features). Our algorithms are based on constrained least squares, network flow and non-trivial adaptations to non-negative matrix factorization. We evaluate our algorithms using both real-world and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009678",
                    "name": "Sofiane Abbar"
                },
                {
                    "authorId": "123014225",
                    "name": "Habibur Rahman"
                },
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "145080425",
                    "name": "Gautam Das"
                }
            ]
        },
        {
            "paperId": "43519ba682542b79d20c8d7407232520aba1700a",
            "title": "Controversy and Sentiment in Online News",
            "abstract": "How do news sources tackle controversial issues? In this work, we take a data-driven approach to understand how controversy interplays with emotional expression and biased language in the news. We begin by introducing a new dataset of controversial and noncontroversial terms collected using crowdsourcing. Then, focusing on 15 major U.S. news outlets, we compare millions of articles discussing controversial and non-controversial issues over a span of 7 months. We find that in general, when it comes to controversial issues, the use of negative affect and biased language is prevalent, while the use of strong emotion is tempered. We also observe many differences across news sources. Using these findings, we show that we can indicate to what extent an issue is controversial, by comparing it with other issues in terms of how they are portrayed across different media.",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "3329961",
                    "name": "Yelena Mejova"
                },
                {
                    "authorId": "144518215",
                    "name": "Amy X. Zhang"
                },
                {
                    "authorId": "2943892",
                    "name": "N. Diakopoulos"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "4d2f636960f80e2ea68a54e6379498d3da9336e5",
            "title": "Expanding The Humanitarian Computing Library Using Automatic Tools",
            "abstract": "Humanitarian computing has become a critical field to support the response to natural and man-made disasters all over the world. The Humanitarian Computing Library (http://humanitariancomp.referata.com/) is a collaboratively edited (\"wiki\") site that includes a compilation of publications related to humanitarian computing. Each publication is represented by its meta-data including type (e.g. conference presentation or journal article), authors, date, abstract, published location as well as its URL. Currently, 770 publications exist in the library and are categorized into a few dozen categories to provide easy access for researchers. The aim of the project is to augment the contents of the library to include up-to-date publications, by automating the search for related publications instead of using the previous, manual search method. In addition, we extend the publications' meta-data by adding information about their citation count, in order to highlight the most important ones. The derived solution is a publicly-available web service, that allows any editor to use it to expand the library. The web services work by performing a programmatic query to Google Scholar and analyzing the result. The query contains a sample obtained from the list of papers on a given category of the library (e.g. \"Social Media and Crises\"). For each of the papers currently in the library, we obtain new papers citing it. We then remove duplicate titles and sort by decreasing frequency, removing papers that are already present in the library. As the Humanitarian Computing research field is quite new, new publications are highly dependent on previous ones, and thus related publications can be identified by three indicators: they cite an existing publication from the library, have common phrases/keywords with existing previous publications, and are highly cited. The developed web service made it easy to add recent publications to the library with much less effort than before. In conclusion, the deployed solution acts as a tool for collecting research papers on humanitarian computing, and helps expanding this knowledge base. Such a publicly-available web service can help advance the state of the art of Humanitarian Computing, leading to an improved understanding of this field, and potentially help minimize the loss of lives and property during disasters by the application of new technologies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "70082815",
                    "name": "Bs.d Undergrad Ali Khalil El Dous"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "691e82dc00e44b0267eb0104ec6e17568bb7fa69",
            "title": "FAST: forecast and analytics of social media and traffic",
            "abstract": "We present FAST (\\url{http://fast.qcri.org/}), a platform for real-time traffic predictions in online news sources. FAST accurately forecasts the future number of page views of an article based on user traffic and social media engagement signals. To our knowledge, this is the first industrial scale, real-time system for predictive web analytics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1699369",
                    "name": "Venkata Rama Kiran Garimella"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "6f6ee28dfb1ba341dd2b74b5bdcd276357182bab",
            "title": "Composite Retrieval of Diverse and Complementary Bundles",
            "abstract": "Users are often faced with the problem of finding complementary items that together achieve a single common goal (e.g., a starter kit for a novice astronomer, a collection of question/answers related to low-carb nutrition, a set of places to visit on holidays). In this paper, we argue that for some application scenarios returning item bundles is more appropriate than ranked lists. Thus we define composite retrieval as the problem of finding k bundles of complementary items. Beyond complementarity of items, the bundles must be valid w.r.t. a given budget, and the answer set of k bundles must exhibit diversity. We formally define the problem and show that in its general form is NP-hard and that also the special cases in which each bundle is formed by only one item, or only one bundle is sought, are hard. Our characterization however suggests how to adopt a two-phase approach (Produce-and-Choose, or PAC) in which we first produce many valid bundles, and then we choose k among them. For the first phase we devise two ad-hoc clustering algorithms, while for the second phase we adapt heuristics with approximation guarantees for a related problem. We also devise another approach which is based on first finding a k-clustering and then selecting a valid bundle from each of the produced clusters (Cluster-and-Pick, or CAP). We compare experimentally the proposed methods on two real-world data sets: the first data set is given by a sample of touristic attractions in 10 large European cities, while the second is a large database of user-generated restaurant reviews from Yahoo! Local. Our experiments show that when diversity is highly important, CAP is the best option, while when diversity is less important, a PAC approach constructing bundles around randomly chosen pivots, is better.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "2722051",
                    "name": "E. Feuerstein"
                },
                {
                    "authorId": "1398937781",
                    "name": "I. M\u00e9ndez-D\u00edaz"
                },
                {
                    "authorId": "144535064",
                    "name": "Paula Zabala"
                }
            ]
        },
        {
            "paperId": "9397d5ae8c7d8119887819067d70107356fe33e6",
            "title": "AIDR: artificial intelligence for disaster response",
            "abstract": "We present AIDR (Artificial Intelligence for Disaster Response), a platform designed to perform automatic classification of crisis-related microblog communications. AIDR enables humans and machines to work together to apply human intelligence to large-scale data at high speed. The objective of AIDR is to classify messages that people post during disasters into a set of user-defined categories of information (e.g., \"needs\", \"damage\", etc.) For this purpose, the system continuously ingests data from Twitter, processes it (i.e., using machine learning classification techniques) and leverages human-participation (through crowdsourcing) in real-time. AIDR has been successfully tested to classify informative vs. non-informative tweets posted during the 2013 Pakistan Earthquake. Overall, we achieved a classification quality (measured using AUC) of 80%. AIDR is available at http://aidr.qcri.org/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "2081346",
                    "name": "J. Lucas"
                },
                {
                    "authorId": "49317293",
                    "name": "P. Meier"
                },
                {
                    "authorId": "3681090",
                    "name": "Sarah Vieweg"
                }
            ]
        },
        {
            "paperId": "a43618ad120c253d8f376cbcaa99ad6364cd1fe2",
            "title": "CrisisLex: A Lexicon for Collecting and Filtering Microblogged Communications in Crises",
            "abstract": "\n \n Locating timely, useful information during crises and mass emergencies is critical for those forced to make potentially life-altering decisions. As the use of Twitter to broadcast useful information during such situations becomes more widespread, the problem of finding it becomes more difficult. We describe an approach toward improving the recall in the sampling of Twitter communications that can lead to greater situational awareness during crisis situations. First, we create a lexicon of crisis-related terms that frequently appear in relevant messages posted during different types of crisis situations. Next, we demonstrate how we use the lexicon to automatically identify new terms that describe a given crisis. Finally, we explain how to efficiently query Twitter to extract crisis-related messages during emergency events. In our experiments, using a crisis lexicon leads to substantial improvements in terms of recall when added to a set of crisis-specific keywords manually chosen by experts; it also helps to preserve the original distribution of message types.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39824354",
                    "name": "Alexandra Olteanu"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "145472333",
                    "name": "Fernando Diaz"
                },
                {
                    "authorId": "3681090",
                    "name": "Sarah Vieweg"
                }
            ]
        },
        {
            "paperId": "c61071de43f54150a7e98c3cb3f3ec7274657bf0",
            "title": "Proceedings of the 7th ACM international conference on Web search and data mining",
            "abstract": "It is our great pleasure to welcome you to the Seventh ACM International Conference on Web Search and Data Mining (WSDM 2014) held on February 24--28, 2014 in New York City, New York, USA. As with previous installments, WSDM attracted many high quality submissions covering a broad spectrum of Web search and data mining topics. WSDM continues be a leading forum for reporting the latest research developments in the field. We are delighted to present here the proceedings of the conference. \n \nWe received a total of 355 submissions from a diverse group of 44 countries and regions, of which 64 were accepted for full paper publication in the proceedings, thus achieving an acceptance rate of 18%. The accepted papers are from 20 different countries and represent a nice mix of academic and industrial research, making this a truly international and diverse forum. Some of the most popular research topics this year include Web search, computational advertising, recommender systems, and social networks. \n \nAs in the past, WSDM 2014 continues to be a single track conference. To accommodate this, 19 papers were chosen to be presented as long presentations, while the remaining 45 will be presented as short presentations. As in the past, all authors of accepted papers were afforded the opportunity to present a poster during the poster session. There were many remarkable papers submitted to the conference. We chose 10 of the most exceptional papers as Best Paper Award candidates.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1750995",
                    "name": "Ben Carterette"
                },
                {
                    "authorId": "145472333",
                    "name": "Fernando Diaz"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "1680617",
                    "name": "Donald Metzler"
                }
            ]
        },
        {
            "paperId": "cd853833424c07ac9a4726f59171803dc217bc90",
            "title": "Processing Social Media Messages in Mass Emergency",
            "abstract": "Social media platforms provide active communication channels during mass convergence and emergency events such as disasters caused by natural hazards. As a result, first responders, decision makers, and the public can use this information to gain insight into the situation as it unfolds. In particular, many social media messages communicated during emergencies convey timely, actionable information. Processing social media messages to obtain such information, however, involves solving multiple challenges including: parsing brief and informal messages, handling information overload, and prioritizing different types of information found in messages. These challenges can be mapped to classical information processing operations such as filtering, classifying, ranking, aggregating, extracting, and summarizing. We survey the state of the art regarding computational methods to process social media messages and highlight both their contributions and shortcomings. In addition, we examine their particularities, and methodically examine a series of key subproblems ranging from the detection of events to the creation of actionable and useful summaries. Research thus far has, to a large extent, produced methods to extract situational awareness information from social media. In this survey, we cover these various approaches, and highlight their benefits and shortcomings. We conclude with research challenges that go beyond situational awareness, and begin to look at supporting decision making and coordinating emergency-response actions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32584303",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "145472333",
                    "name": "Fernando Diaz"
                },
                {
                    "authorId": "3681090",
                    "name": "Sarah Vieweg"
                }
            ]
        },
        {
            "paperId": "cfc1692b7d22e8024c21b356ed2e492a0e9a605f",
            "title": "Volunteer-powered automatic classification of social media messages for public health in AIDR",
            "abstract": "Microblogging platforms such as Twitter have become a valuable resource for disease surveillance and monitoring. Automatic classification can be used to detect disease-related messages and to sort them into meaningful categories. In this paper, we show how the AIDR (Artificial Intelligence for Disaster Response) platform can be used to harvest and perform analysis of tweets in real-time using supervised machine learning techniques. AIDR is a volunteer-powered online social media content classification platform that automatically learns from a set of human-annotated examples to classify tweets into user-defined categories. In addition, it automatically increases classification accuracy as new examples become available. AIDR can be operated through a web interface without the need to deal with the complexity of the machine learning methods used.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32584303",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "d707fd30715d1663545d25a5381d23dced941a79",
            "title": "Coordinating human and machine intelligence to classify microblog communications in crises",
            "abstract": "An emerging paradigm for the processing of data streams involves human and machine computation working together, allowing human intelligence to process large-scale data. We apply this approach to the classification of crisis-related messages in microblog streams. We begin by describing the platform AIDR (Artificial Intelligence for Disaster Response), which collects human annotations over time to create and maintain automatic supervised classifiers for social media messages. Next, we study two significant challenges in its design: (1) identifying which elements must be labeled by humans, and (2) determining when to ask for such annotations to be done. The first challenge is selecting the items to be labeled by crowdsourcing workers to maximize the productivity of their work. The second challenge is to schedule the work in order to reliably maintain high classification accuracy over time. We provide and validate answers to these challenges by extensive experimentation on realworld datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32584303",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "2052982777",
                    "name": "Jesse Lucas"
                },
                {
                    "authorId": "49317293",
                    "name": "P. Meier"
                },
                {
                    "authorId": "3086458",
                    "name": "Jakob Rogstadius"
                }
            ]
        },
        {
            "paperId": "e0d3389abbccf27af87f299152245ec1fe2784e3",
            "title": "Welcome from the program chairs",
            "abstract": "In response to the call for papers, 173 papers were submitted to the conference. These papers were evaluated on the basis of their significance, novelty, technical quality, and practical impact. As in previous years, reviewing was \u201cdouble-blind\u201d: the identities of reviewers were not revealed to the authors of the papers and author identities were not revealed to the reviewers. The program committee meeting was held electronically, yielding intensive discussion over a period of two weeks. Of the papers submitted, 42 were selected for presentation at the conference, giving an acceptance rate under 25%. Besides the technical program composed of these papers, the conference includes invited talks, panels, case studies, panels, and work in progress presentations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "1680617",
                    "name": "Donald Metzler"
                }
            ]
        },
        {
            "paperId": "ea35b2f5428a61504cf7ee3f4c6e3f46ff75c2a3",
            "title": "TweetCred: A Real-time Web-based System for Assessing Credibility of Content on Twitter",
            "abstract": "During large scale events, a large volume of content is posted on Twitter, but not all of this content is trustworthy. The presence of spam, advertisements, rumors and fake images reduces the value of information collected from Twitter, especially during sudden-onset crisis events where information from other sources is scarce. In this research work, we describe various facets of assessing the credibility of usergenerated content on Twitter during large scale events, and develop a novel real-time system to assess the credibility of tweets. Firstly, we develop a semi-supervised ranking model using SVM-rank for assessing credibility, based on training data obtained from six high-impact crisis events of 2013. An extensive set of forty-five features is used to determine the credibility score for each of the tweets. Secondly, we develop and deploy a system\u2013TweetCred\u2013in the form of a browser extension, a web application and an API at the link: http://twitdigest.iiitd.edu.in/TweetCred/. To the best of our knowledge, this is the first research work to develop a practical system for credibility on Twitter and evaluate it with real users. TweetCred was installed and used by 717 Twitter users within a span of three weeks. During this period, a credibility score was computed for more than 1.1 million unique tweets. Thirdly, we evaluated the real-time performance of TweetCred , observing that 84% of the credibility scores were displayed within 6 seconds. We report on the positive feedback that we received from the system\u2019s users and the insights we gained into improving the system for future iterations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109995009",
                    "name": "Aditi Gupta"
                },
                {
                    "authorId": "1734731",
                    "name": "P. Kumaraguru"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "49317293",
                    "name": "P. Meier"
                }
            ]
        },
        {
            "paperId": "16f2e401cba51a93b74043adddacf65f484bae91",
            "title": "Emergency-relief coordination on social media: Automatically matching resource requests and offers",
            "abstract": "Disaster affected communities are increasingly turning to social media for communication and coordination. This includes reports on needs (demands) and offers (supplies) of resources required during emergency situations. Identifying and matching such requests with potential responders can substantially accelerate emergency relief efforts. Current work of disaster management agencies is labor intensive, and there is substantial interest in automated tools.We present machine\u2013learning methods to automatically identify and match needs and offers communicated via social media for items and services such as shelter, money, clothing, etc. For instance, a message such as \u201cwe are coordinating a clothing/food drive for families affected by Hurricane Sandy. If you would like to donate, DM us\u201d can be matched with a message such as \u201cI got a bunch of clothes I\u2019d like to donate to hurricane sandy victims. Anyone know where/how I can do that?\u201d Compared to traditional search, our results can significantly improve the matchmaking efforts of disaster response agencies.",
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "authors": [
                {
                    "authorId": "7147418",
                    "name": "Hemant Purohit"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "145472333",
                    "name": "Fernando Diaz"
                },
                {
                    "authorId": "144463965",
                    "name": "A. Sheth"
                },
                {
                    "authorId": "49317293",
                    "name": "P. Meier"
                }
            ]
        },
        {
            "paperId": "205f1447900f1e69cb144a4cc1a9349ec905df55",
            "title": "Democratizing mobile app development for disaster management",
            "abstract": "Smartphones are being used for a wide range of activities including messaging, social networking, calendar and contact management as well as location and context-aware applications. The ubiquity of handheld computing technology has been found to be especially useful in disaster management and relief operations. Our focus is to enable developers to quickly deploy applications that take advantage of key sources that are fundamental for today's networked citizens, including Twitter feeds, Facebook posts, current news releases, and government data. These applications will also have the capability of empowering citizens involved in crisis situations to contribute via crowdsourcing, and to communicate up-to-date information to others. We will leverage several technologies to develop this application framework, namely (i) Linked Data principles for structured data, (ii) existing data sources and ontologies for disaster management, and (iii) App Inventor, which is a mobile application development framework for non-programmers. In this paper, we describe our motivating use cases, our architecture, and our prototype implementation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4007305",
                    "name": "Fuming Shih"
                },
                {
                    "authorId": "1693195",
                    "name": "O. Seneviratne"
                },
                {
                    "authorId": "2259350",
                    "name": "Ilaria Liccardi"
                },
                {
                    "authorId": "2538510",
                    "name": "E. Patton"
                },
                {
                    "authorId": "49317293",
                    "name": "P. Meier"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "212e81b65a743042a45defb555e9c452e2155b27",
            "title": "The role of information diffusion in the evolution of social networks",
            "abstract": "Every day millions of users are connected through online social networks, generating a rich trove of data that allows us to study the mechanisms behind human interactions. Triadic closure has been treated as the major mechanism for creating social links: if Alice follows Bob and Bob follows Charlie, Alice will follow Charlie. Here we present an analysis of longitudinal micro-blogging data, revealing a more nuanced view of the strategies employed by users when expanding their social circles. While the network structure affects the spread of information among users, the network is in turn shaped by this communication activity. This suggests a link creation mechanism whereby Alice is more likely to follow Charlie after seeing many messages by Charlie. We characterize users with a set of parameters associated with different link creation strategies, estimated by a Maximum-Likelihood approach. Triadic closure does have a strong effect on link formation, but shortcuts based on traffic are another key factor in interpreting network evolution. However, individual strategies for following other users are highly heterogeneous. Link creation behaviors can be summarized by classifying users in different categories with distinct structural and behavioral characteristics. Users who are popular, active, and influential tend to create traffic-based shortcuts, making the information diffusion process more efficient in the network.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2617057",
                    "name": "L. Weng"
                },
                {
                    "authorId": "2951268",
                    "name": "Jacob Ratkiewicz"
                },
                {
                    "authorId": "1826736",
                    "name": "N. Perra"
                },
                {
                    "authorId": "2054864984",
                    "name": "B. Gon\u00e7alves"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "2251027",
                    "name": "Rossano Schifanella"
                },
                {
                    "authorId": "143653472",
                    "name": "F. Menczer"
                },
                {
                    "authorId": "1769960",
                    "name": "A. Flammini"
                }
            ]
        },
        {
            "paperId": "5965bd463a83be9578f448b2c166f64ae7692842",
            "title": "Online matching of web content to closed captions in IntoNow",
            "abstract": "IntoNow is a mobile application that provides a second-screen experience to television viewers. IntoNow uses the microphone of the companion device to sample the audio coming from the TV set, and compares it against a database of TV shows in order to identify the program being watched. The system we demonstrate is activated by IntoNow for specific types of shows. It retrieves information related to the program the user is watching by using closed captions, which are provided by each broadcasting network along the TV signal. It then matches the stream of closed captions in real-time against multiple sources of content. More specifically, during news programs it displays links to online news articles and the profiles of people and organizations in the news, and during music shows it displays links to songs. The matching models are machine-learned from editorial judgments, and tuned to achieve approximately 90% precision.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                },
                {
                    "authorId": "2352710",
                    "name": "Ajay Shekhawat"
                }
            ]
        },
        {
            "paperId": "616057e842be77d6e111ffa96d57130ffd79523f",
            "title": "Traffic prediction and discovery of news via news crowds",
            "abstract": "We consider the users who share a specific online news article on a social media platform. This group is the news crowd of the article, an analogy with the crowd of passersby that gathers around any incident on a busy street. First, we look at the volume and the diversity of the social media conversations of the crowd around the given news item. We observe that there are strikingly different patterns that separate \u201chard\u201d news from editorial/opinion articles. The statistics we extract from social media can help produce improved predictions of the effective life of a news article on the web, and of the total number of visits it will receive in the future [1]. The improvement is more pronounced in the case of opinion/editorial pieces which are less predictable and longer-lived.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "67c53c2a8dc3db0537c27d4977a1ebe141923ccb",
            "title": "Measuring and Summarizing Movement in Microblog Postings",
            "abstract": "\n \n Every day, users publish hundreds of millions of microblog postings in popular social-networking platforms such as Twitter and Facebook. When considered in aggregation, microblog postings have been shown to exhibit temporal patterns that reflect events of global significance. In this paper, we propose techniques to identify and quantify spatial patterns: for instance, a hashtag that is popular in one city on a given day, may become popular in a different city on the next day. Detecting these patterns is challenging given that the data are noisy and posts are not physically moving, i.e., they are not continuous trajectories in space like vehicles. Second, we introduce a multi-granular summarization model to describe the movement of a hashtag between two time periods. For interpretability, we seek representations of spatial changes that follow natural or administrative boundaries on a map, such as cities and states. We compare various movement measures using quantitative approaches and user surveys. We evaluate our movement summarization schemes by analytical loss and coverage functions. Our results show that it is possible to reliably detect relevant spatial changes automatically, and to produce simple summaries that represent accurately these changes.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144795428",
                    "name": "Eduardo J. Ruiz"
                },
                {
                    "authorId": "1754970",
                    "name": "Vagelis Hristidis"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                }
            ]
        },
        {
            "paperId": "78ed4789c9081ef4bf55c35074955615a2ab8225",
            "title": "Characterizing the life cycle of online news stories using social media reactions",
            "abstract": "This paper presents a study of the life cycle of news articles posted online. We describe the interplay between website visitation patterns and social media reactions to news content. We show that we can use this hybrid observation method to characterize distinct classes of articles. We also find that social media reactions can help predict future visitation patterns early and accurately. We validate our methods using qualitative analysis as well as quantitative analysis on data from a large international news network, for a set of articles generating more than 3,000,000 visits and 200,000 social media reactions. We show that it is possible to model accurately the overall traffic articles will ultimately receive by observing the first ten to twenty minutes of social media reactions. Achieving the same prediction accuracy with visits alone would require to wait for three hours of data. We also describe significant improvements on the accuracy of the early prediction of shelf-life for news stories.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "102697193",
                    "name": "Mohammed El-Haddad"
                },
                {
                    "authorId": "145931035",
                    "name": "J. Pfeffer"
                },
                {
                    "authorId": "3147623",
                    "name": "Matt Stempeck"
                }
            ]
        },
        {
            "paperId": "92aec6fa5754b503b53158ff304cdcd6ce1c7792",
            "title": "Complexity and algorithms for composite retrieval",
            "abstract": "Online search has become a daily activity and a source of a variety of valuable information, from the finest granularity such as finding the address of a specific restaurant, to more complex tasks like looking for accessories compatible with an iPhone or planning a trip. The latter typically involves running multiple search queries to gather information about different places, reading online reviews to find out about hotels, and checking geographic proximity of places to visit. We refer to this information seeking activity as composite retrieval and propose to organize results into item bundles that together constitute an improved exploratory experience over ranked lists. As a first step towards composite retrieval definition, we need to formalize intuitive desirable properties of item bundles. We distinguish between properties of each bundle in the answer and properties of the answer as a whole. Consider the case of a user selecting the restaurants to try during a visit to a new city. The user has a limited budget which might be either financial, or simply the number of nights spent in the city. The user prefers suggested restaurants to serve different cuisines. The validity of a bundle of restaurants is given by the budget constraint and the complementarity of the restaurants in the bundle w.r.t. the cuisine they serve. Other restaurant attributes could be used for defining valid bundles. For example, instead of cuisines, different dress codes could be required to every restaurant in a single bundle. Moreover, in order to provide meaningful bundles, restaurants forming each bundle must be compatible, e.g., close geographically, or liked by similar reviewers. The degree of compatibility of the items forming a bundle defines the quality of the bundle. Intuitively, in the case geographic distance is used, the closer restaurants are from each other, the higher the quality of the bundle they belong to. Similarly, when common reviewers are used as the",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "2722051",
                    "name": "E. Feuerstein"
                },
                {
                    "authorId": "1398937781",
                    "name": "I. M\u00e9ndez-D\u00edaz"
                },
                {
                    "authorId": "144535064",
                    "name": "Paula Zabala"
                }
            ]
        },
        {
            "paperId": "9b48a821d1c0b3da9c93579f614ad6557733d737",
            "title": "Information verification during natural disasters",
            "abstract": "Large amounts of unverified and at times contradictory information often appear on social media following natural disasters. Timely verification of this information can be crucial to saving lives and for coordinating relief efforts. Our goal is to enable this verification by developing an online platform that involves ordinary citizens in the evidence gathering and evaluation process. The output of this platform will provide reliable information to humanitarian organizations, journalists, and decision makers involved in relief efforts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144321516",
                    "name": "Abdulfatai Popoola"
                },
                {
                    "authorId": "2221005",
                    "name": "Dmytro Krasnoshtan"
                },
                {
                    "authorId": "50583570",
                    "name": "Attila T\u00f3th"
                },
                {
                    "authorId": "2149740",
                    "name": "V. Naroditskiy"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "49317293",
                    "name": "P. Meier"
                },
                {
                    "authorId": "1705156",
                    "name": "Iyad Rahwan"
                }
            ]
        },
        {
            "paperId": "a7b120741a479699e07ca4997877a61337e713c0",
            "title": "Social media news communities: gatekeeping, coverage, and statement bias",
            "abstract": "We examine biases in online news sources and social media communities around them. To that end, we introduce unsupervised methods considering three types of biases: selection or ``gatekeeping'' bias, coverage bias, and statement bias, characterizing each one through a series of metrics. Our results, obtained by analyzing 80 international news sources during a two-week period, show that biases are subtle but observable, and follow geographical boundaries more closely than political ones. We also demonstrate how these biases are to some extent amplified by social media.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388387493",
                    "name": "Diego S\u00e1ez-Trumper"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "1684032",
                    "name": "M. Lalmas"
                }
            ]
        },
        {
            "paperId": "adc71ce510bc6eaf309cddce37409fee6a56e6c0",
            "title": "Finding news curators in twitter",
            "abstract": "Users interact with online news in many ways, one of them being sharing content through online social networking sites such as Twitter. There is a small but important group of users that devote a substantial amount of effort and care to this activity. These users monitor a large variety of sources on a topic or around a story, carefully select interesting material on this topic, and disseminate it to an interested audience ranging from thousands to millions. These users are news curators, and are the main subject of study of this paper. We adopt the perspective of a journalist or news editor who wants to discover news curators among the audience engaged with a news site. We look at the users who shared a news story on Twitter and attempt to identify news curators who may provide more information related to that story. In this paper we describe how to find this specific class of curators, which we refer to as news story curators. Hence, we proceed to compute a set of features for each user, and demonstrate that they can be used to automatically find relevant curators among the audience of two large news organizations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "16598985",
                    "name": "Janette Lehmann"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "1684032",
                    "name": "M. Lalmas"
                },
                {
                    "authorId": "2214843",
                    "name": "Ethan Zuckerman"
                }
            ]
        },
        {
            "paperId": "ba8977eb5d737d643f38d84d0c1476211bb88986",
            "title": "Extracting information nuggets from disaster- Related messages in social media",
            "abstract": "Microblogging sites such as Twitter can play a vital role in spreading information during \u201cnatural\u201d or man-made disasters. But the volume and velocity of tweets posted during crises today tend to be extremely high, making it hard for disaster-affected communities and professional emergency responders to process the information in a timely manner. Furthermore, posts tend to vary highly in terms of their subjects and usefulness; from messages that are entirely off-topic or personal in nature, to messages containing critical information that augments situational awareness. Finding actionable information can accelerate disaster response and alleviate both property and human losses. In this paper, we describe automatic methods for extracting information from microblog posts. Specifically, we focus on extracting valuable \u201cinformation nuggets\u201d, brief, self-contained information items relevant to disaster response. Our methods leverage machine learning methods for classifying posts and information extraction. Our results, validated over one large disaster-related dataset, reveal that a careful design can yield an effective system, paving the way for more sophisticated data analysis and visualization systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32584303",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "145472333",
                    "name": "Fernando Diaz"
                },
                {
                    "authorId": "49317293",
                    "name": "P. Meier"
                }
            ]
        },
        {
            "paperId": "c4412f81b3834b71d11292b19d0b3cb125df0df6",
            "title": "Predicting information credibility in time-sensitive social media",
            "abstract": "Purpose \u2013 Twitter is a popular microblogging service which has proven, in recent years, its potential for propagating news and information about developing events. The purpose of this paper is to focus on the analysis of information credibility on Twitter. The purpose of our research is to establish if an automatic discovery process of relevant and credible news events can be achieved. Design/methodology/approach \u2013 The paper follows a supervised learning approach for the task of automatic classification of credible news events. A first classifier decides if an information cascade corresponds to a newsworthy event. Then a second classifier decides if this cascade can be considered credible or not. The paper undertakes this effort training over a significant amount of labeled data, obtained using crowdsourcing tools. The paper validates these classifiers under two settings: the first, a sample of automatically detected Twitter \u201ctrends\u201d in English, and second, the paper tests how well this model transfers to...",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "145893609",
                    "name": "Marcelo Mendoza"
                },
                {
                    "authorId": "2272762",
                    "name": "B\u00e1rbara Poblete"
                }
            ]
        },
        {
            "paperId": "c552f6ac4065980a31b27c977b9103c3c5f947c8",
            "title": "Says who?: automatic text-based content analysis of television news",
            "abstract": "We perform an automatic analysis of television news programs, based on the closed captions that accompany them. Specifically, we collect all the news broadcasted in over 140 television channels in the US during a period of six months. We start by segmenting, processing, and annotating the closed captions automatically. Next, we focus on the analysis of their linguistic style and on mentions of people using NLP methods. We present a series of key insights about news providers, people in the news, and we discuss the biases that can be uncovered by automatic means. These insights are contrasted by looking at the data from multiple points of view, including qualitative assessment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                },
                {
                    "authorId": "145893609",
                    "name": "Marcelo Mendoza"
                },
                {
                    "authorId": "2093291948",
                    "name": "Nasir Khan"
                }
            ]
        },
        {
            "paperId": "d0d60c2bfca32b3e6d8feee815a3a132a2f47499",
            "title": "Practical extraction of disaster-relevant information from social media",
            "abstract": "During times of disasters online users generate a significant amount of data, some of which are extremely valuable for relief efforts. In this paper, we study the nature of social-media content generated during two different natural disasters. We also train a model based on conditional random fields to extract valuable information from such content. We evaluate our techniques over our two datasets through a set of carefully designed experiments. We also test our methods over a non-disaster dataset to show that our extraction model is useful for extracting information from socially-generated content in general.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "145472333",
                    "name": "Fernando Diaz"
                },
                {
                    "authorId": "49317293",
                    "name": "P. Meier"
                }
            ]
        },
        {
            "paperId": "ef54bf37d60f37f07c958783fc56021311205d5c",
            "title": "Tweet4act: Using incident-specific profiles for classifying crisis-related messages",
            "abstract": "We present Tweet4act, a system to detect and classify crisis-related messages communicated over a microblogging platform. Our system relies on extracting content features from each message. These features and the use of an incident-specific dictionary allow us to determine the period type of an incident that each message belongs to. The period types are: pre-incident (messages talking about prevention, mitigation, and preparedness), during-incident (messages sent while the incident is taking place), and post-incident (messages related to the response, recovery, and reconstruction). We show that our detection method can effectively identify incident-related messages with high precision and recall, and that our incident-period classification method outperforms standard machine learning classification methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37884573",
                    "name": "Soudip Roy Chowdhury"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "35065376",
                    "name": "M. R. Asghar"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "f2d572733ad8478b55911319f75a928537191008",
            "title": "Transient News Crowds in Social Media",
            "abstract": "\n \n Users increasingly inform themselves of the latest news through online news services. This is further accentuated by the increasingly seamless integration of social network platforms such as Twitter and Facebook into news websites, allowing easy content sharing and distribution. This makes online social network platforms of particular interest to news providers. For instance, online news producers use Twitter to disseminate articles published on their website, to assess the popularity of their contents, but also as an information source to be used on itself. In this paper, we focus on Twitter as a medium to help journalists and news editors rapidly detect follow-up stories to the articles they publish. We propose to do so by leveraging \u201ctransient news crowds\u201d, which are loosely-coupled groups that appear in Twitter around a particular news item, and where transient here reflects the fleeting nature of news. We define transient news crowds on Twitter, study their characteristics, and investigate how their characteristics can be used to discover related news. We validate our approach using Twitter data around news stories published by the BBC and Al Jazeera.\n \n",
            "fieldsOfStudy": [
                "Business",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "16598985",
                    "name": "Janette Lehmann"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "1684032",
                    "name": "M. Lalmas"
                },
                {
                    "authorId": "2214843",
                    "name": "Ethan Zuckerman"
                }
            ]
        },
        {
            "paperId": "0a804919f0ef6f8eb4d18baab8255b07cb4ff713",
            "title": "Emotions and dialogue in a peer-production community: the case of Wikipedia",
            "abstract": "This paper presents a large-scale analysis of emotions in conversations among Wikipedia editors. Our focus is on the emotions expressed by editors in talk pages, measured by using the Affective Norms for English Words (ANEW).\n We find evidence that to a large extent women tend to participate in discussions with a more positive tone, and that administrators are more positive than non-administrators. Surprisingly, female non-administrators tend to behave like administrators in many aspects.\n We observe that replies are on average more positive than the comments they reply to, preventing many discussions from spiralling down into conflict. We also find evidence of emotional homophily: editors having similar emotional styles are more likely to interact with each other.\n Our findings offer novel insights into the emotional dimension of interactions in peer-production communities, and contribute to debates on issues such as the flattening of editor growth and the gender gap.",
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3203785",
                    "name": "David Laniado"
                },
                {
                    "authorId": "1889171",
                    "name": "Andreas Kaltenbrunner"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "3177219",
                    "name": "M. F. Morell"
                }
            ]
        },
        {
            "paperId": "12d2219c530c519f35d458ff9d8983ab68f13011",
            "title": "A data-driven sketch of Wikipedia editors",
            "abstract": "Who edits Wikipedia? We attempt to shed light on this question by using aggregated log data from Yahoo!'s browser toolbar in order to analyze Wikipedians' editing behavior in the context of their online lives beyond Wikipedia. We broadly characterize editors by investigating how their online behavior differs from that of other users; e.g., we find that Wikipedia editors search more, read more news, play more games, and, perhaps surprisingly, are more immersed in pop culture. Then we inspect how editors' general interests relate to the articles to which they contribute; e.g., we confirm the intuition that editors show more expertise in their active domains than average users. Our results are relevant as they illuminate novel aspects of what has become many Web users' prevalent source of information and can help in recruiting new editors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "96502442",
                    "name": "Robert West"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "6c825d293389e5eb4ed2d6899093ee4d2e1902b7",
            "title": "Drawing a data-driven portrait of Wikipedia editors",
            "abstract": "While there has been a substantial amount of research into the editorial and organizational processes within Wikipedia, little is known about how Wikipedia editors (Wikipedians) relate to the online world in general. We attempt to shed light on this issue by using aggregated log data from Yahoo!'s browser toolbar in order to analyze Wikipedians' editing behavior in the context of their online lives beyond Wikipedia. We broadly characterize editors by investigating how their online behavior differs from that of other users; e.g., we find that Wikipedia editors search more, read more news, play more games, and, perhaps surprisingly, are more immersed in popular culture. Then we inspect how editors' general interests relate to the articles to which they contribute; e.g., we confirm the intuition that editors are more familiar with their active domains than average users. Finally, we analyze the data from a temporal perspective; e.g., we demonstrate that a user's interest in the edited topic peaks immediately before the edit. Our results are relevant as they illuminate novel aspects of what has become many Web users' prevalent source of information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145781376",
                    "name": "Robert West"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "53295bc919e02289b78918b07d1252b3e4c5e3ca",
            "title": "Information credibility on twitter",
            "abstract": "We analyze the information credibility of news propagated through Twitter, a popular microblogging service. Previous research has shown that most of the messages posted on Twitter are truthful, but the service is also used to spread misinformation and false rumors, often unintentionally.\n On this paper we focus on automatic methods for assessing the credibility of a given set of tweets. Specifically, we analyze microblog postings related to \"trending\" topics, and classify them as credible or not credible, based on features extracted from them. We use features from users' posting and re-posting (\"re-tweeting\") behavior, from the text of the posts, and from citations to external sources.\n We evaluate our methods using a significant number of human assessments about the credibility of items on a recent sample of Twitter postings. Our results shows that there are measurable differences in the way messages propagate, that can be used to classify them automatically as credible or not credible, with precision and recall in the range of 70% to 80%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "145893609",
                    "name": "Marcelo Mendoza"
                },
                {
                    "authorId": "2272762",
                    "name": "B\u00e1rbara Poblete"
                }
            ]
        },
        {
            "paperId": "2141a4655f65c09dce412bc9125d960b453ed64d",
            "title": "Twitter under crisis: can we trust what we RT?",
            "abstract": "In this article we explore the behavior of Twitter users under an emergency situation. In particular, we analyze the activity related to the 2010 earthquake in Chile and characterize Twitter in the hours and days following this disaster. Furthermore, we perform a preliminary study of certain social phenomenons, such as the dissemination of false rumors and confirmed news. We analyze how this information propagated through the Twitter network, with the purpose of assessing the reliability of Twitter as an information source under extreme circumstances. Our analysis shows that the propagation of tweets that correspond to rumors differs from tweets that spread news because rumors tend to be questioned more than news by the Twitter community. This result shows that it is posible to detect rumors by using aggregate analysis on tweets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "authors": [
                {
                    "authorId": "145893609",
                    "name": "Marcelo Mendoza"
                },
                {
                    "authorId": "2272762",
                    "name": "B\u00e1rbara Poblete"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        },
        {
            "paperId": "4a5ef1eadeed08352e0b20f6aa5c4a1dbee7d550",
            "title": "An Analysis of Factors Used in Search Engine Ranking",
            "abstract": "This paper investigates the influence of different page features on the ranking of search engine results. We use Google (via its API) as our testbed and analyze the result rankings for several queries of different categories using statistical methods. We reformulate the problem of learning the underlying, hidden scores as a binary classification problem. To this problem we then apply both linear and non-linear methods. In all cases, we split the data into a training set and a test set to obtain a meaningful, unbiased estimator for the quality of our predictor. Although our results clearly show that the scoring function cannot be approximated well using only the observed features, we do obtain many interesting insights along the way and discuss ways of obtaining a better estimate and main limitations in trying to do so.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1762931",
                    "name": "A. Bifet"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "2552614",
                    "name": "P. Chirita"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                }
            ]
        }
    ]
}