{
    "authorId": "2258077708",
    "papers": [
        {
            "paperId": "0f01e48b322659e017faeaa76211c209bd15b108",
            "title": "MMSci: A Multimodal Multi-Discipline Dataset for PhD-Level Scientific Comprehension",
            "abstract": "The rapid advancement of Large Language Models (LLMs) and Large Multimodal Models (LMMs) has heightened the demand for AI-based scientific assistants capable of understanding scientific articles and figures. Despite progress, there remains a significant gap in evaluating models' comprehension of professional, graduate-level, and even PhD-level scientific content. Current datasets and benchmarks primarily focus on relatively simple scientific tasks and figures, lacking comprehensive assessments across diverse advanced scientific disciplines. To bridge this gap, we collected a multimodal, multidisciplinary dataset from open-access scientific articles published in Nature Communications journals. This dataset spans 72 scientific disciplines, ensuring both diversity and quality. We created benchmarks with various tasks and settings to comprehensively evaluate LMMs' capabilities in understanding scientific figures and content. Our evaluation revealed that these tasks are highly challenging: many open-source models struggled significantly, and even GPT-4V and GPT-4o faced difficulties. We also explored using our dataset as training resources by constructing visual instruction-following data, enabling the 7B LLaVA model to achieve performance comparable to GPT-4V/o on our benchmark. Additionally, we investigated the use of our interleaved article texts and figure images for pre-training LMMs, resulting in improvements on the material generation task. The source dataset, including articles, figures, constructed benchmarks, and visual instruction-following data, is open-sourced.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2261357619",
                    "name": "Xianjun Yang"
                },
                {
                    "authorId": "2312170621",
                    "name": "Kyuri Choi"
                },
                {
                    "authorId": "51439692",
                    "name": "Wanrong Zhu"
                },
                {
                    "authorId": "2310337940",
                    "name": "Ryan Hsieh"
                },
                {
                    "authorId": "2310433603",
                    "name": "HyeonJung Kim"
                },
                {
                    "authorId": "2310394887",
                    "name": "Jin Hyuk Lim"
                },
                {
                    "authorId": "2310418771",
                    "name": "Sungyoung Ji"
                },
                {
                    "authorId": "2310587270",
                    "name": "Byungju Lee"
                },
                {
                    "authorId": "2258077708",
                    "name": "Xifeng Yan"
                },
                {
                    "authorId": "2253654665",
                    "name": "L. Petzold"
                },
                {
                    "authorId": "2310407665",
                    "name": "Stephen D. Wilson"
                },
                {
                    "authorId": "2310341461",
                    "name": "Woosang Lim"
                },
                {
                    "authorId": "2297934952",
                    "name": "William Yang Wang"
                }
            ]
        },
        {
            "paperId": "54654d54f0be9e4a5a2e443068940a12aee6b7ce",
            "title": "Can Editing LLMs Inject Harm?",
            "abstract": "Knowledge editing has been increasingly adopted to correct the false or outdated knowledge in Large Language Models (LLMs). Meanwhile, one critical but under-explored question is: can knowledge editing be used to inject harm into LLMs? In this paper, we propose to reformulate knowledge editing as a new type of safety threat for LLMs, namely Editing Attack, and conduct a systematic investigation with a newly constructed dataset EditAttack. Specifically, we focus on two typical safety risks of Editing Attack including Misinformation Injection and Bias Injection. For the risk of misinformation injection, we first categorize it into commonsense misinformation injection and long-tail misinformation injection. Then, we find that editing attacks can inject both types of misinformation into LLMs, and the effectiveness is particularly high for commonsense misinformation injection. For the risk of bias injection, we discover that not only can biased sentences be injected into LLMs with high effectiveness, but also one single biased sentence injection can cause a bias increase in general outputs of LLMs, which are even highly irrelevant to the injected sentence, indicating a catastrophic impact on the overall fairness of LLMs. Then, we further illustrate the high stealthiness of editing attacks, measured by their impact on the general knowledge and reasoning capacities of LLMs, and show the hardness of defending editing attacks with empirical evidence. Our discoveries demonstrate the emerging misuse risks of knowledge editing techniques on compromising the safety alignment of LLMs and the feasibility of disseminating misinformation or bias with LLMs as new channels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2163546329",
                    "name": "Canyu Chen"
                },
                {
                    "authorId": "51164501",
                    "name": "Baixiang Huang"
                },
                {
                    "authorId": "2306133127",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2284854420",
                    "name": "Zhaorun Chen"
                },
                {
                    "authorId": "2284765696",
                    "name": "Shiyang Lai"
                },
                {
                    "authorId": "2140517981",
                    "name": "Xiongxiao Xu"
                },
                {
                    "authorId": "2313778918",
                    "name": "Jia-Chen Gu"
                },
                {
                    "authorId": "2261454773",
                    "name": "Jindong Gu"
                },
                {
                    "authorId": "2284759871",
                    "name": "Huaxiu Yao"
                },
                {
                    "authorId": "2312102403",
                    "name": "Chaowei Xiao"
                },
                {
                    "authorId": "2258077708",
                    "name": "Xifeng Yan"
                },
                {
                    "authorId": "2281072607",
                    "name": "William Wang"
                },
                {
                    "authorId": "2257347697",
                    "name": "Philip Torr"
                },
                {
                    "authorId": "2262420606",
                    "name": "Dawn Song"
                },
                {
                    "authorId": "2266239316",
                    "name": "Kai Shu"
                }
            ]
        },
        {
            "paperId": "723a6340ee641e190b22bb47455d05e3b4237179",
            "title": "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling",
            "abstract": "Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% average joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We have made the code publicly available at https://github.com/facebookresearch/FnCTOD",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2284639678",
                    "name": "Zhiyu Zoey Chen"
                },
                {
                    "authorId": "2284594951",
                    "name": "Mike Ross"
                },
                {
                    "authorId": "2287921432",
                    "name": "Patrick Huber"
                },
                {
                    "authorId": "2256132624",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2146396528",
                    "name": "Zhaojiang Lin"
                },
                {
                    "authorId": "2215596266",
                    "name": "Xin Luna Dong"
                },
                {
                    "authorId": "2063995456",
                    "name": "Adithya Sagar"
                },
                {
                    "authorId": "2258077708",
                    "name": "Xifeng Yan"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                }
            ]
        },
        {
            "paperId": "c060976628a0a27489f2c1268818eac5991ad52f",
            "title": "Do you really follow me? Adversarial Instructions for Evaluating the Robustness of Large Language Models",
            "abstract": "Large Language Models (LLMs) have shown remarkable proficiency in following instructions, making them valuable in customer-facing applications. However, their impressive capabilities also raise concerns about the amplification of risks posed by adversarial instructions, which can be injected into the model input by third-party attackers to manipulate LLMs\u2019 original instructions and prompt unintended actions and content. Therefore, it is crucial to understand LLMs\u2019 ability to accurately discern which instructions to follow to ensure their safe deployment in real-world scenarios. In this paper, we propose a pioneering benchmark for automatically evaluating the robustness of LLMs against adversarial instructions. The objective of this benchmark is to quantify the extent to which LLMs are influenced by injected adversarial instructions and assess their ability to differentiate between these adversarial instructions and original user instructions. Through experiments conducted with state-of-the-art instruction-following LLMs, we uncover significant limitations in their robustness against adversarial instruction attacks. Furthermore, our findings indicate that prevalent instruction-tuned models are prone to being \u201coverfitted\u201d to follow any instruction phrase in the prompt without truly understanding which instructions should be followed. This highlights the need to address the challenge of training models to comprehend prompts instead of merely following instruction phrases and completing the text.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2257158371",
                    "name": "Baolin Peng"
                },
                {
                    "authorId": "2257269680",
                    "name": "Pengcheng He"
                },
                {
                    "authorId": "2258077708",
                    "name": "Xifeng Yan"
                }
            ]
        }
    ]
}