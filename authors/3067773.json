{
    "authorId": "3067773",
    "papers": [
        {
            "paperId": "190a0a282ae0d8f06a9f3b57300ae8af511eb5bf",
            "title": "Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy",
            "abstract": "Document retrieval has greatly benefited from the advancements of large-scale pre-trained language models (PLMs). However, their effectiveness is often limited in theme-specific applications for specialized areas or industries, due to unique terminologies, incomplete contexts of user queries, and specialized search intents. To capture the theme-specific information and improve retrieval, we propose to use a corpus topical taxonomy, which outlines the latent topic structure of the corpus while reflecting user-interested aspects. We introduce ToTER (Topical Taxonomy Enhanced Retrieval) framework, which identifies the central topics of queries and documents with the guidance of the taxonomy, and exploits their topical relatedness to supplement missing contexts. As a plug-and-play framework, ToTER can be flexibly employed to enhance various PLM-based retrievers. Through extensive quantitative, ablative, and exploratory experiments on two real-world datasets, we ascertain the benefits of using topical taxonomy for retrieval in theme-specific applications and demonstrate the effectiveness of ToTER.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "71965979",
                    "name": "SeongKu Kang"
                },
                {
                    "authorId": "2290186454",
                    "name": "Shivam Agarwal"
                },
                {
                    "authorId": "2057050247",
                    "name": "Bowen Jin"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "2286165699",
                    "name": "Hwanjo Yu"
                },
                {
                    "authorId": "2257136881",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "1ff7f2b8ed837c8bc64abcadc03e9921c18b9d1e",
            "title": "Learning Discriminative Dynamics with Label Corruption for Noisy Label Detection",
            "abstract": "Label noise, commonly found in real-world datasets, has a detrimental impact on a model's generalization. To effectively detect incorrectly labeled instances, previous works have mostly relied on distinguishable training signals, such as training loss, as indicators to differentiate between clean and noisy labels. However, they have limitations in that the training signals incompletely reveal the model's behavior and are not effectively generalized to various noise types, resulting in limited detection accuracy. In this paper, we propose DynaCor framework that distinguishes incorrectly labeled instances from correctly labeled ones based on the dynamics of the training signals. To cope with the absence of supervision for clean and noisy labels, DynaCor first in-troduces a label corruption strategy that augments the original dataset with intentionally corrupted labels, enabling indirect simulation of the model's behavior on noisy labels. Then, DynaCor learns to identify clean and noisy instances by inducing two clearly distinguishable clusters from the latent representations of training dynamics. Our compre-hensive experiments show that DynaCor outperforms the state-of-the-art competitors and shows strong robustness to various noise types and noise rates.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2303885004",
                    "name": "Suyeon Kim"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "71965979",
                    "name": "SeongKu Kang"
                },
                {
                    "authorId": "2303843148",
                    "name": "Sukang Chae"
                },
                {
                    "authorId": "2287988207",
                    "name": "Sanghwan Jang"
                },
                {
                    "authorId": "2286165699",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "240ac1cc47a00d2c4fe62e228c1fb0dfe85b0c39",
            "title": "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation",
            "abstract": "We study the code generation behavior of instruction-tuned models built on top of code pre-trained language models when they could access an auxiliary function to implement a function. We design several ways to provide auxiliary functions to the models by adding them to the query or providing a response prefix to incorporate the ability to utilize auxiliary functions with the instruction-following capability. Our experimental results show the effectiveness of combining the base models' auxiliary function utilization ability with the instruction following ability. In particular, the performance of adopting our approaches with the open-sourced language models surpasses that of the recent powerful proprietary language models, i.e., gpt-4o.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287842444",
                    "name": "Seonghyeon Lee"
                },
                {
                    "authorId": "2303885004",
                    "name": "Suyeon Kim"
                },
                {
                    "authorId": "2291705807",
                    "name": "Joonwon Jang"
                },
                {
                    "authorId": "2317010068",
                    "name": "Heejae Chon"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "2286165699",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "4fd714b70648e39eb2946e45d2e22a9b8e82764f",
            "title": "Multi-Domain Recommendation to Attract Users via Domain Preference Modeling",
            "abstract": "Recently, web platforms are operating various service domains simultaneously. Targeting a platform that operates multiple service domains, we introduce a new task, Multi-Domain Recommendation to Attract Users (MDRAU), which recommends items from multiple ``unseen'' domains with which each user has not interacted yet, by using knowledge from the user's ``seen'' domains. In this paper, we point out two challenges of MDRAU task. First, there are numerous possible combinations of mappings from seen to unseen domains because users have usually interacted with a different subset of service domains. Second, a user might have different preference for each of the target unseen domains, which requires recommendations to reflect users' preference on domains as well as items. To tackle these challenges, we propose DRIP framework that models users' preference at two levels (i.e., domain and item) and learns various seen-unseen domain mappings in a unified way with masked domain modeling. Our extensive experiments demonstrate the effectiveness of DRIP in MDRAU task and its ability to capture users' domain-level preferences.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293396333",
                    "name": "Hyuunjun Ju"
                },
                {
                    "authorId": "71965979",
                    "name": "SeongKu Kang"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "98573135",
                    "name": "Junyoung Hwang"
                },
                {
                    "authorId": "2287988207",
                    "name": "Sanghwan Jang"
                },
                {
                    "authorId": "2286165699",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "6b9699058152912efedd9a6d724ec08e0c4c9319",
            "title": "Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset",
            "abstract": "Conversational recommender system is an emerging area that has garnered an increasing interest in the community, especially with the advancements in large language models (LLMs) that enable diverse reasoning over conversational input. Despite the progress, the field has many aspects left to explore. The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations. To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators. We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues. Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide recommendations more relevant to the dialogue context than those in prior datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117956135",
                    "name": "Minjin Kim"
                },
                {
                    "authorId": "2110104000",
                    "name": "Minju Kim"
                },
                {
                    "authorId": "2281098420",
                    "name": "Hana Kim"
                },
                {
                    "authorId": "2151245501",
                    "name": "Beong-woo Kwak"
                },
                {
                    "authorId": "2290185769",
                    "name": "Soyeon Chun"
                },
                {
                    "authorId": "2290213007",
                    "name": "Hyunseo Kim"
                },
                {
                    "authorId": "71965979",
                    "name": "SeongKu Kang"
                },
                {
                    "authorId": "2258802492",
                    "name": "Youngjae Yu"
                },
                {
                    "authorId": "2258712913",
                    "name": "Jinyoung Yeo"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                }
            ]
        },
        {
            "paperId": "87ec5cd167e972b3352d06eada73ef2acb1b7e74",
            "title": "Exploring Language Model's Code Generation Ability with Auxiliary Functions",
            "abstract": "Auxiliary function is a helpful component to improve language model's code generation ability. However, a systematic exploration of how they affect has yet to be done. In this work, we comprehensively evaluate the ability to utilize auxiliary functions encoded in recent code-pretrained language models. First, we construct a human-crafted evaluation set, called HumanExtension, which contains examples of two functions where one function assists the other. With HumanExtension, we design several experiments to examine their ability in a multifaceted way. Our evaluation processes enable a comprehensive understanding of including auxiliary functions in the prompt in terms of effectiveness and robustness. An additional implementation style analysis captures the models' various implementation patterns when they access the auxiliary function. Through this analysis, we discover the models' promising ability to utilize auxiliary functions including their self-improving behavior by implementing the two functions step-by-step. However, our analysis also reveals the model's underutilized behavior to call the auxiliary function, suggesting the future direction to enhance their implementation by eliciting the auxiliary function call ability encoded in the models. We release our code and dataset to facilitate this research direction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287842444",
                    "name": "Seonghyeon Lee"
                },
                {
                    "authorId": "2287988207",
                    "name": "Sanghwan Jang"
                },
                {
                    "authorId": "1523619467",
                    "name": "Seongbo Jang"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "2286165699",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "1604c2854370b9c6c9558b360c566f058ef33e7f",
            "title": "Learning Topology-Specific Experts for Molecular Property Prediction",
            "abstract": "Recently, graph neural networks (GNNs) have been successfully applied to predicting molecular properties, which is one of the most classical cheminformatics tasks with various applications. Despite their effectiveness, we empirically observe that training a single GNN model for diverse molecules with distinct structural patterns limits its prediction performance. In this paper, motivated by this observation, we propose TopExpert to leverage topology-specific prediction models (referred to as experts), each of which is responsible for each molecular group sharing similar topological semantics. That is, each expert learns topology-specific discriminative features while being trained with its corresponding topological group. To tackle the key challenge of grouping molecules by their topological patterns, we introduce a clustering-based gating module that assigns an input molecule into one of the clusters and further optimizes the gating module with two different types of self-supervision: topological semantics induced by GNNs and molecular scaffolds, respectively. Extensive experiments demonstrate that TopExpert has boosted the performance for molecular property prediction and also achieved better generalization for new molecules with unseen scaffolds than baselines. The code is available at https://github.com/kimsu55/ToxExpert.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "2143061094",
                    "name": "S. Kim"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "71965979",
                    "name": "SeongKu Kang"
                },
                {
                    "authorId": "72490748",
                    "name": "Seonghyeon Lee"
                },
                {
                    "authorId": "1723357",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "1c10f092ee7cddfcef08fb5eb4e55acc6b71a95e",
            "title": "Distillation from Heterogeneous Models for Top-K Recommendation",
            "abstract": "Recent recommender systems have shown remarkable performance by using an ensemble of heterogeneous models. However, it is exceedingly costly because it requires resources and inference latency proportional to the number of models, which remains the bottleneck for production. Our work aims to transfer the ensemble knowledge of heterogeneous teachers to a lightweight student model using knowledge distillation (KD), to reduce the huge inference costs while retaining high accuracy. Through an empirical study, we find that the efficacy of distillation severely drops when transferring knowledge from heterogeneous teachers. Nevertheless, we show that an important signal to ease the difficulty can be obtained from the teacher\u2019s training trajectory. This paper proposes a new KD framework, named HetComp, that guides the student model by transferring easy-to-hard sequences of knowledge generated from the teachers\u2019 trajectories. To provide guidance according to the student\u2019s learning state, HetComp uses dynamic knowledge construction to provide progressively difficult ranking knowledge and adaptive knowledge transfer to gradually transfer finer-grained ranking information. Our comprehensive experiments show that HetComp significantly improves the distillation quality and the generalization of the student model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "71965979",
                    "name": "SeongKu Kang"
                },
                {
                    "authorId": "1643930327",
                    "name": "Wonbin Kweon"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "2813328",
                    "name": "Jianxun Lian"
                },
                {
                    "authorId": "2110972323",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "1723357",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "93a7e45abbc0e4c522bc49953435fb16efa5cea9",
            "title": "Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding",
            "abstract": "Unsupervised discovery of stories with correlated news articles in real-time helps people digest massive news streams without expensive human annotations. A common approach of the existing studies for unsupervised online story discovery is to represent news articles with symbolic- or graph-based embedding and incrementally cluster them into stories. Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams. In this work, we propose a novel thematic embedding with an off-the-shelf pretrained sentence encoder to dynamically represent articles and stories by considering their shared temporal themes. To realize the idea for unsupervised online story discovery, a scalable framework USTORY is introduced with two main techniques, theme- and time-aware dynamic embedding and novelty-aware adaptive clustering, fueled by lightweight story summaries. A thorough evaluation with real news data sets demonstrates that USTORY achieves higher story discovery performances than baselines while being robust and scalable to various streaming settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3396235",
                    "name": "Susik Yoon"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "48379289",
                    "name": "Yunyi Zhang"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "eaa3f756eecf3ec3dae19c6aaf9d19e132f639a4",
            "title": "SCStory: Self-supervised and Continual Online Story Discovery",
            "abstract": "We present a framework SCStory for online story discovery, that helps people digest rapidly published news article streams in real-time without human annotations. To organize news article streams into stories, existing approaches directly encode the articles and cluster them based on representation similarity. However, these methods yield noisy and inaccurate story discovery results because the generic article embeddings do not effectively reflect the story-indicative semantics in an article and cannot adapt to the rapidly evolving news article streams. SCStory employs self-supervised and continual learning with a novel idea of story-indicative adaptive modeling of news article streams. With a lightweight hierarchical embedding module that first learns sentence representations and then article representations, SCStory identifies story-relevant information of news articles and uses them to discover stories. The embedding module is continuously updated to adapt to evolving news streams with a contrastive learning objective, backed up by two unique techniques, confidence-aware memory replay and prioritized-augmentation, employed for label absence and data scarcity problems. Thorough experiments on real and the latest news data sets demonstrate that SCStory outperforms existing state-of-the-art algorithms for unsupervised online story discovery.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3396235",
                    "name": "Susik Yoon"
                },
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "145325584",
                    "name": "Jiawei Han"
                }
            ]
        }
    ]
}