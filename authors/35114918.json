{
    "authorId": "35114918",
    "papers": [
        {
            "paperId": "c73999b8ac6aab39a46eb54adbe1258c725c679c",
            "title": "Geo-L: Topological Link Discovery for Geospatial Linked Data Made Easy",
            "abstract": "Geospatial linked data are an emerging domain, with growing interest in research and the industry. There is an increasing number of publicly available geospatial linked data resources, which can also be interlinked and easily integrated with private and industrial linked data on the web. The present paper introduces Geo-L, a system for the discovery of RDF spatial links based on topological relations. Experiments show that the proposed system improves state-of-the-art spatial linking processes in terms of mapping time and accuracy, as well as concerning resources retrieval efficiency and robustness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1413815091",
                    "name": "Christian Zinke-Wehlmann"
                },
                {
                    "authorId": "35114918",
                    "name": "Amit Kirschenbaum"
                }
            ]
        },
        {
            "paperId": "145bcc24455b84c0f0879a6cf5a870966e86a9be",
            "title": "Geo-L: Linking Geospatial Data Made Easy.",
            "abstract": "Geospatial Linked Data is an emerging domain with growing interest in research and industry. There is an increasing number of publicly available geospatial Linked Data resources and they need to be interlinked and easily integrated with private and industrial Linked Data on the Web. The present paper introduces Geo-L, a system for discovery of RDF spatial links based on topological relations. Experiments show that the proposed system improves state-of-the-art spatial linking processes in terms of mapping-time and -accuracy, as well as concerning resources retrieval efficiency and robustness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1413815091",
                    "name": "Christian Zinke-Wehlmann"
                },
                {
                    "authorId": "35114918",
                    "name": "Amit Kirschenbaum"
                }
            ]
        },
        {
            "paperId": "53308bc89ff1e9a2101010a6b3459b5abde3ff4c",
            "title": "Linking geospatial data with Geo-L - analysis and experiments of big data readiness of common technologies",
            "abstract": "Geospatial Linked Data is an emerging domain, with growing interest in research and industry. The growing sources of open-source geospatial Linked Data need to be interlinked and easily integrated with private and industrial linked data clouds, e.g., in the agricultural domain. The presented paper analyzes common link discovery tools for the feasibility of big linked data processing. Further, it introduces Geo-L, a system for discovery of RDF spatial links based on topological relations. Experiments show that the proposed system improves state-of-the-art spatial linking processes in terms of mapping-time and -accuracy, as well as concerning resources retrieval efficiency and robustness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1413815091",
                    "name": "Christian Zinke-Wehlmann"
                },
                {
                    "authorId": "35114918",
                    "name": "Amit Kirschenbaum"
                }
            ]
        },
        {
            "paperId": "30596da17f14943eb98f428844e100724e28456d",
            "title": "DBpedia NIF: Open, Large-Scale and Multilingual Knowledge Extraction Corpus",
            "abstract": "In the past decade, the DBpedia community has put significant amount of effort on developing technical infrastructure and methods for efficient extraction of structured information from Wikipedia. These efforts have been primarily focused on harvesting, refinement and publishing semi-structured information found in Wikipedia articles, such as information from infoboxes, categorization information, images, wikilinks and citations. Nevertheless, still vast amount of valuable information is contained in the unstructured Wikipedia article texts. In this paper, we present DBpedia NIF - a large-scale and multilingual knowledge extraction corpus. The aim of the dataset is two-fold: to dramatically broaden and deepen the amount of structured information in DBpedia, and to provide large-scale and multilingual language resource for development of various NLP and IR task. The dataset provides the content of all articles for 128 Wikipedia languages. We describe the dataset creation process and the NLP Interchange Format (NIF) used to model the content, links and the structure the information of the Wikipedia articles. The dataset has been further enriched with about 25% more links and selected partitions published as Linked Data. Finally, we describe the maintenance and sustainability plans, and selected use cases of the dataset from the TextExt knowledge extraction challenge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1819564",
                    "name": "Milan Dojchinovski"
                },
                {
                    "authorId": "123017598",
                    "name": "Julio Hernandez"
                },
                {
                    "authorId": "37683106",
                    "name": "Markus Ackermann"
                },
                {
                    "authorId": "35114918",
                    "name": "Amit Kirschenbaum"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "0b9c2d76835f45639a20b4a8a9d6818897916f84",
            "title": "IDOL: Comprehensive & Complete LOD Insights",
            "abstract": "Over the last decade, we observed a steadily increasing amount of RDF datasets made available on the web of data. The decentralized nature of the web, however, makes it hard to identify all these datasets. Even more so, when downloadable data distributions are discovered, only insufficient metadata is available to describe the datasets properly, thus posing barriers on its usefulness and reuse. In this paper, we describe an attempt to exhaustively identify the whole linked open data cloud by harvesting metadata from multiple sources, providing insights about duplicated data and the general quality of the available metadata. This was only possible by using a probabilistic data structure called Bloom filter. Finally, we published a dump file containing metadata which can further be used to enrich existent datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32124073",
                    "name": "C. Baron"
                },
                {
                    "authorId": "2627116",
                    "name": "D. Kontokostas"
                },
                {
                    "authorId": "35114918",
                    "name": "Amit Kirschenbaum"
                },
                {
                    "authorId": "3489323",
                    "name": "G. Publio"
                },
                {
                    "authorId": "145538480",
                    "name": "Diego Esteves"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "51f8b81d9c860f3bde514835b6c4d1830c55afb8",
            "title": "SEMANTIC REPRESENTATION OF NEWLY LEARNED L2 WORDS AND THEIR INTEGRATION IN THE L2 LEXICON",
            "abstract": "The present semantic priming study explores the integration of newly learnt L2 German words into the L2 semantic network of German advanced learners. It provides additional evidence in support of earlier findings reporting semantic inhibition effects for emergent representations. An inhibitory mechanism is proposed that temporarily decreases the resting levels of the representations with which the new representation is linked and thus enables its selection despite its low resting level.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3041167",
                    "name": "Denisa Bordag"
                },
                {
                    "authorId": "35114918",
                    "name": "Amit Kirschenbaum"
                },
                {
                    "authorId": "67343700",
                    "name": "Maria Rogahn"
                },
                {
                    "authorId": "34103063",
                    "name": "A. Opitz"
                },
                {
                    "authorId": "69355091",
                    "name": "Erwin Tschirner"
                }
            ]
        },
        {
            "paperId": "063711d399af554b35f369774fdaaf2969421726",
            "title": "Unsupervised morphological analysis of small corpora: First experiments with Kilivila",
            "abstract": "Language documentation involves linguistic analysis of the collected material, which is typically done manually. Automatic methods for language processing usually require large corpora. The method presented in this paper uses techniques from bioinformatics and contextual information to morphologically analyze raw text corpora. This paper presents initial results of the method when applied on a small Kilivila corpus. 1. INTRODUCTION. Unsupervised approaches to language processing attempt to discover the structure of language based on machine-learning techniques applied to unannotated text. Present unsupervised methods are typically developed for large corpora and are not suitable for small corpora of a few hundred thousand or even just thousands of words. However, this is the typical size of corpora in language documentation (see Klamer, this volume; Holton, this volume), where the potential of unsupervised learning methods has already been acknowledged: \u201cBasic linguistic descriptions of lexicon and grammar made on the basis of transcribed recordings still form an important component of language documentation, however, and with the realization that languages are disappearing at a far faster rate than linguists can document them, it is natural to look for ways of making this process less labor-intensive.\u201d (Hammarstrom & Borin 2011) In particular, unsupervised methods for morphological analysis aim to learn the internal structure of words from a raw text corpus of a given language, and this typically means segmenting words into morphemes. Unsupervised morphological analysis can be traced back to an algorithm introduced by Harris (1955, 1967), and later improved by Hafer & Weiss (1974). The algorithm detects morpheme boundaries as a function of the number of distinct letters that follow, or precede, a letter sequence which is part of a word (Letter Successor Variety). If a peak is reached in that number, then it would probably be due to a morpheme boundary. A different approach incorporates the Minimum Description Length (MDL) principle (Rissanen 1978), which is based on information-theoretic grounds. This principle follows",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35114918",
                    "name": "Amit Kirschenbaum"
                },
                {
                    "authorId": "1681130",
                    "name": "P. Wittenburg"
                },
                {
                    "authorId": "2013643",
                    "name": "Gerhard Heyer"
                }
            ]
        },
        {
            "paperId": "2da3b86e46d63486221f961a78af2417be29c0ba",
            "title": "A General Method for Creating a Bilingual Transliteration Dictionary",
            "abstract": "Transliteration is the rendering in one language of terms from another language (and, possibly, another writing system), approximating spelling and/or phonetic equivalents between the two languages. A transliteration dictionary is a crucial resource for a variety of natural language applications, most notably machine translation. We describe a general method for creating bilingual transliteration dictionaries from Wikipedia article titles. The method can be applied to any language pair with Wikipedia presence, independently of the writing systems involved, and requires only a single simple resource that can be provided by any literate bilingual speaker. It was successfully applied to extract a Hebrew-English transliteration dictionary which, when incorporated in a machine translation system, indeed improved its performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35114918",
                    "name": "Amit Kirschenbaum"
                },
                {
                    "authorId": "2524073",
                    "name": "S. Wintner"
                }
            ]
        },
        {
            "paperId": "f3671e5d04bd8bf9435f16aba989f917074963d0",
            "title": "Lightly Supervised Transliteration for Machine Translation",
            "abstract": "We present a Hebrew to English transliteration method in the context of a machine translation system. Our method uses machine learning to determine which terms are to be transliterated rather than translated. The training corpus for this purpose includes only positive examples, acquired semi-automatically. Our classifier reduces more than 38% of the errors made by a baseline method. The identified terms are then transliterated. We present an SMT-based transliteration model trained with a parallel corpus extracted from Wikipedia using a fairly simple method which requires minimal knowledge. The correct result is produced in more than 76% of the cases, and in 92% of the instances it is one of the top-5 results. We also demonstrate a small improvement in the performance of a Hebrew-to-English MT system that uses our transliteration module.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35114918",
                    "name": "Amit Kirschenbaum"
                },
                {
                    "authorId": "2524073",
                    "name": "S. Wintner"
                }
            ]
        }
    ]
}