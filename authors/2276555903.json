{
    "authorId": "2276555903",
    "papers": [
        {
            "paperId": "8baf10d11f02a162667087f276c86d187ff9e919",
            "title": "ALISON: Fast and Effective Stylometric Authorship Obfuscation",
            "abstract": "Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods,\nnew AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours.\nTo this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features, allowing sound model interpretation for explainable obfuscation. We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics. To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2276555903",
                    "name": "Eric Xing"
                },
                {
                    "authorId": "2189394679",
                    "name": "Saranya Venkatraman"
                },
                {
                    "authorId": "2260345102",
                    "name": "Thai Le"
                },
                {
                    "authorId": "2158951945",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "68af151b2f66cd90e35aad807f282a960949b57b",
            "title": "A Toolkit for Assessments in Introductory Programming Courses",
            "abstract": "Traditional paper-based exams and LMS-provided online exams for introductory programming courses are not aligned with learning objectives that emphasize problem-solving and coding skills. In this poster, we present a cloud-based assessment solution for introductory programming courses. First, we discuss the requirements and challenges of conducting frequent assessments. We then outline the functions in our online exam toolkit that allow instructors to administer versatile assessments. Instead of relying on a traditional lockdown browser, the plagiarism and cheating detection in our toolkit allows instructors to administer exams in any modern browser for face-to-face classes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2276555903",
                    "name": "Eric Xing"
                },
                {
                    "authorId": "5070192",
                    "name": "Guangming Xing"
                }
            ]
        },
        {
            "paperId": "a23a46af20b6b4d01eef75a47163850973d60134",
            "title": "Beware the Black-Box of Medical Image Generation: an Uncertainty Analysis by the Learned Feature Space",
            "abstract": "Deep neural networks (DNNs) are the primary driving force for the current development of medical imaging analysis tools and often provide exciting performance on various tasks. However, such results are usually reported on the overall performance of DNNs, such as the Peak signal-to-noise ratio (PSNR) or mean square error (MSE) for imaging generation tasks. As a black-box, DNNs usually produce a relatively stable performance on the same task across multiple training trials, while the learned feature spaces could be significantly different. We believe additional insightful analysis, such as uncertainty analysis of the learned feature space, is equally important, if not more. Through this work, we evaluate the learned feature space of multiple U-Net architectures for image generation tasks using computational analysis and clustering analysis methods. We demonstrate that the learned feature spaces are easily separable between different training trials of the same architecture with the same hyperparameter setting, indicating the models using different criteria for the same tasks. This phenomenon naturally raises the question of which criteria are correct to use. Thus, our work suggests that assessments other than overall performance are needed before applying a DNN model to real-world practice.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2184484490",
                    "name": "Yunni Qu"
                },
                {
                    "authorId": "2069088408",
                    "name": "David Yan"
                },
                {
                    "authorId": "2276555903",
                    "name": "Eric Xing"
                },
                {
                    "authorId": "144842331",
                    "name": "Fengbo Zheng"
                },
                {
                    "authorId": "2159189080",
                    "name": "Jie Zhang"
                },
                {
                    "authorId": "2181209735",
                    "name": "Liangliang Liu"
                },
                {
                    "authorId": "1958036",
                    "name": "G. Liang"
                }
            ]
        },
        {
            "paperId": "c3ba21c44e63bc1861ea3c23dd19d0d2a2944d6b",
            "title": "Neural Network Decision-Making Criteria Consistency Analysis via Inputs Sensitivity",
            "abstract": "Neural networks (NNs) have demonstrated exciting results on various tasks within the last decade. For example, the performance on image classification tasks has been improved dramatically. However, the performance evaluations are often based on a black-box performance, such as accuracy, while insightful analysis of the black-box, such as the prediction formation mechanism, is often missing. Empirically, a NN usually produces a stable overall performance on the same task across multiple training trials when treating it as a black-box. However, when unveiling the black-box, the performance is usually volatile. The decision-making criteria learned by the training trials are often significantly different, which is problematic in many ways. We believe achieving consistent criteria between different training trials is equally important to achieving high performance, if not more. This work, firstly, evaluates the decision-making criteria of NNs via inputs sensitivity using feature-attribution explanation methods in combination with computational analysis and clustering analysis. Through intensive experimentation, we find that decision-making criteria are easily distinguishable between training trials of the same architecture and task, suggesting the criteria learned between training trials are significantly inconsistent. To mitigate this inconsistency, we propose three general training schemes. Our demonstration result shows that the proposed methods effectively reduce the inconsistency of the decision-making criteria learned by different training trials while maintaining the overall performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2276555903",
                    "name": "Eric Xing"
                },
                {
                    "authorId": "2181209735",
                    "name": "Liangliang Liu"
                },
                {
                    "authorId": "1491631303",
                    "name": "Xin Xing"
                },
                {
                    "authorId": "2184484490",
                    "name": "Yunni Qu"
                },
                {
                    "authorId": "145801672",
                    "name": "Nathan Jacobs"
                },
                {
                    "authorId": "1958036",
                    "name": "G. Liang"
                }
            ]
        }
    ]
}