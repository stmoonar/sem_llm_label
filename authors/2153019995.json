{
    "authorId": "2153019995",
    "papers": [
        {
            "paperId": "23cc318882b295fda5233768d59740333b9c4e63",
            "title": "Beyond Two-Tower Matching: Learning Sparse Retrievable Cross-Interactions for Recommendation",
            "abstract": "Two-tower models are a prevalent matching framework for recommendation, which have been widely deployed in industrial applications. The success of two-tower matching attributes to its efficiency in retrieval among a large number of items, since the item tower can be precomputed and used for fast Approximate Nearest Neighbor (ANN) search. However, it suffers two main challenges, including limited feature interaction capability and reduced accuracy in online serving. Existing approaches attempt to design novel late interactions instead of dot products, but they still fail to support complex feature interactions or lose retrieval efficiency. To address these challenges, we propose a new matching paradigm named SparCode, which supports not only sophisticated feature interactions but also efficient retrieval. Specifically, SparCode introduces an all-to-all interaction module to model fine-grained query-item interactions. Besides, we design a discrete code-based sparse inverted index jointly trained with the model to achieve effective and efficient model inference. Extensive experiments have been conducted on open benchmark datasets to demonstrate the superiority of our framework. The results show that SparCode significantly improves the accuracy of candidate item matching while retaining the same level of retrieval efficiency with two-tower models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1571162112",
                    "name": "Liangcai Su"
                },
                {
                    "authorId": "2223746444",
                    "name": "Fan Yan"
                },
                {
                    "authorId": "2108997533",
                    "name": "Jieming Zhu"
                },
                {
                    "authorId": "2153019995",
                    "name": "Xi Xiao"
                },
                {
                    "authorId": "2223764345",
                    "name": "Haoyi Duan"
                },
                {
                    "authorId": "2187385241",
                    "name": "Zhou Zhao"
                },
                {
                    "authorId": "3065080",
                    "name": "Zhenhua Dong"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "2f06c35c0ddaafd8a80690141b7044968a434448",
            "title": "STEM: Unleashing the Power of Embeddings for Multi-task Recommendation",
            "abstract": "Multi-task learning (MTL) has gained significant popularity in recommender systems as it enables simultaneous optimization of multiple objectives. A key challenge in MTL is negative transfer, but existing studies explored negative transfer on all samples, overlooking the inherent complexities within them. We split the samples according to the relative amount of positive feedback among tasks. Surprisingly, negative transfer still occurs in existing MTL methods on samples that receive comparable feedback across tasks. Existing work commonly employs a shared-embedding paradigm, limiting the ability of modeling diverse user preferences on different tasks. In this paper, we introduce a novel Shared and Task-specific EMbeddings (STEM) paradigm that aims to incorporate both shared and task-specific embeddings to effectively capture task-specific user preferences. Under this paradigm, we propose a simple model STEM-Net, which is equipped with an All Forward Task-specific Backward gating network to facilitate the learning of task-specific embeddings and direct knowledge transfer across tasks. Remarkably, STEM-Net demonstrates exceptional performance on comparable samples, achieving positive transfer. Comprehensive evaluation on three public MTL recommendation datasets demonstrates that STEM-Net outperforms state-of-the-art models by a substantial margin. Our code is released at https://github.com/LiangcaiSu/STEM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1571162112",
                    "name": "Liangcai Su"
                },
                {
                    "authorId": "12692416",
                    "name": "Junwei Pan"
                },
                {
                    "authorId": "2561964",
                    "name": "Ximei Wang"
                },
                {
                    "authorId": "2153019995",
                    "name": "Xi Xiao"
                },
                {
                    "authorId": "2234352886",
                    "name": "Shijie Quan"
                },
                {
                    "authorId": "2192666804",
                    "name": "Xihua Chen"
                },
                {
                    "authorId": "2203425092",
                    "name": "Jie Jiang"
                }
            ]
        },
        {
            "paperId": "1970c198f4e2bdbb4f006c44aef5ec3b7c9ac80e",
            "title": "Hypergraph Convolutional Networks via Equivalency between Hypergraphs and Undirected Graphs",
            "abstract": "As a powerful tool for modeling complex relationships, hypergraphs are gaining popularity from the graph learning community. However, commonly used frameworks in deep hypergraph learning focus on hypergraphs with edge-independent vertex weights (EIVWs), without considering hypergraphs with edge-dependent vertex weights (EDVWs) that have more modeling power. To compensate for this, we present General Hypergraph Spectral Convolution (GHSC), a general learning framework that not only handles EDVW and EIVW hypergraphs, but more importantly, enables theoretically explicitly utilizing the existing powerful Graph Convolutional Neural Networks (GCNNs) such that largely ease the design of Hypergraph Neural Networks. In this framework, the graph Laplacian of the given undirected GCNNs is replaced with a unified hypergraph Laplacian that incorporates vertex weight information from a random walk perspective by equating our defined generalized hypergraphs with simple undirected graphs. Extensive experiments from various domains including social network analysis, visual objective classification, and protein learning demonstrate the state-of-the-art performance of the proposed framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107966843",
                    "name": "Jiying Zhang"
                },
                {
                    "authorId": "144186273",
                    "name": "Fuyang Li"
                },
                {
                    "authorId": "2153019995",
                    "name": "Xi Xiao"
                },
                {
                    "authorId": "1754673",
                    "name": "Tingyang Xu"
                },
                {
                    "authorId": "48537464",
                    "name": "Yu Rong"
                },
                {
                    "authorId": "1768190",
                    "name": "Junzhou Huang"
                },
                {
                    "authorId": "2419616",
                    "name": "Yatao Bian"
                }
            ]
        },
        {
            "paperId": "c0c546b6b8029203f2e65c7348e712efea881625",
            "title": "PEAR: Personalized Re-ranking with Contextualized Transformer for Recommendation",
            "abstract": "The goal of recommender systems is to provide ordered item lists to users that best match their interests. As a critical task in the recommendation pipeline, re-ranking has received increasing attention in recent years. In contrast to conventional ranking models that score each item individually, re-ranking aims to explicitly model the mutual influences among items to further refine the ordering of items given an initial ranking list. In this paper, we present a personalized re-ranking model (dubbed PEAR) based on contextualized transformer. PEAR makes several major improvements over the existing methods. Specifically, PEAR not only captures feature-level and item-level interactions, but also models item contexts from both the initial ranking list and the historical clicked item list. In addition to item-level ranking score prediction, we also augment the training of PEAR with a list-level classification task to assess users\u2019 satisfaction on the whole ranking list. Experimental results on both public and production datasets have shown the superior effectiveness of PEAR compared to the previous re-ranking models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153684869",
                    "name": "Yi Li"
                },
                {
                    "authorId": "2108997533",
                    "name": "Jieming Zhu"
                },
                {
                    "authorId": "2130051800",
                    "name": "Weiwen Liu"
                },
                {
                    "authorId": "1571162112",
                    "name": "Liangcai Su"
                },
                {
                    "authorId": "24140498",
                    "name": "Guohao Cai"
                },
                {
                    "authorId": "2145908338",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2153019995",
                    "name": "Xi Xiao"
                },
                {
                    "authorId": "1996703",
                    "name": "Xiuqiang He"
                }
            ]
        },
        {
            "paperId": "dfc64d868b7b257a38e402896bb9b1c223864139",
            "title": "Fine-Tuning Graph Neural Networks via Graph Topology induced Optimal Transport",
            "abstract": "Recently, the pretrain-finetuning paradigm has attracted tons of attention in graph learning community due to its power of alleviating the lack of labels problem in many real-world applications. Current studies use existing techniques, such as weight constraint, representation constraint, which are derived from images or text data, to transfer the invariant knowledge from the pre-train stage to fine-tuning stage. However, these methods failed to preserve invariances from graph structure and Graph Neural Network (GNN) style models. In this paper, we present a novel optimal transport-based fine-tuning framework called GTOT-Tuning, namely, Graph Topology induced Optimal Transport fine-Tuning, for GNN style backbones. GTOT-Tuning is required to utilize the property of graph data to enhance the preservation of representation produced by fine-tuned networks. Toward this goal, we formulate graph local knowledge transfer as an Optimal Transport (OT) problem with a structural prior and construct the GTOT regularizer to constrain the fine-tuned model behaviors. By using the adjacency relationship amongst nodes, the GTOT regularizer achieves node-level optimal transport procedures and reduces redundant transport procedures, resulting in efficient knowledge transfer from the pre-trained models. We evaluate GTOT-Tuning on eight downstream tasks with various GNN backbones and demonstrate that it achieves state-of-the-art fine-tuning performance for GNNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107966843",
                    "name": "Jiying Zhang"
                },
                {
                    "authorId": "2153019995",
                    "name": "Xi Xiao"
                },
                {
                    "authorId": "2957079",
                    "name": "Long-Kai Huang"
                },
                {
                    "authorId": "48537464",
                    "name": "Yu Rong"
                },
                {
                    "authorId": "2419616",
                    "name": "Yatao Bian"
                }
            ]
        },
        {
            "paperId": "eb217e5e245305e3cb628f5a2e9b6d0a25d1032d",
            "title": "BARS: Towards Open Benchmarking for Recommender Systems",
            "abstract": "The past two decades have witnessed the rapid development of personalized recommendation techniques. Despite the significant progress made in both research and practice of recommender systems, to date, there is a lack of a widely-recognized benchmarking standard in this field. Many of the existing studies perform model evaluations and comparisons in an ad-hoc manner, for example, by employing their own private data splits or using a different experimental setting. However, such conventions not only increase the difficulty in reproducing existing studies, but also lead to inconsistent experimental results among them. This largely limits the credibility and practical value of research results in this field. To tackle these issues, we present an initiative project aimed for open benchmarking for recommender systems. In contrast to some earlier attempts towards this goal, we take one further step by setting up a standardized benchmarking pipeline for reproducible research, which integrates all the details about datasets, source code, hyper-parameter settings, running logs, and evaluation results. The benchmark is designed with comprehensiveness and sustainability in mind. It spans both matching and ranking tasks, and also allows anyone to easily follow and contribute. We believe that our benchmark could not only reduce the redundant efforts of researchers to re-implement or re-run existing baselines, but also drive more solid and reproducible research on recommender systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108997533",
                    "name": "Jieming Zhu"
                },
                {
                    "authorId": "1580228663",
                    "name": "Kelong Mao"
                },
                {
                    "authorId": "29969336",
                    "name": "Quanyu Dai"
                },
                {
                    "authorId": "1571162112",
                    "name": "Liangcai Su"
                },
                {
                    "authorId": "2165650825",
                    "name": "Ronglin Ma"
                },
                {
                    "authorId": "2108510525",
                    "name": "Jinyang Liu"
                },
                {
                    "authorId": "24140498",
                    "name": "Guohao Cai"
                },
                {
                    "authorId": "1897235",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "2153019995",
                    "name": "Xi Xiao"
                },
                {
                    "authorId": "2163112250",
                    "name": "Rui Zhang"
                }
            ]
        },
        {
            "paperId": "17be6098c6dba71eef609b3d45cf63a073c94221",
            "title": "UltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation",
            "abstract": "With the recent success of graph convolutional networks (GCNs), they have been widely applied for recommendation, and achieved impressive performance gains. The core of GCNs lies in its message passing mechanism to aggregate neighborhood information. However, we observed that message passing largely slows down the convergence of GCNs during training, especially for large-scale recommender systems, which hinders their wide adoption. LightGCN makes an early attempt to simplify GCNs for collaborative filtering by omitting feature transformations and nonlinear activations. In this paper, we take one step further to propose an ultra-simplified formulation of GCNs (dubbed UltraGCN), which skips infinite layers of message passing for efficient recommendation. Instead of explicit message passing, UltraGCN resorts to directly approximate the limit of infinite-layer graph convolutions via a constraint loss. Meanwhile, UltraGCN allows for more appropriate edge weight assignments and flexible adjustment of the relative importances among different types of relationships. This finally yields a simple yet effective UltraGCN model, which is easy to implement and efficient to train. Experimental results on four benchmark datasets show that UltraGCN not only outperforms the state-of-the-art GCN models but also achieves more than 10x speedup over LightGCN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1580228663",
                    "name": "Kelong Mao"
                },
                {
                    "authorId": "2108997533",
                    "name": "Jieming Zhu"
                },
                {
                    "authorId": "2153019995",
                    "name": "Xi Xiao"
                },
                {
                    "authorId": "2064916670",
                    "name": "Biao Lu"
                },
                {
                    "authorId": "2144715839",
                    "name": "Zhaowei Wang"
                },
                {
                    "authorId": "1996703",
                    "name": "Xiuqiang He"
                }
            ]
        },
        {
            "paperId": "6d552f03bd4c49565f5e76190712c41da1aa862b",
            "title": "SimpleX: A Simple and Strong Baseline for Collaborative Filtering",
            "abstract": "Collaborative filtering (CF) is a widely studied research topic in recommender systems. The learning of a CF model generally depends on three major components, namely interaction encoder, loss function, and negative sampling. While many existing studies focus on the design of more powerful interaction encoders, the impacts of loss functions and negative sampling ratios have not yet been well explored. In this work, we show that the choice of loss function as well as negative sampling ratio is equivalently important. More specifically, we propose the cosine contrastive loss (CCL) and further incorporate it to a simple unified CF model, dubbed SimpleX. Extensive experiments have been conducted on 10 benchmark datasets and compared with 28 existing CF models in total. Surprisingly, the results show that, under our CCL loss and a large negative sampling ratio, SimpleX can surpass most sophisticated state-of-the-art models by a large margin (e.g., max 48.5% improvement in NDCG@20 over LightGCN). We believe that SimpleX could not only serve as a simple strong baseline to foster future research on CF, but also shed light on the potential research direction towards improving loss function and negative sampling.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1580228663",
                    "name": "Kelong Mao"
                },
                {
                    "authorId": "2108997533",
                    "name": "Jieming Zhu"
                },
                {
                    "authorId": "2115922965",
                    "name": "Jinpeng Wang"
                },
                {
                    "authorId": "29969336",
                    "name": "Quanyu Dai"
                },
                {
                    "authorId": "3065080",
                    "name": "Zhenhua Dong"
                },
                {
                    "authorId": "2153019995",
                    "name": "Xi Xiao"
                },
                {
                    "authorId": "1996703",
                    "name": "Xiuqiang He"
                }
            ]
        }
    ]
}