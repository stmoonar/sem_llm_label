{
    "authorId": "1722362",
    "papers": [
        {
            "paperId": "a969b4563f2c9d19412f5d3add352d68ac88252e",
            "title": "Optimizing Airbnb Search Journey with Multi-task Learning",
            "abstract": "At Airbnb, an online marketplace for stays and experiences, guests often spend weeks exploring and comparing multiple items before making a final reservation request. Each reservation request may then potentially be rejected or cancelled by the host prior to check-in. The long and exploratory nature of the search journey, as well as the need to balance both guest and host preferences, present unique challenges for Airbnb search ranking. In this paper, we present Journey Ranker, a new multi-task deep learning model architecture that addresses these challenges. Journey Ranker leverages intermediate guest actions as milestones, both positive and negative, to better progress the guest towards a successful booking. It also uses contextual information such as guest state and search query to balance guest and host preferences. Its modular and extensible design, consisting of four modules with clear separation of concerns, allows for easy application to use cases beyond the Airbnb search ranking context. We conducted offline and online testing of the Journey Ranker and successfully deployed it in production to four different Airbnb products with significant business metrics improvements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2831229",
                    "name": "Chun How Tan"
                },
                {
                    "authorId": "2218482406",
                    "name": "Austin Chan"
                },
                {
                    "authorId": "2060199699",
                    "name": "Malay Haldar"
                },
                {
                    "authorId": "2218289739",
                    "name": "Jie Tang"
                },
                {
                    "authorId": "2120101748",
                    "name": "Xin Liu"
                },
                {
                    "authorId": "80481782",
                    "name": "Mustafa Abdool"
                },
                {
                    "authorId": "1722362",
                    "name": "Huiji Gao"
                },
                {
                    "authorId": "47648453",
                    "name": "Li He"
                },
                {
                    "authorId": "1403671551",
                    "name": "Sanjeev Katariya"
                }
            ]
        },
        {
            "paperId": "e54c341008634f9e109e5eadc1eb5c18115fc872",
            "title": "Data-centric AI: Techniques and Future Perspectives",
            "abstract": "The role of data in AI has been significantly magnified by the emerging concept of data-centric AI. In contrast to the traditional model-centric paradigm, which focuses on developing more effective models given fixed datasets, data-centric AI emphasizes the systematic engineering of data in building AI systems. However, as a new concept, many critical aspects of data-centric AI remain ambiguous, such as its definitions, associated tasks, algorithms, challenges, and benchmarks. This tutorial aims to review and discuss this emerging field, with a particular focus on the three general data-centric AI goals: training data development, inference data development, and data maintenance. The objective of this tutorial is threefold: (1) to formally categorize the field of data-centric AI using a goal-driven taxonomy and discuss the needs and challenges of each goal, (2) to comprehensively review the state-of-the-art techniques, and (3) to discuss the future perspectives and open research directions to inspire further innovations in this field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1759658",
                    "name": "D. Zha"
                },
                {
                    "authorId": "51238382",
                    "name": "Kwei-Herng Lai"
                },
                {
                    "authorId": "47829900",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "49648991",
                    "name": "Na Zou"
                },
                {
                    "authorId": "1722362",
                    "name": "Huiji Gao"
                },
                {
                    "authorId": "2109724398",
                    "name": "Xia Hu"
                }
            ]
        },
        {
            "paperId": "765ad480e2c6f08decce773c288709fcc7471594",
            "title": "Generalized Deep Mixed Models",
            "abstract": "We introduce generalized deep mixed model (GDMix), a class of machine learning models for large-scale recommender systems that combines the power of deep neural networks and the efficiency of logistic regression. GDMix leverages state-of-the-art deep neural networks (DNNs) as the global models (fixed effects), and further improves the performance by adding entity-specific personalized models (random effects). For instance, the click response from a particular user m to a job posting j may consist of contributions from a DNN model common to all users and job postings, a model specific to the user m and a model specific to the job j. GDMix models not only possess powerful modeling capabilities but also enjoy high training efficiency especially for web-scale recommender systems. We demonstrate the capabilities by detailing their use in Feed and Ads recommendation at LinkedIn. The source code for the GDMix training framework is available at https://github.com/linkedin/gdmix https://github.com/linkedin/gdmix under the BSD-2-Clause License.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113235078",
                    "name": "Jun Shi"
                },
                {
                    "authorId": "2156197059",
                    "name": "Chengming Jiang"
                },
                {
                    "authorId": "1633418058",
                    "name": "Aman Gupta"
                },
                {
                    "authorId": "1893250486",
                    "name": "Mingzhou Zhou"
                },
                {
                    "authorId": "1828232",
                    "name": "Yunbo Ouyang"
                },
                {
                    "authorId": "2181260857",
                    "name": "Q. Xiao"
                },
                {
                    "authorId": "25274194",
                    "name": "Qingquan Song"
                },
                {
                    "authorId": "2181446519",
                    "name": "Yi (Alice) Wu"
                },
                {
                    "authorId": "2175570239",
                    "name": "Haichao Wei"
                },
                {
                    "authorId": "1722362",
                    "name": "Huiji Gao"
                }
            ]
        },
        {
            "paperId": "aa11b040ce0b3f345c5a1a3d568b51b8f5c538b0",
            "title": "Learning To Rank Diversely At Airbnb",
            "abstract": "Airbnb is a two-sided marketplace, bringing together hosts who own listings for rent, with prospective guests from around the globe. Applying neural network-based learning to rank techniques has led to significant improvements in matching guests with hosts. These improvements in ranking were driven by a core strategy: order the listings by their estimated booking probabilities, then iterate on techniques to make these booking probability estimates more and more accurate. Embedded implicitly in this strategy was an assumption that the booking probability of a listing could be determined independently of other listings in search results. In this paper we discuss how this assumption, pervasive throughout the commonly-used learning to rank frameworks, is false. We provide a theoretical foundation correcting this assumption, followed by efficient neural network architectures based on the theory. Explicitly accounting for possible similarities between listings, and reducing them to diversify the search results generated strong positive impact. We discuss these metric wins as part of the online A/B tests of the theory. Our method provides a practical way to diversify search results for large-scale production ranking systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2060199699",
                    "name": "Malay Haldar"
                },
                {
                    "authorId": "80481782",
                    "name": "Mustafa Abdool"
                },
                {
                    "authorId": "47648453",
                    "name": "Li He"
                },
                {
                    "authorId": "49870899",
                    "name": "Dillon Davis"
                },
                {
                    "authorId": "1722362",
                    "name": "Huiji Gao"
                },
                {
                    "authorId": "1403671551",
                    "name": "Sanjeev Katariya"
                }
            ]
        },
        {
            "paperId": "16b0573b8a99746bd56c725bbbacab985be58557",
            "title": "AutoDim: Field-aware Embedding Dimension Searchin Recommender Systems",
            "abstract": "Practical large-scale recommender systems usually contain thousands of feature fields from users, items, contextual information, and their interactions. Most of them empirically allocate a unified dimension to all feature fields, which is memory inefficient. Thus it is highly desired to assign various embedding dimensions to different feature fields according to their importance and predictability. Due to the large amounts of feature fields and the nuanced relationship between embedding dimensions with feature distributions and neural network architectures, manually allocating embedding dimensions in practical recommender systems can be challenging. To this end, we propose an AutoML-based framework (AutoDim) in this paper, which can automatically select dimensions for different feature fields in a data-driven fashion. Specifically, we first proposed an end-to-end differentiable framework that can calculate the weights over various dimensions in a soft and continuous manner for feature fields, and an AutoML-based optimization algorithm; then, we derive a hard and discrete embedding component architecture according to the maximal weights and retrain the whole recommender framework. We conduct extensive experiments on benchmark datasets to validate the effectiveness of AutoDim.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2733057",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "66442354",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "2146672392",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "2109155757",
                    "name": "Weiwei Guo"
                },
                {
                    "authorId": "2113235078",
                    "name": "Jun Shi"
                },
                {
                    "authorId": "16230274",
                    "name": "Sida Wang"
                },
                {
                    "authorId": "1722362",
                    "name": "Huiji Gao"
                },
                {
                    "authorId": "143947042",
                    "name": "Bo Long"
                }
            ]
        },
        {
            "paperId": "1c2c570b78174522268567d545a91e5e377f3bdd",
            "title": "Deep Natural Language Processing for LinkedIn Search Systems",
            "abstract": "Many search systems work with large amounts of natural language data, e.g., search queries, user profiles and documents, where deep learning based natural language processing techniques (deep NLP) can be of great help. In this paper, we introduce a comprehensive study of applying deep NLP techniques to five representative tasks in search engines. Through the model design and experiments of the five tasks, readers can find answers to three important questions: (1) When is deep NLP helpful/not helpful in search systems? (2) How to address latency challenges? (3) How to ensure model robustness? This work builds on existing efforts of LinkedIn search, and is tested at scale on a commercial search engine. We believe our experiences can provide useful insights for the industry and research communities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109155757",
                    "name": "Weiwei Guo"
                },
                {
                    "authorId": "2109116511",
                    "name": "Xiaowei Liu"
                },
                {
                    "authorId": "16230274",
                    "name": "Sida Wang"
                },
                {
                    "authorId": "3315931",
                    "name": "Michaeel Kazi"
                },
                {
                    "authorId": "1887610660",
                    "name": "Zhoutong Fu"
                },
                {
                    "authorId": "1722362",
                    "name": "Huiji Gao"
                },
                {
                    "authorId": "2117241913",
                    "name": "Jun Jia"
                },
                {
                    "authorId": "2146644146",
                    "name": "Liang Zhang"
                },
                {
                    "authorId": "2052143728",
                    "name": "Bo Long"
                }
            ]
        },
        {
            "paperId": "73b1ca258d3440e7690ddf0db756e4ea08061705",
            "title": "Incremental Learning for Personalized Recommender Systems",
            "abstract": "Ubiquitous personalized recommender systems are built to achieve two seemingly conflicting goals, to serve high quality content tailored to individual user's taste and to adapt quickly to the ever changing environment. The former requires a complex machine learning model that is trained on a large amount of data; the latter requires frequent update to the model. We present an incremental learning solution to provide both the training efficiency and the model quality. Our solution is based on sequential Bayesian update and quadratic approximation. Our focus is on large-scale personalized logistic regression models, with extensions to deep learning models. This paper fills in the gap between the theory and the practice by addressing a few implementation challenges that arise when applying incremental learning to large personalized recommender systems. Detailed offline and online experiments demonstrated our approach can significantly shorten the training time while maintaining the model accuracy. The solution is deployed in LinkedIn and directly applicable to industrial scale recommender systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1828232",
                    "name": "Yunbo Ouyang"
                },
                {
                    "authorId": "2113235078",
                    "name": "Jun Shi"
                },
                {
                    "authorId": "2799847",
                    "name": "Haichao Wei"
                },
                {
                    "authorId": "1722362",
                    "name": "Huiji Gao"
                }
            ]
        },
        {
            "paperId": "0765baddc2b83166f123f3fddd5557638d8d8f43",
            "title": "Deep Search Query Intent Understanding",
            "abstract": "Understanding a user's query intent behind a search is critical for modern search engine success. Accurate query intent prediction allows the search engine to better serve the user's need by rendering results from more relevant categories. This paper aims to provide a comprehensive learning framework for modeling query intent under different stages of a search. We focus on the design for 1) predicting users' intents as they type in queries on-the-fly in typeahead search using character-level models; and 2) accurate word-level intent prediction models for complete queries. Various deep learning components for query text understanding are experimented. Offline evaluation and online A/B test experiments show that the proposed methods are effective in understanding query intent and efficient to scale for online search systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109116511",
                    "name": "Xiaowei Liu"
                },
                {
                    "authorId": "48544634",
                    "name": "Weiwei Guo"
                },
                {
                    "authorId": "1722362",
                    "name": "Huiji Gao"
                },
                {
                    "authorId": "143947042",
                    "name": "Bo Long"
                }
            ]
        },
        {
            "paperId": "0bce51554266705b746f3ade64d581b63cd2a4af",
            "title": "Incorporating User Feedback into Sequence to Sequence Model Training",
            "abstract": "As the largest professional network, LinkedIn hosts millions of user profiles and job postings. Users effectively find what they need by entering search queries. However, finding what they are looking for can be a challenge, especially if they are unfamiliar with specific keywords from their industry. Query Suggestion is a popular feature where a search engine can suggest alternate, related queries. At LinkedIn, we have productionized a deep learning Seq2Seq model to transform an input query into several alternatives. This model is trained by examining search history directly typed by users. Once online, we can determine whether or not users clicked on suggested queries. This new feedback data indicates which suggestions caught the user's attention. In this work, we propose training a model with both the search history and user feedback datasets. We examine several ways to incorporate feedback without any architectural change, including adding a novel pairwise ranking loss term during training. The proposed new training technique produces the best combined score out of several alternatives in offline metrics. Deployed in the LinkedIn search engine, it significantly outperforms the control model with respect to key business metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3315931",
                    "name": "Michaeel Kazi"
                },
                {
                    "authorId": "48544634",
                    "name": "Weiwei Guo"
                },
                {
                    "authorId": "1722362",
                    "name": "Huiji Gao"
                },
                {
                    "authorId": "143947042",
                    "name": "Bo Long"
                }
            ]
        },
        {
            "paperId": "110ef8f751d1abf3f18b10ee90f883f2709f2312",
            "title": "DeText: A Deep Text Ranking Framework with BERT",
            "abstract": "Ranking is the most important component in a search system. Most search systems deal with large amounts of natural language data, hence an effective ranking system requires a deep understanding of text semantics. Recently, deep learning based natural language processing (deep NLP) models have generated promising results on ranking systems. BERT is one of the most successful models that learn contextual embedding, which has been applied to capture complex query-document relations for search ranking. However, this is generally done by exhaustively interacting each query word with each document word, which is inefficient for online serving in search product systems. In this paper, we investigate how to build an efficient BERT-based ranking model for industry use cases. The solution is further extended to a general ranking framework, DeText, that is open sourced and can be applied to various ranking productions. Offline and online experiments of DeText on three real-world search systems present significant improvement over state-of-the-art approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48544634",
                    "name": "Weiwei Guo"
                },
                {
                    "authorId": "2109116511",
                    "name": "Xiaowei Liu"
                },
                {
                    "authorId": "16230274",
                    "name": "Sida Wang"
                },
                {
                    "authorId": "1722362",
                    "name": "Huiji Gao"
                },
                {
                    "authorId": "145630384",
                    "name": "A. Sankar"
                },
                {
                    "authorId": "2111907175",
                    "name": "Zimeng Yang"
                },
                {
                    "authorId": "2153928779",
                    "name": "Qi Guo"
                },
                {
                    "authorId": "2146644146",
                    "name": "Liang Zhang"
                },
                {
                    "authorId": "143947042",
                    "name": "Bo Long"
                },
                {
                    "authorId": "34252531",
                    "name": "Bee-Chung Chen"
                },
                {
                    "authorId": "2596058",
                    "name": "D. Agarwal"
                }
            ]
        }
    ]
}