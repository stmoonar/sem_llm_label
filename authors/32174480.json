{
    "authorId": "32174480",
    "papers": [
        {
            "paperId": "7d3b96b370ab105c8c9302cbeeb0f5e94bdabbc8",
            "title": "SPC: Soft Prompt Construction for Cross Domain Generalization",
            "abstract": "Recent advances in prompt tuning have proven effective as a new language modeling paradigm for various natural language understanding tasks. However, it is challenging to adapt the soft prompt embeddings to different domains or generalize to low-data settings when learning soft prompts itself is unstable, task-specific, and bias-prone. This paper proposes a principled learning framework\u2014soft prompt construction (SPC)\u2014to facilitate learning domain-adaptable soft prompts. Derived from the SPC framework is a simple loss that can plug into various models and tuning approaches to improve their cross-domain performance. We show SPC can improve upon SOTA for contextual query rewriting, summarization, and paraphrase detection by up to 5%, 19%, and 16%, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1902008743",
                    "name": "Wenbo Zhao"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "2217929400",
                    "name": "Jing Huang"
                }
            ]
        },
        {
            "paperId": "e65f6420f0876d4681dc11a0163e12a8a236dc5a",
            "title": "Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning",
            "abstract": "Dialogue state tracking (DST) is an important step in dialogue management to keep track of users' beliefs. Existing works fine-tune all language model (LM) parameters to tackle the DST task, which requires significant data and computing resources for training and hosting. The cost grows exponentially in the real-world deployment where dozens of fine-tuned LM are used for different domains and tasks. To reduce parameter size and better utilize cross-task shared information, we propose to use soft prompt token embeddings to learn task properties. Without tuning LM parameters, our method drastically reduces the number of parameters needed to less than 0.5% of prior works while achieves better low-resource DST performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "ef4bd399a66f6e2b4a0b6bfe2aa2063eaef10375",
            "title": "DiNADO: Norm-Disentangled Neurally-Decomposed Oracles for Controlling Language Models",
            "abstract": "NeurAlly-Decomposed Oracle (NADO) is a powerful approach for controllable generation with large language models. It is designed to avoid catastrophic forgetting while achieving guaranteed convergence to an entropy-maximized closed-form optimal solution with reasonable modeling capacity. Despite the success, several challenges arise when apply NADO to a wide range of scenarios. Vanilla NADO suffers from gradient vanishing for low-probability control signals and is highly reliant on a regularization to satisfy the stochastic version of Bellman equation. In addition, the vanilla implementation of NADO introduces a few additional transformer layers, suffering from a limited capacity especially compared to other finetune-based model adaptation methods like LoRA. In this paper, we propose a improved version of the NADO algorithm, namely DiNADO (norm-Disentangled NeurAlly-Decomposed Oracles), which improves the performance of the NADO algorithm through disentangling the step-wise global norm over the approximated oracle $R$-value for all potential next-tokens, allowing DiNADO to be combined with finetuning methods like LoRA. We discuss in depth how DiNADO achieves better capacity, stability and flexibility with both empirical and theoretical results. Experiments on formality control in machine translation and the lexically constrained generation task CommonGen demonstrates the significance of the improvements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167235968",
                    "name": "Sidi Lu"
                },
                {
                    "authorId": "1902008743",
                    "name": "Wenbo Zhao"
                },
                {
                    "authorId": "46387857",
                    "name": "Chenyang Tao"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "48914934",
                    "name": "Shan Wu"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "7e5712806ec37a554914dc8dae19fe57b9645763",
            "title": "GRAVL-BERT: Graphical Visual-Linguistic Representations for Multimodal Coreference Resolution",
            "abstract": "Learning from multimodal data has become a popular research topic in recent years. Multimodal coreference resolution (MCR) is an important task in this area. MCR involves resolving the references across different modalities, e.g., text and images, which is a crucial capability for building next-generation conversational agents. MCR is challenging as it requires encoding information from different modalities and modeling associations between them. Although significant progress has been made for visual-linguistic tasks such as visual grounding, most of the current works involve single turn utterances and focus on simple coreference resolutions. In this work, we propose an MCR model that resolves coreferences made in multi-turn dialogues with scene images. We present GRAVL-BERT, a unified MCR framework which combines visual relationships between objects, background scenes, dialogue, and metadata by integrating Graph Neural Networks with VL-BERT. We present results on the SIMMC 2.0 multimodal conversational dataset, achieving the rank-1 on the DSTC-10 SIMMC 2.0 MCR challenge with F1 score 0.783. Our code is available at https://github.com/alexa/gravl-bert.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2068107537",
                    "name": "Danfeng Guo"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "150290293",
                    "name": "Sanchit Agarwal"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "2052990580",
                    "name": "Arijit Biswas"
                },
                {
                    "authorId": "2116675467",
                    "name": "Chien-Wei Lin"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "143977268",
                    "name": "Mohit Bansal"
                }
            ]
        },
        {
            "paperId": "d1b918a625dea0816003e06de99224c62e4e362d",
            "title": "A dataset for resolving referring expressions in spoken dialogue via contextual query rewrites (CQR)",
            "abstract": "We present Contextual Query Rewrite (CQR) a dataset for multi-domain task-oriented spoken dialogue systems that is an extension of the Stanford dialog corpus (Eric et al., 2017a). While previous approaches have addressed the issue of diverse schemas by learning candidate transformations (Naik et al., 2018), we instead model the reference resolution task as a user query reformulation task, where the dialog state is serialized into a natural language query that can be executed by the downstream spoken language understanding system. In this paper, we describe our methodology for creating the query reformulation extension to the dialog corpus, and present an initial set of experiments to establish a baseline for the CQR task. We have released the corpus to the public [1] to support further research in this area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145666891",
                    "name": "Michael Regan"
                },
                {
                    "authorId": "2294125",
                    "name": "Pushpendre Rastogi"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "36299222",
                    "name": "Lambert Mathias"
                }
            ]
        },
        {
            "paperId": "3673feae845f678f70433ca055e77e6d774bfa93",
            "title": "Cross-Lingual Approaches to Reference Resolution in Dialogue Systems",
            "abstract": "In the slot-filling paradigm, where a user can refer back to slots in the context during the conversation, the goal of the contextual understanding system is to resolve the referring expressions to the appropriate slots in the context. In this paper, we build on the context carryover system~\\citep{Naik2018ContextualSC}, which provides a scalable multi-domain framework for resolving references. However, scaling this approach across languages is not a trivial task, due to the large demand on acquisition of annotated data in the target language. Our main focus is on cross-lingual methods for reference resolution as a way to alleviate the need for annotated data in the target language. In the cross-lingual setup, we assume there is access to annotated resources as well as a well trained model in the source language and little to no annotated data in the target language. In this paper, we explore three different approaches for cross-lingual transfer \\textemdash~\\ delexicalization as data augmentation, multilingual embeddings and machine translation. We compare these approaches both on a low resource setting as well as a large resource setting. Our experiments show that multilingual embeddings and delexicalization via data augmentation have a significant impact in the low resource setting, but the gains diminish as the amount of available data in the target language increases. Furthermore, when combined with machine translation we can get performance very close to actual live data in the target language, with only 25\\% of the data projected into the target language.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143816740",
                    "name": "Amr Sharaf"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "9090287",
                    "name": "Hancheng Ge"
                },
                {
                    "authorId": "2065416343",
                    "name": "Chetan Naik"
                },
                {
                    "authorId": "36299222",
                    "name": "Lambert Mathias"
                }
            ]
        },
        {
            "paperId": "a1f7f3604870fb9797973fbb8109abe21df8ecea",
            "title": "Contextual Slot Carryover for Disparate Schemas",
            "abstract": "In the slot-filling paradigm, where a user can refer back to slots in the context during a conversation, the goal of the contextual understanding system is to resolve the referring expressions to the appropriate slots in the context. In large-scale multi-domain systems, this presents two challenges - scaling to a very large and potentially unbounded set of slot values, and dealing with diverse schemas. We present a neural network architecture that addresses the slot value scalability challenge by reformulating the contextual interpretation as a decision to carryover a slot from a set of possible candidates. To deal with heterogenous schemas, we introduce a simple data-driven method for trans- forming the candidate slots. Our experiments show that our approach can scale to multiple domains and provides competitive results over a strong baseline.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065416343",
                    "name": "Chetan Naik"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "9090287",
                    "name": "Hancheng Ge"
                },
                {
                    "authorId": "36299222",
                    "name": "Lambert Mathias"
                },
                {
                    "authorId": "1705701",
                    "name": "R. Sarikaya"
                }
            ]
        },
        {
            "paperId": "7d71eff3db47ab9c482f5121a1d6a00cea37c0f0",
            "title": "Just ASK: Building an Architecture for Extensible Self-Service Spoken Language Understanding",
            "abstract": "This paper presents the design of the machine learning architecture that underlies the Alexa Skills Kit (ASK), which was the first Spoken Language Understanding (SLU) Software Development Kit (SDK) for a virtual digital assistant, as far as we are aware. At Amazon, the infrastructure powers more than 20,000 skills built through the ASK, as well as AWS's Amazon Lex SLU Service. The ASK emphasizes flexibility, predictability and a rapid iteration cycle for third party developers. It imposes inductive biases that allow it to learn robust SLU models from extremely small and sparse datasets and, in doing so, removes significant barriers to entry for software developers and dialog systems researchers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27203082",
                    "name": "Anjishnu Kumar"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "144762366",
                    "name": "Julian Chan"
                },
                {
                    "authorId": "47571047",
                    "name": "S. Tucker"
                },
                {
                    "authorId": "145878402",
                    "name": "Bj\u00f6rn Hoffmeister"
                },
                {
                    "authorId": "40262269",
                    "name": "Markus Dreyer"
                }
            ]
        },
        {
            "paperId": "ecb68636743c981e7481cc9aecff9c94c8757bff",
            "title": "Samiksha: mining issue tracking system for contribution and performance assessment",
            "abstract": "Individual contribution and performance assessment is a standard practice conducted in organizations to measure the value addition by various contributors. Accurate measurement of individual contributions based on pre-defined objectives, roles and Key Performance Indicators (KPIs) is a challenging task. In this paper, we propose a contribution and performance assessment framework (called as Samiksha) in the context of Software Maintenance. The focus of the study presented in this paper is Software Maintenance Activities (such as bug fixing and feature enhancement) performed by bug reporters, bug triagers, bug fixers, software developers, quality assurance and project managers facilitated by an Issue Tracking System.\n We present the result of a survey that we conducted to understand practitioner's perspective and experience (specifically on the topic of contribution assessment for software maintenance professionals). We propose several performance metrics covering different aspects (such as number of bugs fixed weighted by priority and quality of bugs reported) and various roles (such as bug reporter and bug fixer). We conduct a series of experiments on Google Chromium Project data (extracting data from the issue tracker for Google Chromium Project) and present results demonstrating the effectiveness of our proposed framework.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40464536",
                    "name": "Ayushi Rastogi"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "145914213",
                    "name": "A. Sureka"
                }
            ]
        }
    ]
}