{
    "authorId": "2280276151",
    "papers": [
        {
            "paperId": "8bc08a3fa45a25822f5926fbdc306f192dcfbd20",
            "title": "Efficient and Accurate Contextual Re-Ranking for Knowledge Graph Question Answering",
            "abstract": "The efficacy of neural \u201cretrieve and generate\u201d systems is well established for question answering (QA) over unstructured text. Recent efforts seek to extend this approach to knowledge graph (KG) QA by converting structured triples to unstructured text. However, the relevance of KG triples retrieved by these systems limits their accuracy. In this paper, we improve the relevance of retrieved triples using a carefully designed re-ranker. Specifically, our pipeline (i) retrieves over documents of triples grouped by entity, (ii) re-ranks triples from these documents with context: triples in the 1-hop neighborhood of the documents\u2019 subject entity, and (iii) generates an answer from highly relevant re-ranked triples. To train our re-ranker, we propose a novel \u201ctriple-level\u201d labeling strategy that infers fine-grained labels and shows that these significantly improve the relevance of retrieved information. We show that the resulting \u201cretrieve, re-rank, and generate\u201d pipeline significantly improves upon prior KGQA systems, achieving a new state-of-the-art on FreebaseQA by 5.56% Exact Match. We perform multiple ablations that reveal the distinct benefits of our contextual re-ranker and labeling strategy and conclude with a case study that highlights opportunities for future works.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280276151",
                    "name": "Kexuan Sun"
                },
                {
                    "authorId": "2139649061",
                    "name": "Nic Jedema"
                },
                {
                    "authorId": "2303002552",
                    "name": "Karishma Sharma"
                },
                {
                    "authorId": "2301582504",
                    "name": "Ruben Janssen"
                },
                {
                    "authorId": "2259931859",
                    "name": "Jay Pujara"
                },
                {
                    "authorId": "2278903766",
                    "name": "Pedro A. Szekely"
                },
                {
                    "authorId": "1719404",
                    "name": "Alessandro Moschitti"
                }
            ]
        },
        {
            "paperId": "cdce4525bc94b8b72d7330f4e26775142edd0018",
            "title": "The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models",
            "abstract": "While large language models (LLMs) are still being adopted to new domains and utilized in novel applications, we are experiencing an influx of the new generation of foundation models, namely multi-modal large language models (MLLMs). These models integrate verbal and visual information, opening new possibilities to demonstrate more complex reasoning abilities at the intersection of the two modalities. However, despite the revolutionizing prospect of MLLMs, our understanding of their reasoning abilities is limited. In this study, we assess the nonverbal abstract reasoning abilities of open-source and closed-source MLLMs using variations of Raven's Progressive Matrices. Our experiments reveal the challenging nature of such problems for MLLMs while showcasing the immense gap between open-source and closed-source models. We also uncover critical shortcomings of visual and textual perceptions, subjecting the models to low-performance ceilings. Finally, to improve MLLMs' performance, we experiment with different methods, such as Chain-of-Thought prompting, leading to a significant (up to 100%) boost in performance. Our code and datasets are available at https://github.com/usc-isi-i2/isi-mmlm-rpm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32732621",
                    "name": "Kian Ahrabian"
                },
                {
                    "authorId": "2196148017",
                    "name": "Zhivar Sourati"
                },
                {
                    "authorId": "2280276151",
                    "name": "Kexuan Sun"
                },
                {
                    "authorId": "2261394194",
                    "name": "Jiarui Zhang"
                },
                {
                    "authorId": "2280252269",
                    "name": "Yifan Jiang"
                },
                {
                    "authorId": "2258550405",
                    "name": "Fred Morstatter"
                },
                {
                    "authorId": "2259931859",
                    "name": "Jay Pujara"
                }
            ]
        },
        {
            "paperId": "ff05de1241fd5a5a2e05f7ed298e5ba9a123d7ba",
            "title": "MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning",
            "abstract": "While multi-modal large language models (MLLMs) have shown significant progress on many popular visual reasoning benchmarks, whether they possess abstract visual reasoning abilities remains an open question. Similar to the Sudoku puzzles, abstract visual reasoning (AVR) problems require finding high-level patterns (e.g., repetition constraints) that control the input shapes (e.g., digits) in a specific task configuration (e.g., matrix). However, existing AVR benchmarks only considered a limited set of patterns (addition, conjunction), input shapes (rectangle, square), and task configurations (3 by 3 matrices). To evaluate MLLMs' reasoning abilities comprehensively, we introduce MARVEL, a multidimensional AVR benchmark with 770 puzzles composed of six core knowledge patterns, geometric and abstract shapes, and five different task configurations. To inspect whether the model accuracy is grounded in perception and reasoning, MARVEL complements the general AVR question with perception questions in a hierarchical evaluation framework. We conduct comprehensive experiments on MARVEL with nine representative MLLMs in zero-shot and few-shot settings. Our experiments reveal that all models show near-random performance on the AVR question, with significant performance gaps (40%) compared to humans across all patterns and task configurations. Further analysis of perception questions reveals that MLLMs struggle to comprehend the visual features (near-random performance) and even count the panels in the puzzle (<45%), hindering their ability for abstract reasoning. We release our entire code and dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280252269",
                    "name": "Yifan Jiang"
                },
                {
                    "authorId": "2261394194",
                    "name": "Jiarui Zhang"
                },
                {
                    "authorId": "2280276151",
                    "name": "Kexuan Sun"
                },
                {
                    "authorId": "2196148017",
                    "name": "Zhivar Sourati"
                },
                {
                    "authorId": "32732621",
                    "name": "Kian Ahrabian"
                },
                {
                    "authorId": "22244290",
                    "name": "Kaixin Ma"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                },
                {
                    "authorId": "2259931859",
                    "name": "Jay Pujara"
                }
            ]
        }
    ]
}