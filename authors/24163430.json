{
    "authorId": "24163430",
    "papers": [
        {
            "paperId": "5527c8aca3e9f30b2f6382ab66a83eec2757051e",
            "title": "Construction of Knowledge Graphs: State and Challenges",
            "abstract": "With Knowledge Graphs (KGs) at the center of numerous applications such as recommender systems and question-answering, the need for generalized pipelines to construct and continuously update such KGs is increasing. While the individual steps that are necessary to create KGs from unstructured sources (e.g., text) and structured data sources (e.g., databases) are mostly well researched for their one-shot execution, their adoption for incremental KG updates and the interplay of the individual steps have hardly been investigated in a systematic manner so far. In this work, we first discuss the main graph models for KGs and introduce the major requirements for future KG construction pipelines. Next, we provide an overview of the necessary steps to build high-quality KGs, including cross-cutting topics such as metadata management, ontology development, and quality assurance. We then evaluate the state of the art of KG construction with respect to the introduced requirements for specific popular KGs, as well as some recent tools and strategies for KG construction. Finally, we identify areas in need of further research and improvement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "24163430",
                    "name": "M. Hofer"
                },
                {
                    "authorId": "46247832",
                    "name": "Daniel Obraczka"
                },
                {
                    "authorId": "119857682",
                    "name": "A. Saeedi"
                },
                {
                    "authorId": "2209205638",
                    "name": "Hanna Kopcke"
                },
                {
                    "authorId": "1747414",
                    "name": "E. Rahm"
                }
            ]
        },
        {
            "paperId": "fc92b5c32c1027922f1ab47a6140a5a42aa66530",
            "title": "Studying Linked Data Accessibility Healthiness for the Long Tail of the Data Web",
            "abstract": "In this paper, we explore the accessibility healthiness of Linked Data within the context of the Data Web, focusing on the long tail of data sources. Unlike the traditional web, Linked Data lacks a driving infrastructure to enhance accessibility, leading to negative impacts on data consumers, adoption, and the creation of large-scale infrastructures. We investigate challenges posed by issues such as link rot, unparseable content, downtime, and timeouts that hinder effective access to Linked Data. The study involves a novel Linked Data client that logs debugging information, providing insights into the efficiency and effectiveness of accessing Linked Data. The research also includes discussions on the methods and approach taken, IRI identity mismatch handling, crawling results, and Linked Data parsing statistics. Through extensive analysis of HTTP response status codes and accessibility issues, the paper quantifies common problems but also proposes methods for enhancing Linked Data accessibility in order to retrieve consistent sub-graphs from the Data Web.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "24163430",
                    "name": "M. Hofer"
                },
                {
                    "authorId": "2260895451",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "21798e23fc4af57aace2e76850daf372229ac02c",
            "title": "Towards a Systematic Approach to Sync Factual Data across Wikipedia, Wikidata and External Data Sources",
            "abstract": ". This paper addresses one of the largest and most complex data curation work\ufb02ows in existence: Wikipedia and Wikidata, with a high number of users and curators adding factual information from exter-nal sources via a non-systematic Wiki work\ufb02ow to Wikipedia\u2019s infoboxes and Wikidata items. We present high-level analyses of the current state, the challenges and limitations in this work\ufb02ow and supplement it with a quantitative and semantic analysis of the resulting data spaces by deploying DBpedia\u2019s integration and extraction capabilities. Based on an analysis of millions of references from Wikipedia infoboxes in di\ufb00erent languages, we can \ufb01nd the most important sources which can be used to enrich other knowledge bases with information of better quality. An initial tool is presented, the GlobalFactSync browser, as a prototype to discuss further measures to develop a more systematic approach for data curation in the WikiVerse.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                },
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "24163430",
                    "name": "M. Hofer"
                },
                {
                    "authorId": "1819564",
                    "name": "Milan Dojchinovski"
                },
                {
                    "authorId": "3223508",
                    "name": "Krzysztof W\u0119cel"
                },
                {
                    "authorId": "2513991",
                    "name": "W\u0142odzimierz Lewoniewski"
                }
            ]
        }
    ]
}