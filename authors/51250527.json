{
    "authorId": "51250527",
    "papers": [
        {
            "paperId": "10a0541be17d10d922ffc68a3dae55a13d9c1ab9",
            "title": "LLM is Like a Box of Chocolates: the Non-determinism of ChatGPT in Code Generation",
            "abstract": "There has been a recent explosion of research on Large Language Models (LLMs) for software engineering tasks, in particular code generation. However, results from LLMs can be highly unstable; nondeterministically returning very different codes for the same prompt. Non-determinism is a potential menace to scientific conclusion validity. When non-determinism is high, scientific conclusions simply cannot be relied upon unless researchers change their behaviour to control for it in their empirical analyses. This paper conducts an empirical study to demonstrate that non-determinism is, indeed, high, thereby underlining the need for this behavioural change. We choose to study ChatGPT because it is already highly prevalent in the code generation research literature. We report results from a study of 829 code generation problems from three code generation benchmarks (i.e., CodeContests, APPS, and HumanEval). Our results reveal high degrees of non-determinism: the ratio of coding tasks with zero equal test output across different requests is 72.73%, 60.40%, and 65.85% for CodeContests, APPS, and HumanEval, respectively. In addition, we find that setting the temperature to 0 does not guarantee determinism in code generation, although it indeed brings less non-determinism than the default configuration (temperature=1). These results confirm that there is, currently, a significant threat to scientific conclusion validity. In order to put LLM-based research on firmer scientific foundations, researchers need to take into account non-determinism in drawing their conclusions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1492047220",
                    "name": "Shuyin Ouyang"
                },
                {
                    "authorId": "51250527",
                    "name": "J Zhang"
                },
                {
                    "authorId": "145836176",
                    "name": "M. Harman"
                },
                {
                    "authorId": "2146058962",
                    "name": "Meng Wang"
                }
            ]
        },
        {
            "paperId": "148134fac202889ff6256b6b7d574cb714941887",
            "title": "Bias Testing and Mitigation in LLM-based Code Generation",
            "abstract": "Utilizing state-of-the-art Large Language Models (LLMs), automatic code generation models play a pivotal role in enhancing the productivity of software development procedures. As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social bias and unfairness, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models, yet is under-explored in the literature. This paper presents a novel bias testing framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive evaluation of the bias in code generated by five state-of-the-art LLMs. Our findings reveal that 20.29% to 44.93% code functions generated by the models under study are biased when handling bias sensitive tasks (i.e., tasks that involve sensitive attributes such as age and gender). This indicates that the existing LLMs can be unfair in code generation, posing risks of unintended and harmful software behaviors. To mitigate bias for code generation models, we evaluate five bias mitigation prompt strategies, i.e., utilizing bias testing results to refine the code (zero-shot), one-, few-shot, and two Chain-of-Thought (CoT) prompts. Our evaluation results illustrate that these strategies are all effective in mitigating bias. Overall, one-shot and few-shot learning are the two most effective. For GPT-4, 80% to 90% code bias can be removed with one-shot learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145252513",
                    "name": "Dong Huang"
                },
                {
                    "authorId": "2290184536",
                    "name": "Qingwen Bu"
                },
                {
                    "authorId": "51250527",
                    "name": "J Zhang"
                },
                {
                    "authorId": "2265090245",
                    "name": "Xiaofei Xie"
                },
                {
                    "authorId": "123878420",
                    "name": "Junjie Chen"
                },
                {
                    "authorId": "2944075",
                    "name": "Heming Cui"
                }
            ]
        },
        {
            "paperId": "21b49a149046b39a38b96d9326260ea0d32fddd3",
            "title": "Who Judges the Judge: An Empirical Study on Online Judge Tests",
            "abstract": "Online Judge platforms play a pivotal role in education, competitive programming, recruitment, career training, and large language model training. They rely on predefined test suites to judge the correctness of submitted solutions. It is therefore important that the solution judgement is reliable and free from potentially misleading false positives (i.e., incorrect solutions that are judged as correct). In this paper, we conduct an empirical study of 939 coding problems with 541,552 solutions, all of which are judged to be correct according to the test suites used by the platform, finding that 43.4% of the problems include false positive solutions (3,440 bugs are revealed in total). We also find that test suites are, nevertheless, of high quality according to widely-studied test effectiveness measurements: 88.2% of false positives have perfect (100%) line coverage, 78.9% have perfect branch coverage, and 32.5% have a perfect mutation score. Our findings indicate that more work is required to weed out false positive solutions and to further improve test suite effectiveness. We have released the detected false positive solutions and the generated test inputs to facilitate future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2279437904",
                    "name": "Kaibo Liu"
                },
                {
                    "authorId": "2152494777",
                    "name": "Yudong Han"
                },
                {
                    "authorId": "51250527",
                    "name": "J Zhang"
                },
                {
                    "authorId": "115023982",
                    "name": "Zhenpeng Chen"
                },
                {
                    "authorId": "2103653",
                    "name": "Federica Sarro"
                },
                {
                    "authorId": "145836176",
                    "name": "M. Harman"
                },
                {
                    "authorId": "2181800064",
                    "name": "Gang Huang"
                },
                {
                    "authorId": "48521633",
                    "name": "Yun Ma"
                }
            ]
        },
        {
            "paperId": "29b64e41bbea997019b64696d2374c3c96dbdf5c",
            "title": "Delving into the Adversarial Robustness of Federated Learning",
            "abstract": "In Federated Learning (FL), models are as fragile as centrally trained models against adversarial examples. However, the adversarial robustness of federated learning remains largely unexplored. This paper casts light on the challenge of adversarial robustness of federated learning. To facilitate a better understanding of the adversarial vulnerability of the existing FL methods, we conduct comprehensive robustness evaluations on various attacks and adversarial training methods. Moreover, we reveal the negative impacts induced by directly adopting adversarial training in FL, which seriously hurts the test accuracy, especially in non-IID settings. In this work, we propose a novel algorithm called Decision Boundary based Federated Adversarial Training (DBFAT), which consists of two components (local re-weighting and global regularization) to improve both accuracy and robustness of FL systems. Extensive experiments on multiple datasets demonstrate that DBFAT consistently outperforms other baselines under both IID and non-IID settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51250527",
                    "name": "J Zhang"
                },
                {
                    "authorId": "71788673",
                    "name": "Bo Li"
                },
                {
                    "authorId": null,
                    "name": "Chen Chen"
                },
                {
                    "authorId": "3366777",
                    "name": "L. Lyu"
                },
                {
                    "authorId": "2117212193",
                    "name": "Shuang Wu"
                },
                {
                    "authorId": "7406856",
                    "name": "Shouhong Ding"
                },
                {
                    "authorId": "2146290239",
                    "name": "Chao Wu"
                }
            ]
        },
        {
            "paperId": "3e284844d4cbd7258037b7b19bf3b5ad82f3f5e2",
            "title": "Noncontact Blood Pressure Estimation Using BP-Related Cardiovascular Knowledge: An Uncalibrated Method Based on Consumer-Level Camera",
            "abstract": "Objective: The tiny change of skin color, caused by a heartbeat, can be captured with consumer-level cameras by using the imaging photoplethysmography (iPPG) technique, offering a noncontact way of extracting pulse signals. Pulse signals have been demonstrated to contain information on human physiological characteristics and have been used for blood pressure (BP) estimation in recent years. According to BP-related cardiovascular knowledge, this article presents a new method for BP estimation based on the iPPG pulse signals, featured by incorporating cardiovascular characteristics including heart rate (HR), stroke volume (SV), the elasticity of vessel walls (EVW), and peripheral vascular resistance (PVR). Correlations between the systolic BP (SBP), diastolic BP (DBP), pulse pressure (PP), and cardiovascular characteristics are extracted, which facilitates the selection of pulse features consistent with BP properties. Based on the selected features, two Bayesian neural network (BNN) models are constructed for the estimation of SBP and DBP, respectively, where the machine learning (ML) uncertainty of the estimation is also evaluated. This method is uncalibrated which means it can work without additional information except for the videos from the camera. The proposed method has been tested on 220 patients with a history of cardiovascular diseases. Errors of the BP estimation are 9 \u00b1 13 (MAE \u00b1 STD) mmHg for SBP, 7 \u00b1 10 (MAE \u00b1 STD) mmHg for DBP, and the ML uncertainty of the estimation indicates the reliability of the proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2189081404",
                    "name": "Xuesong Han"
                },
                {
                    "authorId": "40282811",
                    "name": "Xuezhi Yang"
                },
                {
                    "authorId": "145771094",
                    "name": "Shuai Fang"
                },
                {
                    "authorId": "1430708893",
                    "name": "Rencheng Song"
                },
                {
                    "authorId": "2111942676",
                    "name": "Longwei Li"
                },
                {
                    "authorId": "51250527",
                    "name": "J Zhang"
                }
            ]
        },
        {
            "paperId": "646c8ebbe6a4c3c07d7f74c455b9ff60ffe2dc4a",
            "title": "Rethinking Data Distillation: Do Not Overlook Calibration",
            "abstract": "Neural networks trained on distilled data often produce over-confident output and require correction by calibration methods. Existing calibration methods such as temperature scaling and mixup work well for networks trained on original large-scale data. However, we find that these methods fail to calibrate networks trained on data distilled from large source datasets. In this paper, we show that distilled data lead to networks that are not calibratable due to (i) a more concentrated distribution of the maximum logits and (ii) the loss of information that is semantically meaningful but unrelated to classification tasks. To address this problem, we propose1 Masked Temperature Scaling (MTS) and Masked Distillation Training (MDT) which mitigate the limitations of distilled data and achieve better calibration results while maintaining the efficiency of dataset distillation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2210044747",
                    "name": "Dongyao Zhu"
                },
                {
                    "authorId": "2144399315",
                    "name": "Bowen Lei"
                },
                {
                    "authorId": "51250527",
                    "name": "J Zhang"
                },
                {
                    "authorId": "2112742987",
                    "name": "Yanbo Fang"
                },
                {
                    "authorId": "2294313542",
                    "name": "Ruqi Zhang"
                },
                {
                    "authorId": "2149183481",
                    "name": "Yiqun Xie"
                },
                {
                    "authorId": "2116459424",
                    "name": "Dongkuan Xu"
                }
            ]
        },
        {
            "paperId": "6c4dee041b3e6a2e73befdf4f950f31a49f1e882",
            "title": "SaTransformer: Semantic-aware transformer for breast cancer classification and segmentation",
            "abstract": "Breast cancer classification and segmentation play an important role in identifying and detecting benign and malignant breast lesions. However, segmentation and classification still face many challenges: 1) The characteristics of cancer itself, such as fuzzy edges, complex backgrounds, and significant changes in size, shape, and intensity distribution make accurate segment and classification challenges. 2) Existing methods ignore the potential relationship between classification and segmentation tasks, due to the classification and segmentation being treated as two separate tasks. To overcome these challenges, in this paper, a novel Semantic\u2010aware transformer (SaTransformer) for breast cancer classification and segmentation is proposed. Specifically, the SaTransformer enables doing the two takes simultaneously through one unified framework. Unlike existing well\u2010known methods, the segmentation and classification information are semantically interactive, reinforcing each other during feature representation learning and improving the ability of feature representation learning while consuming less memory and computational complexity. The SaTransformer is validated on two publicly available breast cancer datasets \u2013 BUSI and UDIAT. Experimental results and quantitative evaluations (accuracy: 97.97%, precision: 98.20%, DSC: 86.34%) demonstrate that the SaTransformer outperforms other state\u2010of\u2010the\u2010art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51250527",
                    "name": "J Zhang"
                },
                {
                    "authorId": "2229017294",
                    "name": "Zhichao Zhang"
                },
                {
                    "authorId": "153803838",
                    "name": "Hua Liu"
                },
                {
                    "authorId": "2228246405",
                    "name": "Shiqiang Xu"
                }
            ]
        },
        {
            "paperId": "6e59977d820101ea36ff7b71d7d0dfff55e8b5f3",
            "title": "Design and Fabrication of Hollow Mushroom-Like Cilia MEMS Vector Hydrophone",
            "abstract": "Vector hydrophone is the core equipment of underwater acoustic detection. Aiming at the problem of low sensitivity and short detection distance of existing vector hydrophone. In this article, a hollow mushroom-like cilia-sensitive structure is designed to optimize the hydrophone-sensitive unit. The optimal size of the hollow mushroom-like cilia MEMS vector hydrophone (MCVH) microstructure was determined by COMSOL5.6 simulation. The hollow structure is adopted, which not only ensures the working bandwidth of the hydrophone is 20\u20131000 Hz, but also improves the sensitivity of the hydrophone by increasing the receiving area of the sound wave. The test results show that the hollow structure has obvious \u201c8\u201d directivity, and the pit depth of the \u201c8\u201d directivity is larger than 40 dB at 315 Hz. Meantime, the sensitivity of MCVH can reach \u2212180.9 dB at 1000 Hz. Compared with ciliary MEMS vector hydrophone (CVH), the sensitivity is improved by 16.8 dB.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2052218905",
                    "name": "Yanan Geng"
                },
                {
                    "authorId": "2142199134",
                    "name": "Weirong Ren"
                },
                {
                    "authorId": "47776920",
                    "name": "Guojun Zhang"
                },
                {
                    "authorId": "2158171752",
                    "name": "Peng Chen"
                },
                {
                    "authorId": "2109827236",
                    "name": "Shan Zhu"
                },
                {
                    "authorId": "2108465210",
                    "name": "Yifan Zhang"
                },
                {
                    "authorId": "2156649419",
                    "name": "Yan Liu"
                },
                {
                    "authorId": "96874739",
                    "name": "Lina Jia"
                },
                {
                    "authorId": "51250527",
                    "name": "J Zhang"
                },
                {
                    "authorId": "2180536611",
                    "name": "Yansong Wang"
                },
                {
                    "authorId": "2154774155",
                    "name": "Wendong Zhang"
                }
            ]
        },
        {
            "paperId": "6fc0fa563148f768e3c18d67b405bbfbc5b907cf",
            "title": "Federated Domain Adaptation via Pseudo-label Refinement",
            "abstract": "Unsupervised domain adaptation (UDA) methods usually assume data from multiple domains can be put together for centralized adaptation. Unfortunately, this assumption impairs data privacy, which leads to the failure of traditional methods in practical scenarios. To cope with the above issue, we present a novel decentralized domain adaptation approach which conducts target adaptation in an iterative training process during which only models can be delivered across domains. More specifically, to train a promising target model, we leverage Adversarial Examples (AEs) to filter out error prone predictions of source models towards each target sample based on both robustness and confidence, and then treat the most frequent prediction as the pseudo-label. Besides, to improve central model aggregation, we introduce Knowledge Contribution (KC) to compute reasonable aggregation weights. Extensive experiments conducted on several standard datasets verify the superiority of the proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143736626",
                    "name": "Gang Li"
                },
                {
                    "authorId": "2125235212",
                    "name": "Qifei Zhang"
                },
                {
                    "authorId": "30924928",
                    "name": "Pei Wang"
                },
                {
                    "authorId": "51250527",
                    "name": "J Zhang"
                },
                {
                    "authorId": "2146290239",
                    "name": "Chao Wu"
                }
            ]
        },
        {
            "paperId": "70762a912d07f4c11dbc5d02a2eb416dd21eaa5c",
            "title": "Stealthy Backdoor Attack for Code Models",
            "abstract": "Code models, such as CodeBERT and CodeT5, offer general-purpose representations of code and play a vital role in supporting downstream automated software engineering tasks. Most recently, code models were revealed to be vulnerable to backdoor attacks. A code model that is backdoor-attacked can behave normally on clean examples but will produce pre-defined malicious outputs on examples injected with triggers that activate the backdoors. Existing backdoor attacks on code models use unstealthy and easy-to-detect triggers. This paper aims to investigate the vulnerability of code models with stealthy backdoor attacks. To this end, we propose Afraidoor (Adversarial Feature as Adaptive Backdoor). Afraidoor achieves stealthiness by leveraging adversarial perturbations to inject adaptive triggers into different inputs. We apply Afraidoor to three widely adopted code models (CodeBERT, PLBART, and CodeT5) and two downstream tasks (code summarization and method name prediction). We evaluate three widely used defense methods and find that Afraidoor is more unlikely to be detected by the defense methods than by baseline methods. More specifically, when using spectral signature as defense, around 85% of adaptive triggers in Afraidoor bypass the detection in the defense process. By contrast, only less than 12% of the triggers from previous work bypass the defense. When the defense method is not applied, both Afraidoor and baselines have almost perfect attack success rates. However, once a defense is applied, the attack success rates of baselines decrease dramatically, while the success rate of Afraidoor remains high. Our finding exposes security weaknesses in code models under stealthy backdoor attacks and shows that state-of-the-art defense methods cannot provide sufficient protection. We call for more research efforts in understanding security threats to code models and developing more effective countermeasures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2139059234",
                    "name": "Zhou Yang"
                },
                {
                    "authorId": "2203459",
                    "name": "Bowen Xu"
                },
                {
                    "authorId": "51250527",
                    "name": "J Zhang"
                },
                {
                    "authorId": "48654378",
                    "name": "Hong Jin Kang"
                },
                {
                    "authorId": "151496516",
                    "name": "Jieke Shi"
                },
                {
                    "authorId": "2158107537",
                    "name": "Junda He"
                },
                {
                    "authorId": "2150912791",
                    "name": "David Lo"
                }
            ]
        }
    ]
}