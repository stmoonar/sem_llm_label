{
    "authorId": "2064285348",
    "papers": [
        {
            "paperId": "19afd7b97061fdf0dd4a4726bef4ed8acd31a0a8",
            "title": "Corpus Poisoning via Approximate Greedy Gradient Descent",
            "abstract": "Dense retrievers are widely used in information retrieval and have also been successfully extended to other knowledge intensive areas such as language models, e.g., Retrieval-Augmented Generation (RAG) systems. Unfortunately, they have recently been shown to be vulnerable to corpus poisoning attacks in which a malicious user injects a small fraction of adversarial passages into the retrieval corpus to trick the system into returning these passages among the top-ranked results for a broad set of user queries. Further study is needed to understand the extent to which these attacks could limit the deployment of dense retrievers in real-world applications. In this work, we propose Approximate Greedy Gradient Descent (AGGD), a new attack on dense retrieval systems based on the widely used HotFlip method for efficiently generating adversarial passages. We demonstrate that AGGD can select a higher quality set of token-level perturbations than HotFlip by replacing its random token sampling with a more structured search. Experimentally, we show that our method achieves a high attack success rate on several datasets and using several retrievers, and can generalize to unseen queries and new domains. Notably, our method is extremely effective in attacking the ANCE retrieval model, achieving attack success rates that are 17.6\\% and 13.37\\% higher on the NQ and MS MARCO datasets, respectively, compared to HotFlip. Additionally, we demonstrate AGGD's potential to replace HotFlip in other adversarial attacks, such as knowledge poisoning of RAG systems.\\footnote{Code can be find in \\url{https://github.com/JinyanSu1/AGGD}}",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265989879",
                    "name": "Jinyan Su"
                },
                {
                    "authorId": "2305562387",
                    "name": "John X. Morris"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2064285348",
                    "name": "Claire Cardie"
                }
            ]
        },
        {
            "paperId": "301659c54f451ec5c963dae46d28f397a562fb53",
            "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
            "abstract": "The increasing availability of real-world conversation data offers exciting opportunities for researchers to study user-chatbot interactions. However, the sheer volume of this data makes manually examining individual conversations impractical. To overcome this challenge, we introduce WildVis, an interactive tool that enables fast, versatile, and large-scale conversation analysis. WildVis provides search and visualization capabilities in the text and embedding spaces based on a list of criteria. To manage million-scale datasets, we implemented optimizations including search index construction, embedding precomputation and compression, and caching to ensure responsive user interactions within seconds. We demonstrate WildVis' utility through three case studies: facilitating chatbot misuse research, visualizing and comparing topic distributions across datasets, and characterizing user-specific conversation patterns. WildVis is open-source and designed to be extendable, supporting additional datasets and customized search and visualization functionalities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268298457",
                    "name": "Yuntian Deng"
                },
                {
                    "authorId": "2266434757",
                    "name": "Wenting Zhao"
                },
                {
                    "authorId": "2689239",
                    "name": "Jack Hessel"
                },
                {
                    "authorId": "2228515529",
                    "name": "Xiang Ren"
                },
                {
                    "authorId": "2064285348",
                    "name": "Claire Cardie"
                },
                {
                    "authorId": "2285939518",
                    "name": "Yejin Choi"
                }
            ]
        },
        {
            "paperId": "6c94d59a5f8ab934b9259967d59101c32072f9c2",
            "title": "WildHallucinations: Evaluating Long-form Factuality in LLMs with Real-World Entity Queries",
            "abstract": "While hallucinations of large language models (LLMs) prevail as a major challenge, existing evaluation benchmarks on factuality do not cover the diverse domains of knowledge that the real-world users of LLMs seek information about. To bridge this gap, we introduce WildHallucinations, a benchmark that evaluates factuality. It does so by prompting LLMs to generate information about entities mined from user-chatbot conversations in the wild. These generations are then automatically fact-checked against a systematically curated knowledge source collected from web search. Notably, half of these real-world entities do not have associated Wikipedia pages. We evaluate 118,785 generations from 15 LLMs on 7,919 entities. We find that LLMs consistently hallucinate more on entities without Wikipedia pages and exhibit varying hallucination rates across different domains. Finally, given the same base models, adding a retrieval component only slightly reduces hallucinations but does not eliminate hallucinations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266434757",
                    "name": "Wenting Zhao"
                },
                {
                    "authorId": "2312760350",
                    "name": "Tanya Goyal"
                },
                {
                    "authorId": "2277244466",
                    "name": "Yu Ying Chiu"
                },
                {
                    "authorId": "2112504145",
                    "name": "Liwei Jiang"
                },
                {
                    "authorId": "2263557371",
                    "name": "Benjamin Newman"
                },
                {
                    "authorId": "3023068",
                    "name": "Abhilasha Ravichander"
                },
                {
                    "authorId": "2302810573",
                    "name": "K. Chandu"
                },
                {
                    "authorId": "2069676542",
                    "name": "R. L. Bras"
                },
                {
                    "authorId": "2064285348",
                    "name": "Claire Cardie"
                },
                {
                    "authorId": "2268298457",
                    "name": "Yuntian Deng"
                },
                {
                    "authorId": "2285939518",
                    "name": "Yejin Choi"
                }
            ]
        },
        {
            "paperId": "98b09c1cb63db6b3234607e0c7d1acf56459245d",
            "title": "Edward Said at Touch\u00e9: Human Value Detection Using Transformers and Upsampling",
            "abstract": "In this paper, we tackle both subtasks of the proposed shared task Human Value Classification at Touch\u00e9\u2013 that aims to classify dialogue speech into one of 19 human values determined by Schwartz\u2019s Refined Theory of Basic Individual Values. We fine-tune models like DeBERTa and RoBERTa with F1-loss to handle multi-label settings. We additionally test different sampling strategies to accommodate for data imbalance. We found that by training on the English-translated utterances, we beat the baselines by at least 2 F1 points across both subtasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2316057068",
                    "name": "Aisha Nur Aydin"
                },
                {
                    "authorId": "65877664",
                    "name": "Shaden Shaar"
                },
                {
                    "authorId": "2064285348",
                    "name": "Claire Cardie"
                }
            ]
        },
        {
            "paperId": "f4ac895ad3b7398515a3913f815e40516b197a31",
            "title": "I Could've Asked That: Reformulating Unanswerable Questions",
            "abstract": "When seeking information from unfamiliar documents, users frequently pose questions that cannot be answered by the documents. While existing large language models (LLMs) identify these unanswerable questions, they do not assist users in reformulating their questions, thereby reducing their overall utility. We curate CouldAsk, an evaluation benchmark composed of existing and new datasets for document-grounded question answering, specifically designed to study reformulating unanswerable questions. We evaluate state-of-the-art open-source and proprietary LLMs on CouldAsk. The results demonstrate the limited capabilities of these models in reformulating questions. Specifically, GPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the time, respectively. Error analysis shows that 62% of the unsuccessful reformulations stem from the models merely rephrasing the questions or even generating identical questions. We publicly release the benchmark and the code to reproduce the experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266434757",
                    "name": "Wenting Zhao"
                },
                {
                    "authorId": "2084530453",
                    "name": "Ge Gao"
                },
                {
                    "authorId": "2064285348",
                    "name": "Claire Cardie"
                },
                {
                    "authorId": "2312751781",
                    "name": "Alexander M. Rush"
                }
            ]
        },
        {
            "paperId": "03076fbfb37a21e8b546e6577e5fd0ef777e1b0b",
            "title": "Fashionpedia-Taste: A Dataset towards Explaining Human Fashion Taste",
            "abstract": "Existing fashion datasets do not consider the multi-facts that cause a consumer to like or dislike a fashion image. Even two consumers like a same fashion image, they could like this image for total different reasons. In this paper, we study the reason why a consumer like a certain fashion image. Towards this goal, we introduce an interpretability dataset, Fashionpedia-taste, consist of rich annotation to explain why a subject like or dislike a fashion image from the following 3 perspectives: 1) localized attributes; 2) human attention; 3) caption. Furthermore, subjects are asked to provide their personal attributes and preference on fashion, such as personality and preferred fashion brands. Our dataset makes it possible for researchers to build computational models to fully understand and interpret human fashion taste from different humanistic perspectives and modalities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8002897",
                    "name": "Mengyun Shi"
                },
                {
                    "authorId": "2067789287",
                    "name": "Serge J. Belongie"
                },
                {
                    "authorId": "2064285348",
                    "name": "Claire Cardie"
                }
            ]
        },
        {
            "paperId": "185b6b42fbb01ce8a5a08b64bd56a6fbe011d3dd",
            "title": "Policy-Gradient Training of Language Models for Ranking",
            "abstract": "Text retrieval plays a crucial role in incorporating factual knowledge for decision making into language processing pipelines, ranging from chat-based web search to question answering systems. Current state-of-the-art text retrieval models leverage pre-trained large language models (LLMs) to achieve competitive performance, but training LLM-based retrievers via typical contrastive losses requires intricate heuristics, including selecting hard negatives and using additional supervision as learning signals. This reliance on heuristics stems from the fact that the contrastive loss itself is heuristic and does not directly optimize the downstream metrics of decision quality at the end of the processing pipeline. To address this issue, we introduce Neural PG-RANK, a novel training algorithm that learns to rank by instantiating a LLM as a Plackett-Luce ranking policy. Neural PG-RANK provides a principled method for end-to-end training of retrieval models as part of larger decision systems via policy gradient, with little reliance on complex heuristics, and it effectively unifies the training objective with downstream decision-making quality. We conduct extensive experiments on various text retrieval benchmarks. The results demonstrate that when the training objective aligns with the evaluation setup, Neural PG-RANK yields remarkable in-domain performance improvement, with substantial out-of-domain generalization to some critical datasets employed in downstream question answering tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2084530453",
                    "name": "Ge Gao"
                },
                {
                    "authorId": "2295558747",
                    "name": "Jonathan D. Chang"
                },
                {
                    "authorId": "2064285348",
                    "name": "Claire Cardie"
                },
                {
                    "authorId": "11963742",
                    "name": "Kiant\u00e9 Brantley"
                },
                {
                    "authorId": "2243190230",
                    "name": "Thorsten Joachims"
                }
            ]
        },
        {
            "paperId": "52fc39a83e0686745a2cf92d9a2007bf18a70c82",
            "title": "Fashionpedia-Ads: Do Your Favorite Advertisements Reveal Your Fashion Taste?",
            "abstract": "Consumers are exposed to advertisements across many different domains on the internet, such as fashion, beauty, car, food, and others. On the other hand, fashion represents second highest e-commerce shopping category. Does consumer digital record behavior on various fashion ad images reveal their fashion taste? Does ads from other domains infer their fashion taste as well? In this paper, we study the correlation between advertisements and fashion taste. Towards this goal, we introduce a new dataset, Fashionpedia-Ads, which asks subjects to provide their preferences on both ad (fashion, beauty, car, and dessert) and fashion product (social network and e-commerce style) images. Furthermore, we exhaustively collect and annotate the emotional, visual and textual information on the ad images from multi-perspectives (abstractive level, physical level, captions, and brands). We open-source Fashionpedia-Ads to enable future studies and encourage more approaches to interpretability research between advertisements and fashion taste.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8002897",
                    "name": "Mengyun Shi"
                },
                {
                    "authorId": "2064285348",
                    "name": "Claire Cardie"
                },
                {
                    "authorId": "2067789287",
                    "name": "Serge J. Belongie"
                }
            ]
        },
        {
            "paperId": "5378fea4bfec4a38b242a2ff9a88295570e67ab1",
            "title": "Probing Representations for Document-level Event Extraction",
            "abstract": "The probing classifiers framework has been employed for interpreting deep neural network models for a variety of natural language processing (NLP) applications. Studies, however, have largely focused on sentencelevel NLP tasks. This work is the first to apply the probing paradigm to representations learned for document-level information extraction (IE). We designed eight embedding probes to analyze surface, semantic, and event-understanding capabilities relevant to document-level event extraction. We apply them to the representations acquired by learning models from three different LLM-based document-level IE approaches on a standard dataset. We found that trained encoders from these models yield embeddings that can modestly improve argument detections and labeling but only slightly enhance event-level tasks, albeit trade-offs in information helpful for coherence and event-type prediction. We further found that encoder models struggle with document length and cross-sentence discourse.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2160611160",
                    "name": "Barry Wang"
                },
                {
                    "authorId": "13728923",
                    "name": "Xinya Du"
                },
                {
                    "authorId": "2064285348",
                    "name": "Claire Cardie"
                }
            ]
        },
        {
            "paperId": "893960d503f3f182ddb6b15fb61032a488de87bc",
            "title": "Adapting Fake News Detection to the Era of Large Language Models",
            "abstract": "In the age of large language models (LLMs) and the widespread adoption of AI-driven content creation, the landscape of information dissemination has witnessed a paradigm shift. With the proliferation of both human-written and machine-generated real and fake news, robustly and effectively discerning the veracity of news articles has become an intricate challenge. While substantial research has been dedicated to fake news detection, this either assumes that all news articles are human-written or abruptly assumes that all machine-generated news are fake. Thus, a significant gap exists in understanding the interplay between machine-(paraphrased) real news, machine-generated fake news, human-written fake news, and human-written real news. In this paper, we study this gap by conducting a comprehensive evaluation of fake news detectors trained in various scenarios. Our primary objectives revolve around the following pivotal question: How to adapt fake news detectors to the era of LLMs? Our experiments reveal an interesting pattern that detectors trained exclusively on human-written articles can indeed perform well at detecting machine-generated fake news, but not vice versa. Moreover, due to the bias of detectors against machine-generated texts \\cite{su2023fake}, they should be trained on datasets with a lower machine-generated news ratio than the test set. Building on our findings, we provide a practical strategy for the development of robust fake news detectors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265989879",
                    "name": "Jinyan Su"
                },
                {
                    "authorId": "2064285348",
                    "name": "Claire Cardie"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        }
    ]
}