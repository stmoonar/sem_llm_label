{
    "authorId": "2158534066",
    "papers": [
        {
            "paperId": "9251d2b7b25675b46b3ba0ff4f74342637a78ee0",
            "title": "Bayesian Dumbbell Diffusion Model for RGBT Object Tracking With Enriched Priors",
            "abstract": "RGBT tracking can be accomplished by constructing Bayesian estimators that incorporate fusion prior distributions for the visible (RGB) and thermal (T) modalities. Such estimators enable the computation of a posterior distribution for the variables of interest to locate the target. Incorporating rich prior information can improve the performance of predictors. However, current RGBT trackers face limited fusion prior data. To mitigate this issue, we propose a novel tracker, BD$^{2}$ Track, which employs a diffusion model. Firstly, this letter introduces a dumbbell diffusion model, and employ convolution networks and the dumbbell model to derive the fusion feature prior information from various index frames in the same tracking video sequence. Secondly, we propose a plug-and-play channel augmented joint learning strategy to derive the images prior distribution. This strategy not only homogeneously generates modality-relevant prior information but also increases the distance between positive and negative samples within the modality, while reducing the distance between modalities during fusion. Results demonstrate promising performance in the GTOT, RGBT234, LasHeR, and VTUAV-ST datasets, surpassing other state-of-the-art trackers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192079347",
                    "name": "Shenghua Fan"
                },
                {
                    "authorId": "2158534066",
                    "name": "Chu He"
                },
                {
                    "authorId": "2223729383",
                    "name": "Chenxia Wei"
                },
                {
                    "authorId": "2025173347",
                    "name": "Yujin Zheng"
                },
                {
                    "authorId": "41192764",
                    "name": "Xi Chen"
                }
            ]
        },
        {
            "paperId": "b746b5e711ec665b64c8cc00c815e59740a6f4ea",
            "title": "A Statistical-Texture Feature Learning Network for PolSAR Image Classification",
            "abstract": "Both traditional and deep-learning-based methods have limitations in extracting statistical features from polarimetric synthetic aperture radar (PolSAR) images that contain regions with different levels of heterogeneity. To address this issue, we present a statistical-texture feature learning network (STLNet) for PolSAR image classification. Our approach includes several strategies. First, we propose a novel <inline-formula> <tex-math notation=\"LaTeX\">$N$ </tex-math></inline-formula>th-order statistical feature learning (<inline-formula> <tex-math notation=\"LaTeX\">$N$ </tex-math></inline-formula>-SL) module as the statistical modeling interface to be combined with the network. In addition, we propose a multilevel high-order statistical feature learning (MSL) module based on the <inline-formula> <tex-math notation=\"LaTeX\">$N$ </tex-math></inline-formula>-SL module to represent the statistical characteristics of PolSAR images. Second, we propose a texture feature learning (TL) module to explore the spatial relationships among pixels and supplement the learned statistical features. Experimental results on the experimental synthetic aperture radar (E-SAR) and airborne synthetic aperture radar (AIRSAR) datasets demonstrate that the proposed MSL and TL modules can effectively improve classification performance. Furthermore, STLNet outperforms other networks of comparable size.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51149162",
                    "name": "Qingyi Zhang"
                },
                {
                    "authorId": "2158534066",
                    "name": "Chu He"
                },
                {
                    "authorId": "2232625910",
                    "name": "Xiaoxiao Fang"
                },
                {
                    "authorId": "2142622581",
                    "name": "M. Tong"
                },
                {
                    "authorId": "52152402",
                    "name": "Bokun He"
                }
            ]
        },
        {
            "paperId": "eb8db286e1f6fb618ce4e1165a27a5c7ce11a0c9",
            "title": "Deep Homography Estimation With Feature Correlation Transformer",
            "abstract": "Homography estimation is an important image alignment method that has been widely used in computer vision applications. Traditional methods heavily rely on the distribution of features and usually fail in low-texture and large-baseline scenes. Most learning-based methods use convolutional neural networks(CNNs) to extract features. However, the dense features extracted in this way have a limited receptive field, leading to poor accuracy of results. In this paper, we propose a novel method for homography estimation. We first estimate the projective transformation between the reference image and the target image at a coarse level and then refine the estimated homography at the fine level. Unlike approaches that use simple CNNs or global correlations to search correspondences, we add self- and cross-attention layers in the transformer to enhance the feature correlations. The experiments show that our method significantly outperforms the existing solutions in challenging large-baseline scenes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111824941",
                    "name": "Haoyu Zhou"
                },
                {
                    "authorId": "1486300471",
                    "name": "Wei Hu"
                },
                {
                    "authorId": "1718694",
                    "name": "Yunyao Li"
                },
                {
                    "authorId": "2158534066",
                    "name": "Chu He"
                },
                {
                    "authorId": "1683647",
                    "name": "X. Chen"
                }
            ]
        },
        {
            "paperId": "b4e6c36496f5982d9ea96e79fc3f61e385afce90",
            "title": "Frequency-Dividing Downsampling Module of the Lifting Scheme for Image Classification",
            "abstract": "Convolutional neural networks(CNNs) currently dominate the field of computer vision, where the pooling layer plays an important role in reducing computational effort and avoiding overfitting. However, the commonly used methods do not design the pooling layer from the perspective of frequency. In this paper, we propose a Lifting Scheme-based frequency-dividing downsampling framework and describe a pooling layer called frequency-dividing pooling (FDP). The two branches of the Lifting Scheme process the images by frequency, which not only enhances the interpretability of the neural network but also improves the classification accuracy of the neural network. We conduct experiments on three standard datasets and the results all demonstrate that our proposed FDP is effective.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2142972161",
                    "name": "Zhichao Wang"
                },
                {
                    "authorId": "1424252789",
                    "name": "Zishan Shi"
                },
                {
                    "authorId": "2495406",
                    "name": "Dingwen Wang"
                },
                {
                    "authorId": "2158534066",
                    "name": "Chu He"
                }
            ]
        },
        {
            "paperId": "c07899fea4081fdf03311d4c0d388b8252345191",
            "title": "Enhancing Event-based Structured Light Imaging with a Single Frame",
            "abstract": "Benefiting from the extremely low latency, events have been used for Structured Light Imaging (SLI) to predict the depth surface. However, existing methods only focus on improving scanning speeds but neglect perturbations from event noise and timestamp jittering for depth estimation. In this paper, we build a hybrid SLI system equipped with an event camera, a high-resolution frame camera, and a digital light projector, where a single intensity frame is adopted as a guidance to enhance the event-based SLI quality. To achieve this end, we propose a Multi-Modal Feature Fusion Network (MFFN) consisting of a feature fusion module and an upscale module to simultaneously fuse events and a single intensity frame, suppress event perturbations, and reconstruct a high-quality depth surface. Further, for training MFFN, we build a new Structured Light Imaging based on Event and Frame cameras (EF-SLI) dataset collected from the hybrid SLI system, containing paired inputs composed of a set of synchronized events and one single corresponding frame, and ground-truth references obtained by a high-quality SLI approach. Experiments demonstrate that our proposed MFFN outperforms state-of-the-art event-based SLI approaches in terms of accuracy at different scanning speeds.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2130322643",
                    "name": "Huijiao Wang"
                },
                {
                    "authorId": "2391341",
                    "name": "Tangbo Liu"
                },
                {
                    "authorId": "2158534066",
                    "name": "Chu He"
                },
                {
                    "authorId": "2157461654",
                    "name": "Cheng Li"
                },
                {
                    "authorId": "114462250",
                    "name": "Jian-zhuo Liu"
                },
                {
                    "authorId": "2109352265",
                    "name": "Lei Yu"
                }
            ]
        }
    ]
}