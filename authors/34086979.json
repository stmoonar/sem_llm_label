{
    "authorId": "34086979",
    "papers": [
        {
            "paperId": "c115905240ac2c3e451d3fd3a93441ff68fea85e",
            "title": "SemEval-2023 Task 3: Detecting the Category, the Framing, and the Persuasion Techniques in Online News in a Multi-lingual Setup",
            "abstract": "We describe SemEval-2023 task 3 on Detecting the Category, the Framing, and the Persuasion Techniques in Online News in a Multilingual Setup: the dataset, the task organization process, the evaluation setup, the results, and the participating systems. The task focused on news articles in nine languages (six known to the participants upfront: English, French, German, Italian, Polish, and Russian), and three additional ones revealed to the participants at the testing phase: Spanish, Greek, and Georgian). The task featured three subtasks: (1) determining the genre of the article (opinion, reporting, or satire), (2) identifying one or more frames used in an article from a pool of 14 generic frames, and (3) identify the persuasion techniques used in each paragraph of the article, using a taxonomy of 23 persuasion techniques. This was a very popular task: a total of 181 teams registered to participate, and 41 eventually made an official submission on the test set.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1678833",
                    "name": "J. Piskorski"
                },
                {
                    "authorId": "2894039",
                    "name": "Nicolas Stefanovitch"
                },
                {
                    "authorId": "34086979",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "e15fabff4dcceca48ae22118b4cb13a9fa3323fb",
            "title": "Temporal Dynamics of Coordinated Online Behavior: Stability, Archetypes, and Influence",
            "abstract": "Large-scale online campaigns, malicious or otherwise, require a significant degree of coordination among participants, which sparked interest in the study of coordinated online behavior. State-of-the-art methods for detecting coordinated behavior perform static analyses, disregarding the temporal dynamics of coordination. Here, we carry out a dynamic analysis of coordinated behavior. To reach our goal, we build a multiplex temporal network and we perform dynamic community detection to identify groups of users that exhibited coordinated behaviors in time. We find that i) coordinated communities (CCs) feature variable degrees of temporal instability; ii) dynamic analyses are needed to account for such instability, and results of static analyses can be unreliable and scarcely representative of unstable communities; iii) some users exhibit distinct archetypal behaviors that have important practical implications; iv) content and network characteristics contribute to explaining why users leave and join CCs. Our results demonstrate the advantages of dynamic analyses and open up new directions of research on the unfolding of online debates, on the strategies of CCs, and on the patterns of online influence.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "41017909",
                    "name": "S. Tardelli"
                },
                {
                    "authorId": "46206525",
                    "name": "Leonardo Nizzoli"
                },
                {
                    "authorId": "2704289",
                    "name": "Maurizio Tesconi"
                },
                {
                    "authorId": "145746490",
                    "name": "M. Conti"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "34086979",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "40598011",
                    "name": "S. Cresci"
                }
            ]
        },
        {
            "paperId": "1dd25d1bc1266f9f1e088c99f8b7f22d038f4d19",
            "title": "Detecting and Understanding Harmful Memes: A Survey",
            "abstract": "The automatic identification of harmful content online is of major concern for social media platforms, policymakers, and society. Researchers have studied textual, visual, and audio content, but typically in isolation. Yet, harmful content often combines multiple modalities, as in the case of memes. With this in mind, here we offer a comprehensive survey with a focus on harmful memes. Based on a systematic analysis of recent literature, we first propose a new typology of harmful memes, and then we highlight and summarize the relevant state of the art. One interesting finding is that many types of harmful memes are not really studied, e.g., such featuring self-harm and extremism, partly due to the lack of suitable datasets. We further find that existing datasets mostly capture multi-class scenarios, which are not inclusive of the affective spectrum that memes can represent. Another observation is that memes can propagate globally through repackaging in different languages and that they can also be multilingual, blending different cultures. We conclude by highlighting several challenges related to multimodal semiotics, technological constraints, and non-trivial social engagement, and we present several open-ended aspects such as delineating online harm and empirically examining related frameworks and assistive interventions, which we believe will motivate and drive future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491627343",
                    "name": "Shivam Sharma"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "46815454",
                    "name": "Md. Shad Akhtar"
                },
                {
                    "authorId": "2105813793",
                    "name": "Dimitar I. Dimitrov"
                },
                {
                    "authorId": "34086979",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "22593971",
                    "name": "Hamed Firooz"
                },
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "144925193",
                    "name": "F. Silvestri"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "144054829",
                    "name": "Tanmoy Chakraborty"
                }
            ]
        },
        {
            "paperId": "2a75cf81f1cafbcdc99d15467bdd4b8fca9a6902",
            "title": "Overview of the CLEF-2022 CheckThat! Lab Task 1 on Identifying Relevant Claims in Tweets",
            "abstract": "We present an overview of CheckThat! lab 2022 Task 1, part of the 2022 Conference and Labs of the Evaluation Forum (CLEF). Task 1 asked to predict which posts in a Twitter stream are worth fact-checking, focusing on COVID-19 and politics in six languages: Arabic, Bulgarian, Dutch, English, Spanish, and Turkish. A total of 19 teams participated and most submissions managed to achieve sizable improvements over the baselines using Transformer-based models such as BERT and GPT-3. Across the four subtasks, approaches that targetted multiple languages (be it individually or in conjunction, in general obtained the best performance. We describe the dataset and the task setup, including the evaluation settings, and we give a brief overview of the participating systems. As usual in the CheckThat! lab, we release to the research community all datasets from the lab as well as the evaluation scripts, which should enable further research on finding relevant tweets that can help different stakeholders such as fact-checkers, journalists, and policymakers. \u00a9 2022 Copyright for this paper by its authors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "1397442049",
                    "name": "Alberto Barr\u00f3n-Cede\u00f1o"
                },
                {
                    "authorId": "34086979",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "31329752",
                    "name": "Rub\u00e9n M\u00edguez"
                },
                {
                    "authorId": "1864635",
                    "name": "Tommaso Caselli"
                },
                {
                    "authorId": "38330039",
                    "name": "Mucahid Kutlu"
                },
                {
                    "authorId": "2034351",
                    "name": "W. Zaghouani"
                },
                {
                    "authorId": "2128664093",
                    "name": "Chengkai Li"
                },
                {
                    "authorId": "65877664",
                    "name": "Shaden Shaar"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "47195288",
                    "name": "Alex Nikolov"
                },
                {
                    "authorId": "1413965894",
                    "name": "Yavuz Selim Kartal"
                }
            ]
        },
        {
            "paperId": "49edf137981ee2770560d724a40799aefce94a5d",
            "title": "Overview of the CLEF-2022 CheckThat! Lab Task 2 on Detecting Previously Fact-Checked Claims",
            "abstract": "We describe the fourth edition of the CheckThat! Lab, part of the 2022 Conference and Labs of the Evaluation Forum (CLEF). The lab evaluates technology supporting three tasks related to factuality, and it covers seven languages such as Arabic, Bulgarian, Dutch, English, German, Spanish, and Turkish. Here, we present the task 2, which asks to detect previously fact-checked claims (in two languages). A total of six teams participated in this task, submitted a total of 37 runs, and most submissions managed to achieve sizable improvements over the baselines using transformer based models such as BERT, RoBERTa. In this paper, we describe the process of data collection and the task setup, including the evaluation measures, and we give a brief overview of the participating systems. Last but not least, we release to the research community all datasets from the lab as well as the evaluation scripts, which should enable further research in detecting previously fact-checked claims. \u00a9 2022 Copyright for this paper by its authors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "34086979",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "65877664",
                    "name": "Shaden Shaar"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "1693976811",
                    "name": "Nikolay Babulkov"
                }
            ]
        },
        {
            "paperId": "7135f82cf14f0ffefcffdb45667d3c559314679d",
            "title": "Overview of the WANLP 2022 Shared Task on Propaganda Detection in Arabic",
            "abstract": "Propaganda is defined as an expression of opinion or action by individuals or groups deliberately designed to influence opinions or actions of other individuals or groups with reference to predetermined ends and this is achieved by means of well-defined rhetorical and psychological devices. Currently, propaganda (or persuasion) techniques have been commonly used on social media to manipulate or mislead social media users. Automatic detection of propaganda techniques from textual, visual, or multimodal content has been studied recently, however, major of such efforts are focused on English language content. In this paper, we propose a shared task on detecting propaganda techniques for Arabic textual content. We have done a pilot annotation of 200 Arabic tweets, which we plan to extend to 2,000 tweets, covering diverse topics. We hope that the shared task will help in building a community for Arabic propaganda detection. The dataset will be made publicly available, which can help in future studies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "2034351",
                    "name": "W. Zaghouani"
                },
                {
                    "authorId": "34086979",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "069cff8081de01bd9210b00677b77f9d0b2c869c",
            "title": "Overview of the CLEF-2021 CheckThat! Lab Task 1 on Check-Worthiness Estimation in Tweets and Political Debates",
            "abstract": "We present an overview of Task 1 of the fourth edition of the CheckThat! Lab, part of the 2021 Conference and Labs of the Evaluation Forum (CLEF). The task asks to predict which posts in a Twitter stream are worth fact-checking, focusing on COVID-19 and politics in five languages: Arabic, Bulgarian, English, Spanish, and Turkish. A total of 15 teams participated in this task and most submissions managed to achieve sizable improvements over the baselines using Transformer-based models such as BERT and RoBERTa. Here, we describe the process of data collection and the task setup, including the evaluation measures, and we give a brief overview of the participating systems. We release to the research community all datasets from the lab as well as the evaluation scripts, which should enable further research in check-worthiness estimation for tweets and political debates. \u00a9 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2121286474",
                    "name": "S. Shaar"
                },
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "2065206121",
                    "name": "Bayan Hamdan"
                },
                {
                    "authorId": "151267539",
                    "name": "Zien Sheikh Ali"
                },
                {
                    "authorId": "8685373",
                    "name": "Fatima Haouari"
                },
                {
                    "authorId": "47195288",
                    "name": "Alex Nikolov"
                },
                {
                    "authorId": "38330039",
                    "name": "Mucahid Kutlu"
                },
                {
                    "authorId": "1413965894",
                    "name": "Yavuz Selim Kartal"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "34086979",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "1397442049",
                    "name": "Alberto Barr\u00f3n-Cede\u00f1o"
                },
                {
                    "authorId": "31329752",
                    "name": "Rub\u00e9n M\u00edguez"
                },
                {
                    "authorId": "2065257134",
                    "name": "Javier Beltr\u00e1n"
                },
                {
                    "authorId": "1693370300",
                    "name": "Tamer Elsayed"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "0a9ff57a83527a7bf22f1261b8e117cbcd52ba3b",
            "title": "Overview of the CLEF-2021 CheckThat! Lab Task 2 on Detecting Previously Fact-Checked Claims in Tweets and Political Debates",
            "abstract": "We describe the fourth edition of the CheckThat! Lab, part of the 2021 Conference and Labs of the Evaluation Forum (CLEF). The lab evaluates technology supporting three tasks related to factuality, and it covers Arabic, Bulgarian, English, Spanish, and Turkish. Here, we present the task 2, which asks to detect previously fact-checked claims (in two languages). A total of four teams participated in this task, submitted a total of sixteen runs, and most submissions managed to achieve sizable improvements over the baselines using transformer based models such as BERT, RoBERTa. In this paper, we describe the process of data collection and the task setup, including the evaluation measures used, and we give a brief overview of the participating systems. Last but not least, we release to the research community all datasets from the lab as well as the evaluation scripts, which should enable further research in detecting previously fact-checked claims. \u00a9 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2121286474",
                    "name": "S. Shaar"
                },
                {
                    "authorId": "8685373",
                    "name": "Fatima Haouari"
                },
                {
                    "authorId": "2044959584",
                    "name": "Watheq Mansour"
                },
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "1693976811",
                    "name": "Nikolay Babulkov"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "34086979",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "1693370300",
                    "name": "Tamer Elsayed"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "38d243b9f6e2c786699dbc83513fb190372cde07",
            "title": "Automated Fact-Checking for Assisting Human Fact-Checkers",
            "abstract": "The reporting and the analysis of current events around the globe has expanded from professional, editor-lead journalism all the way to citizen journalism. Nowadays, politicians and other key players enjoy direct access to their audiences through social media, bypassing the filters of official cables or traditional media. However, the multiple advantages of free speech and direct communication are dimmed by the misuse of media to spread inaccurate or misleading claims. These phenomena have led to the modern incarnation of the fact-checker --- a professional whose main aim is to examine claims using available evidence and to assess their veracity. Here, we survey the available intelligent technologies that can support the human expert in the different steps of her fact-checking endeavor. These include identifying claims worth fact-checking, detecting relevant previously fact-checked claims, retrieving relevant evidence to fact-check a claim, and actually verifying a claim. In each case, we pay attention to the challenges and the potential impact on real-world fact-checking.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "1801487",
                    "name": "D. Corney"
                },
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "1693370300",
                    "name": "Tamer Elsayed"
                },
                {
                    "authorId": "2063950160",
                    "name": "A. Barr'on-Cedeno"
                },
                {
                    "authorId": "1802817",
                    "name": "Paolo Papotti"
                },
                {
                    "authorId": "65877664",
                    "name": "Shaden Shaar"
                },
                {
                    "authorId": "34086979",
                    "name": "Giovanni Da San Martino"
                }
            ]
        },
        {
            "paperId": "39d9d7088699f5879470db259d4bd281b24486e2",
            "title": "Interpretable Propaganda Detection in News Articles",
            "abstract": "Online users today are exposed to misleading and propagandistic news articles and media posts on a daily basis. To counter thus, a number of approaches have been designed aiming to achieve a healthier and safer online news and media consumption. Automatic systems are able to support humans in detecting such content; yet, a major impediment to their broad adoption is that besides being accurate, the decisions of such systems need also to be interpretable in order to be trusted and widely adopted by users. Since misleading and propagandistic content influences readers through the use of a number of deception techniques, we propose to detect and to show the use of such techniques as a way to offer interpretability. In particular, we define qualitatively descriptive features and we analyze their suitability for detecting deception techniques. We further show that our interpretable features can be easily combined with pre-trained language models, yielding state-of-the-art results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1885974",
                    "name": "Seunghak Yu"
                },
                {
                    "authorId": "34086979",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "1754057",
                    "name": "Mitra Mohtarami"
                },
                {
                    "authorId": "145898106",
                    "name": "James R. Glass"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        }
    ]
}