{
    "authorId": "66986482",
    "papers": [
        {
            "paperId": "0ac43cb23cdb84b6c7dc6986c036fb3152e9a286",
            "title": "LLM Internal States Reveal Hallucination Risk Faced With a Query",
            "abstract": "The hallucination problem of Large Language Models (LLMs) significantly limits their reliability and trustworthiness. Humans have a self-awareness process that allows us to recognize what we don't know when faced with queries. Inspired by this, our paper investigates whether LLMs can estimate their own hallucination risk before response generation. We analyze the internal mechanisms of LLMs broadly both in terms of training data sources and across 15 diverse Natural Language Generation (NLG) tasks, spanning over 700 datasets. Our empirical analysis reveals two key insights: (1) LLM internal states indicate whether they have seen the query in training data or not; and (2) LLM internal states show they are likely to hallucinate or not regarding the query. Our study explores particular neurons, activation layers, and tokens that play a crucial role in the LLM perception of uncertainty and hallucination risk. By a probing estimator, we leverage LLM self-assessment, achieving an average hallucination estimation accuracy of 84.32\\% at run time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3391272",
                    "name": "Ziwei Ji"
                },
                {
                    "authorId": "2309482588",
                    "name": "Delong Chen"
                },
                {
                    "authorId": "38524906",
                    "name": "Etsuko Ishii"
                },
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "23672613",
                    "name": "Yejin Bang"
                },
                {
                    "authorId": "150048491",
                    "name": "Bryan Wilie"
                },
                {
                    "authorId": "2265382811",
                    "name": "Pascale Fung"
                }
            ]
        },
        {
            "paperId": "1435e5405f718b9da7ea35063360f6fb3f471ea8",
            "title": "Linguistic Minimal Pairs Elicit Linguistic Similarity in Large Language Models",
            "abstract": "We introduce a novel analysis that leverages linguistic minimal pairs to probe the internal linguistic representations of Large Language Models (LLMs). By measuring the similarity between LLM activation differences across minimal pairs, we quantify the and gain insight into the linguistic knowledge captured by LLMs. Our large-scale experiments, spanning 100+ LLMs and 150k minimal pairs in three languages, reveal properties of linguistic similarity from four key aspects: consistency across LLMs, relation to theoretical categorizations, dependency to semantic context, and cross-lingual alignment of relevant phenomena. Our findings suggest that 1) linguistic similarity is significantly influenced by training data exposure, leading to higher cross-LLM agreement in higher-resource languages. 2) Linguistic similarity strongly aligns with fine-grained theoretical linguistic categories but weakly with broader ones. 3) Linguistic similarity shows a weak correlation with semantic similarity, showing its context-dependent nature. 4) LLMs exhibit limited cross-lingual alignment in their understanding of relevant linguistic phenomena. This work demonstrates the potential of minimal pairs as a window into the neural representations of language in LLMs, shedding light on the relationship between LLMs and linguistic theory.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2243582862",
                    "name": "Xinyu Zhou"
                },
                {
                    "authorId": "2243101953",
                    "name": "Delong Chen"
                },
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "2301206224",
                    "name": "Xufeng Duan"
                },
                {
                    "authorId": "2265524511",
                    "name": "Zhenguang G. Cai"
                }
            ]
        },
        {
            "paperId": "3aabd69e13f64f10fd210e4e9e6b2e75c0e734d1",
            "title": "CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark",
            "abstract": "Visual Question Answering (VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models to understand and reason on knowledge present in both visual and textual data. However, most of the current VQA models use datasets that are primarily focused on English and a few major world languages, with images that are typically Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, although these datasets often extend their linguistic range via translation or some other approaches, they usually keep images the same, resulting in narrow cultural representation. To address these limitations, we construct CVQA, a new Culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures, where we engage native speakers and cultural experts in the data collection process. As a result, CVQA includes culturally-driven images and questions from across 28 countries on four continents, covering 26 languages with 11 scripts, providing a total of 9k questions. We then benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and show that the dataset is challenging for the current state-of-the-art models. This benchmark can serve as a probing evaluation suite for assessing the cultural capability and bias of multimodal models and hopefully encourage more research efforts toward increasing cultural awareness and linguistic diversity in this field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2305602940",
                    "name": "David Romero"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "49918371",
                    "name": "Haryo Akbarianto Wibowo"
                },
                {
                    "authorId": "2298756162",
                    "name": "Teresa Lynn"
                },
                {
                    "authorId": "3248560",
                    "name": "Injy Hamed"
                },
                {
                    "authorId": "2305619033",
                    "name": "Aditya Nanda Kishore"
                },
                {
                    "authorId": "2305622255",
                    "name": "Aishik Mandal"
                },
                {
                    "authorId": "2305619182",
                    "name": "Alina Dragonetti"
                },
                {
                    "authorId": "1396213362",
                    "name": "Artem Abzaliev"
                },
                {
                    "authorId": "2148631756",
                    "name": "A. Tonja"
                },
                {
                    "authorId": "2305622331",
                    "name": "Bontu Fufa Balcha"
                },
                {
                    "authorId": "2161240241",
                    "name": "Chenxi Whitehouse"
                },
                {
                    "authorId": "51124788",
                    "name": "Christian Salamea"
                },
                {
                    "authorId": "1994718316",
                    "name": "Dan John Velasco"
                },
                {
                    "authorId": "2273673245",
                    "name": "D. Adelani"
                },
                {
                    "authorId": "70145452",
                    "name": "D. Meur"
                },
                {
                    "authorId": "2183780558",
                    "name": "Emilio Villa-Cueva"
                },
                {
                    "authorId": "2789148",
                    "name": "Fajri Koto"
                },
                {
                    "authorId": "2266756359",
                    "name": "Fauzan Farooqui"
                },
                {
                    "authorId": "1738707459",
                    "name": "Frederico Belcavello"
                },
                {
                    "authorId": "2151970366",
                    "name": "Ganzorig Batnasan"
                },
                {
                    "authorId": "2305623074",
                    "name": "Gisela Vallejo"
                },
                {
                    "authorId": "2305619231",
                    "name": "Grainne Caulfield"
                },
                {
                    "authorId": "2213060824",
                    "name": "Guido Ivetta"
                },
                {
                    "authorId": "2980506",
                    "name": "Haiyue Song"
                },
                {
                    "authorId": "2305619369",
                    "name": "Henok Biadglign Ademtew"
                },
                {
                    "authorId": "2139773809",
                    "name": "Hern\u00e1n Maina"
                },
                {
                    "authorId": "116344405",
                    "name": "Holy Lovenia"
                },
                {
                    "authorId": "2304752238",
                    "name": "Israel Abebe Azime"
                },
                {
                    "authorId": "2282499634",
                    "name": "Jan Christian Blaise Cruz"
                },
                {
                    "authorId": "1992915388",
                    "name": "Jay Gala"
                },
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "1724941617",
                    "name": "Jes\u00fas-Germ\u00e1n Ortiz-Barajas"
                },
                {
                    "authorId": "90765684",
                    "name": "Jinheon Baek"
                },
                {
                    "authorId": "2305082736",
                    "name": "Jocelyn Dunstan"
                },
                {
                    "authorId": "2276687",
                    "name": "L. A. Alemany"
                },
                {
                    "authorId": "2290013575",
                    "name": "Kumaranage Ravindu Yasas Nagasinghe"
                },
                {
                    "authorId": "2066254822",
                    "name": "Luciana Benotti"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1738707461",
                    "name": "Marcelo Viridiano"
                },
                {
                    "authorId": "2168569460",
                    "name": "Marcos Estecha-Garitagoitia"
                },
                {
                    "authorId": "2305622553",
                    "name": "Maria Camila Buitrago Cabrera"
                },
                {
                    "authorId": "2220406508",
                    "name": "Mario Rodr'iguez-Cantelar"
                },
                {
                    "authorId": "71090258",
                    "name": "M\u00e9lanie Jouitteau"
                },
                {
                    "authorId": "121947924",
                    "name": "M. Mihaylov"
                },
                {
                    "authorId": "2305619376",
                    "name": "Mohamed Fazli Mohamed Imam"
                },
                {
                    "authorId": "2191731497",
                    "name": "Muhammad Farid Adilazuarda"
                },
                {
                    "authorId": "66556569",
                    "name": "Munkhjargal Gochoo"
                },
                {
                    "authorId": "2159634278",
                    "name": "Munkh-Erdene Otgonbold"
                },
                {
                    "authorId": "1742219452",
                    "name": "Naome A. Etori"
                },
                {
                    "authorId": "2305623065",
                    "name": "Olivier Niyomugisha"
                },
                {
                    "authorId": "2307313242",
                    "name": "Paula M'onica Silva"
                },
                {
                    "authorId": "2040713514",
                    "name": "Pranjal A. Chitale"
                },
                {
                    "authorId": "3209719",
                    "name": "Raj Dabre"
                },
                {
                    "authorId": "2148764367",
                    "name": "Rendi Chevi"
                },
                {
                    "authorId": "49775305",
                    "name": "Ruochen Zhang"
                },
                {
                    "authorId": "2197070752",
                    "name": "Ryandito Diandaru"
                },
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "2305622481",
                    "name": "Santiago G'ongora"
                },
                {
                    "authorId": "8599185",
                    "name": "Soyeong Jeong"
                },
                {
                    "authorId": "152881983",
                    "name": "Sukannya Purkayastha"
                },
                {
                    "authorId": "83446147",
                    "name": "Tatsuki Kuribayashi"
                },
                {
                    "authorId": "2219413815",
                    "name": "Thanmay Jayakumar"
                },
                {
                    "authorId": "2244512282",
                    "name": "T. Torrent"
                },
                {
                    "authorId": "2305621229",
                    "name": "Toqeer Ehsan"
                },
                {
                    "authorId": "2283931820",
                    "name": "Vladimir Araujo"
                },
                {
                    "authorId": "51208524",
                    "name": "Yova Kementchedjhieva"
                },
                {
                    "authorId": "2305621242",
                    "name": "Zara Burzo"
                },
                {
                    "authorId": "2305621264",
                    "name": "Zheng Wei Lim"
                },
                {
                    "authorId": "2282475073",
                    "name": "Zheng-Xin Yong"
                },
                {
                    "authorId": "2293317558",
                    "name": "Oana Ignat"
                },
                {
                    "authorId": "2218338376",
                    "name": "Joan Nwatu"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                },
                {
                    "authorId": "1794626",
                    "name": "T. Solorio"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                }
            ]
        },
        {
            "paperId": "4c01dfec69f5d55daae00b668665e94eb9da7f39",
            "title": "LLMs Are Few-Shot In-Context Low-Resource Language Learners",
            "abstract": "In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages.Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages.Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages.Our study concludes the significance of few-shot in-context information on enhancing the low-resource understanding quality of LLMs through semantically relevant information by closing the language gap in the target language and aligning the semantics between the targeted low-resource and the high-resource language that the model is proficient in. Our work highlights the importance of advancing ICL research, particularly for low-resource languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "116344405",
                    "name": "Holy Lovenia"
                },
                {
                    "authorId": "2057151752",
                    "name": "Pascale Fung"
                }
            ]
        },
        {
            "paperId": "64e50707a5398ad0d0cb08cf4b372000f3e14562",
            "title": "Belief Revision: The Adaptability of Large Language Models Reasoning",
            "abstract": "The capability to reason from text is crucial for real-world NLP applications. Real-world scenarios often involve incomplete or evolving data. In response, individuals update their beliefs and understandings accordingly. However, most existing evaluations assume that language models (LMs) operate with consistent information. We introduce Belief-R, a new dataset designed to test LMs' belief revision ability when presented with new evidence. Inspired by how humans suppress prior inferences, this task assesses LMs within the newly proposed delta reasoning ($\\Delta R$) framework. Belief-R features sequences of premises designed to simulate scenarios where additional information could necessitate prior conclusions drawn by LMs. We evaluate $\\sim$30 LMs across diverse prompting strategies and found that LMs generally struggle to appropriately revise their beliefs in response to new information. Further, models adept at updating often underperformed in scenarios without necessary updates, highlighting a critical trade-off. These insights underscore the importance of improving LMs' adaptiveness to changing information, a step toward more reliable AI systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150048491",
                    "name": "Bryan Wilie"
                },
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "38524906",
                    "name": "Etsuko Ishii"
                },
                {
                    "authorId": "2309114644",
                    "name": "Junxian He"
                },
                {
                    "authorId": "2265382811",
                    "name": "Pascale Fung"
                }
            ]
        },
        {
            "paperId": "80945e0f3e60df0ee545caa665aad1b78123ff4a",
            "title": "What Makes for Good Image Captions?",
            "abstract": "This paper establishes a formal information-theoretic framework for image captioning, conceptualizing captions as compressed linguistic representations that selectively encode semantic units in images. Our framework posits that good image captions should balance three key aspects: informationally sufficient, minimally redundant, and readily comprehensible by humans. By formulating these aspects as quantitative measures with adjustable weights, our framework provides a flexible foundation for analyzing and optimizing image captioning systems across diverse task requirements. To demonstrate its applicability, we introduce the Pyramid of Captions (PoCa) method, which generates enriched captions by integrating local and global visual information. We present both theoretical proof that PoCa improves caption quality under certain assumptions, and empirical validation of its effectiveness across various image captioning models and datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "94210578",
                    "name": "Delong Chen"
                },
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "38524906",
                    "name": "Etsuko Ishii"
                },
                {
                    "authorId": "2299313662",
                    "name": "Ho Shu Chan"
                },
                {
                    "authorId": "23672613",
                    "name": "Yejin Bang"
                },
                {
                    "authorId": "2265382811",
                    "name": "Pascale Fung"
                }
            ]
        },
        {
            "paperId": "8ddd669fad4272223ec94f3cbdfd8d67876a60b5",
            "title": "High-Dimension Human Value Representation in Large Language Models",
            "abstract": "The widespread application of Large Language Models (LLMs) across various tasks and fields has necessitated the alignment of these models with human values and preferences. Given various approaches of human value alignment, ranging from Reinforcement Learning with Human Feedback (RLHF), to constitutional learning, etc. there is an urgent need to understand the scope and nature of human values injected into these models before their release. There is also a need for model alignment without a costly large scale human annotation effort. We propose UniVaR, a high-dimensional representation of human value distributions in LLMs, orthogonal to model architecture and training data. Trained from the value-relevant output of eight multilingual LLMs and tested on the output from four multilingual LLMs, namely LlaMA2, ChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the distribution of human values embedded in different LLMs with different langauge sources. Through UniVaR, we explore how different LLMs prioritize various values in different languages and cultures, shedding light on the complex interplay between human values and language modeling.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "94210578",
                    "name": "Delong Chen"
                },
                {
                    "authorId": "23672613",
                    "name": "Yejin Bang"
                },
                {
                    "authorId": "50824937",
                    "name": "Leila Khalatbari"
                },
                {
                    "authorId": "2258553695",
                    "name": "Bryan Wilie"
                },
                {
                    "authorId": "3391272",
                    "name": "Ziwei Ji"
                },
                {
                    "authorId": "38524906",
                    "name": "Etsuko Ishii"
                },
                {
                    "authorId": "2265382811",
                    "name": "Pascale Fung"
                }
            ]
        },
        {
            "paperId": "d46c6507a7b40920d378ac4bdf1c52b6e09d05a3",
            "title": "Subobject-level Image Tokenization",
            "abstract": "Transformer-based vision models typically tokenize images into fixed-size square patches as input units, which lacks the adaptability to image content and overlooks the inherent pixel grouping structure. Inspired by the subword tokenization widely adopted in language models, we propose an image tokenizer at a subobject level, where the subobjects are represented by semantically meaningful image segments obtained by segmentation models (e.g., segment anything models). To implement a learning system based on subobject tokenization, we first introduced a Direct Segment Anything Model (DirectSAM) that efficiently produces comprehensive segmentation of subobjects, then embed subobjects into compact latent vectors and fed them into a large language model for vision language learning. Empirical results demonstrated that our subobject-level tokenization significantly facilitates efficient learning of translating images into object and attribute descriptions compared to the traditional patch-level tokenization. Codes and models are open-sourced at https://github.com/ChenDelong1999/subobjects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "94210578",
                    "name": "Delong Chen"
                },
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "2285265252",
                    "name": "Jianfeng Liu"
                },
                {
                    "authorId": "2285362605",
                    "name": "Baoyuan Wang"
                },
                {
                    "authorId": "2057151752",
                    "name": "Pascale Fung"
                }
            ]
        },
        {
            "paperId": "e397cac3f38aba116cc623bdf1a6d638537f5e29",
            "title": "LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization",
            "abstract": "Pretrained language models (PLMs) have become remarkably adept at task and language generalization. Nonetheless, they often fail when faced with unseen languages. In this work, we present LinguAlchemy, a regularization method that incorporates various linguistic information covering typological, geographical, and phylogenetic features to align PLMs representation to the corresponding linguistic information on each language. Our LinguAlchemy significantly improves the performance of mBERT and XLM-R on low-resource languages in multiple downstream tasks such as intent classification, news classification, and semantic relatedness compared to fully finetuned models and displaying a high degree of unseen language generalization. We further introduce AlchemyScale and AlchemyTune, extension of LinguAlchemy which adjusts the linguistic regularization weights automatically, alleviating the need for hyperparameter search.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2191731497",
                    "name": "Muhammad Farid Adilazuarda"
                },
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "9162688",
                    "name": "Genta Indra Winata"
                },
                {
                    "authorId": "2257345523",
                    "name": "Ayu Purwarianti"
                }
            ]
        },
        {
            "paperId": "f04e6bb8108cf191c88c461b5f367cd3ec336ebc",
            "title": "Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages",
            "abstract": "Large language models (LLMs) show remarkable human-like capability in various domains and languages. However, a notable quality gap arises in low-resource languages, e.g., Indonesian indigenous languages, rendering them ineffective and inefficient in such linguistic contexts. To bridge this quality gap, we introduce Cendol, a collection of Indonesian LLMs encompassing both decoder-only and encoder-decoder architectures across a range of model sizes. We highlight Cendol's effectiveness across a diverse array of tasks, attaining 20% improvement, and demonstrate its capability to generalize to unseen tasks and indigenous languages of Indonesia. Furthermore, Cendol models showcase improved human favorability despite their limitations in capturing indigenous knowledge and cultural values in Indonesia. In addition, we discuss the shortcomings of parameter-efficient tunings, such as LoRA, for language adaptation. Alternatively, we propose the usage of vocabulary adaptation to enhance efficiency. Lastly, we evaluate the safety of Cendol and showcase that safety in pre-training in one language such as English is transferable to low-resource languages, such as Indonesian, even without RLHF and safety fine-tuning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "116344405",
                    "name": "Holy Lovenia"
                },
                {
                    "authorId": "2789148",
                    "name": "Fajri Koto"
                },
                {
                    "authorId": "9358635",
                    "name": "Rifki Afina Putri"
                },
                {
                    "authorId": "2242161249",
                    "name": "Emmanuel Dave"
                },
                {
                    "authorId": "2242192851",
                    "name": "Jhonson Lee"
                },
                {
                    "authorId": "2242166155",
                    "name": "Nuur Shadieq"
                },
                {
                    "authorId": "2295729963",
                    "name": "Wawan Cenggoro"
                },
                {
                    "authorId": "2242161500",
                    "name": "Salsabil Maulana Akbar"
                },
                {
                    "authorId": "2295729969",
                    "name": "Muhammad Ihza Mahendra"
                },
                {
                    "authorId": "2295845767",
                    "name": "Dea Annisayanti Putri"
                },
                {
                    "authorId": "2258553695",
                    "name": "Bryan Wilie"
                },
                {
                    "authorId": "9162688",
                    "name": "Genta Indra Winata"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "2257345523",
                    "name": "Ayu Purwarianti"
                },
                {
                    "authorId": "2057151752",
                    "name": "Pascale Fung"
                }
            ]
        }
    ]
}