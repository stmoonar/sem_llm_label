{
    "authorId": "2155700347",
    "papers": [
        {
            "paperId": "00fec110cb998e344af8fa9ddb37aca2caec4c71",
            "title": "PCQPR: Proactive Conversational Question Planning with Reflection",
            "abstract": "Conversational Question Generation (CQG) enhances the interactivity of conversational question-answering systems in fields such as education, customer service, and entertainment. However, traditional CQG, focusing primarily on the immediate context, lacks the conversational foresight necessary to guide conversations toward specified conclusions. This limitation significantly restricts their ability to achieve conclusion-oriented conversational outcomes. In this work, we redefine the CQG task as Conclusion-driven Conversational Question Generation (CCQG) by focusing on proactivity, not merely reacting to the unfolding conversation but actively steering it towards a conclusion-oriented question-answer pair. To address this, we propose a novel approach, called Proactive Conversational Question Planning with self-Refining (PCQPR). Concretely, by integrating a planning algorithm inspired by Monte Carlo Tree Search (MCTS) with the analytical capabilities of large language models (LLMs), PCQPR predicts future conversation turns and continuously refines its questioning strategies. This iterative self-refining mechanism ensures the generation of contextually relevant questions strategically devised to reach a specified outcome. Our extensive evaluations demonstrate that PCQPR significantly surpasses existing CQG methods, marking a paradigm shift towards conclusion-oriented conversational question-answering systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119113081",
                    "name": "Shasha Guo"
                },
                {
                    "authorId": "2294564280",
                    "name": "Lizi Liao"
                },
                {
                    "authorId": "2155700347",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "2287979930",
                    "name": "Cuiping Li"
                },
                {
                    "authorId": "2191043357",
                    "name": "Hong Chen"
                }
            ]
        },
        {
            "paperId": "112426cca46a7eeb7c521576fcb9df805841dfbf",
            "title": "Streamlining Redundant Layers to Compress Large Language Models",
            "abstract": "This paper introduces LLM-Streamline, a novel layer pruning approach for large language models. It is based on the observation that different layers have varying impacts on hidden states, enabling the identification of less important layers. LLMStreamline comprises two parts: layer pruning, which removes consecutive layers with the lowest importance based on target sparsity, and layer replacement, where a lightweight network is trained to replace the pruned layers to mitigate performance loss. Additionally, a new metric called\"stability\"is proposed to address the limitations of accuracy in evaluating model compression. Experiments show that LLM-Streamline surpasses previous state-of-the-art pruning methods in both accuracy and stability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293709918",
                    "name": "Xiaodong Chen"
                },
                {
                    "authorId": "2236902930",
                    "name": "Yuxuan Hu"
                },
                {
                    "authorId": "2155700347",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "2108975817",
                    "name": "Yanling Wang"
                },
                {
                    "authorId": "1625473962",
                    "name": "Cuiping Li"
                },
                {
                    "authorId": "2191043357",
                    "name": "Hong Chen"
                }
            ]
        },
        {
            "paperId": "3fd483fc22f0768049597b389ae3b48d2cffede7",
            "title": "LLMTune: Accelerate Database Knob Tuning with Large Language Models",
            "abstract": "Database knob tuning is a critical challenge in the database community, aiming to optimize knob values to enhance database performance for specific workloads. DBMS often feature hundreds of tunable knobs, posing a significant challenge for DBAs to recommend optimal configurations. Consequently, many machine learning-based tuning methods have been developed to automate this process. Despite the introduction of various optimizers, practical applications have unveiled a new problem: they typically require numerous workload runs to achieve satisfactory performance, a process that is both time-consuming and resource-intensive. This inefficiency largely stems from the optimal configuration often being substantially different from the default setting, necessitating multiple iterations during tuning. Recognizing this, we argue that an effective starting point could significantly reduce redundant exploration in less efficient areas, thereby potentially speeding up the tuning process for the optimizers. Based on this assumption, we introduce LLMTune, a large language model-based configuration generator designed to produce an initial, high-quality configuration for new workloads. These generated configurations can then serve as starting points for various base optimizers, accelerating their tuning processes. To obtain training data for LLMTune's supervised fine-tuning, we have devised a new automatic data generation framework capable of efficiently creating a large number ofpairs. We have conducted thorough experiments to evaluate LLMTune's effectiveness with different workloads, such as TPC-H and JOB. In comparison to leading methods, LLMTune demonstrates a quicker ability to identify superior configurations. For instance, with the challenging TPC-H workload, our LLMTune achieves a significant 15.6x speed-up ratio in finding the best-performing configurations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2198199708",
                    "name": "Xinmei Huang"
                },
                {
                    "authorId": "2274084217",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "2155700347",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "2297344386",
                    "name": "Xinxin Zhao"
                },
                {
                    "authorId": "2297127833",
                    "name": "Zhiming Yao"
                },
                {
                    "authorId": "2297341653",
                    "name": "Yiyan Li"
                },
                {
                    "authorId": "2164113313",
                    "name": "Zhuohao Yu"
                },
                {
                    "authorId": "2297140223",
                    "name": "Tieying Zhang"
                },
                {
                    "authorId": "2191043357",
                    "name": "Hong Chen"
                },
                {
                    "authorId": "1625473962",
                    "name": "Cuiping Li"
                }
            ]
        },
        {
            "paperId": "526d39d34aec276bb3fd339ee13f16900610d926",
            "title": "A Learn-Then-Reason Model Towards Generalization in Knowledge Base Question Answering",
            "abstract": "Large-scale knowledge bases (KBs) like Freebase and Wikidata house millions of structured knowledge. Knowledge Base Question Answering (KBQA) provides a user-friendly way to access these valuable KBs via asking natural language questions. In order to improve the generalization capabilities of KBQA models, extensive research has embraced a retrieve-then-reason framework to retrieve relevant evidence for logical expression generation. These multi-stage efforts prioritize acquiring external sources but overlook the incorporation of new knowledge into their model parameters. In effect, even advanced language models and retrievers have knowledge boundaries, thereby limiting the generalization capabilities of previous KBQA models. Therefore, this paper develops KBLLaMA, which follows a learn-then-reason framework to inject new KB knowledge into a large language model for flexible end-to-end KBQA. At the core of KBLLaMA, we study (1) how to organize new knowledge about KBQA and (2) how to facilitate the learning of the organized knowledge. Extensive experiments on various KBQA generalization tasks showcase the state-of-the-art performance of KBLLaMA. Especially on the general benchmark GrailQA and domain-specific benchmark Bio-chemical, KBLLaMA respectively derives a performance gain of up to 3.8% and 9.8% compared to the baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145402325",
                    "name": "Lingxi Zhang"
                },
                {
                    "authorId": "2155700347",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "2108975817",
                    "name": "Yanling Wang"
                },
                {
                    "authorId": "2287979930",
                    "name": "Cuiping Li"
                },
                {
                    "authorId": "2191043357",
                    "name": "Hong Chen"
                }
            ]
        },
        {
            "paperId": "8263f915cc3aec207e7ed6b9685ea7ba8bf4b4f3",
            "title": "Is Large Language Model Good at Database Knob Tuning? A Comprehensive Experimental Evaluation",
            "abstract": "Knob tuning plays a crucial role in optimizing databases by adjusting knobs to enhance database performance. However, traditional tuning methods often follow a Try-Collect-Adjust approach, proving inefficient and database-specific. Moreover, these methods are often opaque, making it challenging for DBAs to grasp the underlying decision-making process. The emergence of large language models (LLMs) like GPT-4 and Claude-3 has excelled in complex natural language tasks, yet their potential in database knob tuning remains largely unexplored. This study harnesses LLMs as experienced DBAs for knob-tuning tasks with carefully designed prompts. We identify three key subtasks in the tuning system: knob pruning, model initialization, and knob recommendation, proposing LLM-driven solutions to replace conventional methods for each subtask. We conduct extensive experiments to compare LLM-driven approaches against traditional methods across the subtasks to evaluate LLMs' efficacy in the knob tuning domain. Furthermore, we explore the adaptability of LLM-based solutions in diverse evaluation settings, encompassing new benchmarks, database engines, and hardware environments. Our findings reveal that LLMs not only match or surpass traditional methods but also exhibit notable interpretability by generating responses in a coherent ``chain-of-thought'' manner. We further observe that LLMs exhibit remarkable generalizability through simple adjustments in prompts, eliminating the necessity for additional training or extensive code modifications. Drawing insights from our experimental findings, we identify several opportunities for future research aimed at advancing the utilization of LLMs in the realm of database management.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2297341653",
                    "name": "Yiyan Li"
                },
                {
                    "authorId": "2274084217",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "2321153399",
                    "name": "Pu Zhao"
                },
                {
                    "authorId": "2155700347",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "2314516332",
                    "name": "Xinyi Zhang"
                },
                {
                    "authorId": "2273556317",
                    "name": "Tao Ji"
                },
                {
                    "authorId": "2110786066",
                    "name": "Luming Sun"
                },
                {
                    "authorId": "2287979930",
                    "name": "Cuiping Li"
                },
                {
                    "authorId": "2191043357",
                    "name": "Hong Chen"
                }
            ]
        },
        {
            "paperId": "978040c55f9094212d15b7ffadfab1440913b6cf",
            "title": "CodeS: Towards Building Open-source Language Models for Text-to-SQL",
            "abstract": "Language models have shown promising performance on the task of translating natural language questions into SQL queries (Text-to-SQL). However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source large language models (LLMs), such as ChatGPT and GPT-4, which may have the limitations of unclear model architectures, data privacy risks, and expensive inference overheads. To address the limitations, we introduce CodeS, a series of pre-trained language models with parameters ranging from 1B to 15B, specifically designed for the text-to-SQL task. CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes. This paper studies the research challenges in building CodeS. To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus. Based on this, we address the challenges of schema linking and rapid domain adaptation through strategic prompt construction and a bi-directional data augmentation technique. We conduct comprehensive evaluations on multiple datasets, including the widely used Spider benchmark, the newly released BIRD benchmark, robustness-diagnostic benchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, as well as two real-world datasets created for financial and academic applications. The experimental results show that our CodeS achieves new SOTA accuracy and robustness on nearly all challenging text-to-SQL benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2274084217",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "2155700347",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "2303287269",
                    "name": "Hanbing Liu"
                },
                {
                    "authorId": "2297832474",
                    "name": "Ju Fan"
                },
                {
                    "authorId": "2108046717",
                    "name": "Xiaokang Zhang"
                },
                {
                    "authorId": "2287070483",
                    "name": "Jun Zhu"
                },
                {
                    "authorId": "2286888373",
                    "name": "Renjie Wei"
                },
                {
                    "authorId": "2289618868",
                    "name": "Hongyan Pan"
                },
                {
                    "authorId": "1625473962",
                    "name": "Cuiping Li"
                },
                {
                    "authorId": "2191043357",
                    "name": "Hong Chen"
                }
            ]
        },
        {
            "paperId": "d37bd7aac01f8bc1b22ba74df79facb2b62be0a4",
            "title": "Open-World Semi-Supervised Learning for Node Classification",
            "abstract": "Open-world semi-supervised learning (Open-world SSL) for node classification, that classifies unlabeled nodes into seen classes or multiple novel classes, is a practical but under-explored problem in the graph community. As only seen classes have human labels, they are usually better learned than novel classes, and thus exhibit smaller intra-class variances within the embedding space (named as imbalance of intra-class variances between seen and novel classes). Based on empirical and theoretical analysis, we find the variance imbalance can negatively impact the model performance. Pre-trained feature encoders can alleviate this issue via producing compact representations for novel classes. However, creating general pre-trained encoders for various types of graph data has been proven to be challenging. As such, there is a demand for an effective method that does not rely on pre-trained graph encoders. In this paper, we propose an IMbalance-A ware method named OpenIMA for Open-world semi-supervised node classification, which trains the node classification model from scratch via contrastive learning with bias-reduced pseudo labels. Extensive experiments on seven popular graph benchmarks demonstrate the effectiveness of OpenIMA, and the source code has been available on GitHub11https://github.com/RUCKBReasoning/OpenIMA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108975817",
                    "name": "Yanling Wang"
                },
                {
                    "authorId": "2155700347",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "2145402325",
                    "name": "Lingxi Zhang"
                },
                {
                    "authorId": "2292089866",
                    "name": "Lixin Liu"
                },
                {
                    "authorId": "2292122925",
                    "name": "Yuxiao Dong"
                },
                {
                    "authorId": "1625473962",
                    "name": "Cuiping Li"
                },
                {
                    "authorId": "2191043357",
                    "name": "Hong Chen"
                },
                {
                    "authorId": "2292126832",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "f77c46d911bf8d4d8b94177dfc285dc7535c796a",
            "title": "SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation",
            "abstract": "Knowledge base question generation (KBQG) aims to generate natural language questions from a set of triplet facts extracted from KB. Existing methods have significantly boosted the performance of KBQG via pre-trained language models (PLMs) thanks to the richly endowed semantic knowledge. With the advance of pre-training techniques, large language models (LLMs) (e.g., GPT-3.5) undoubtedly possess much more semantic knowledge. Therefore, how to effectively organize and exploit the abundant knowledge for KBQG becomes the focus of our study. In this work, we propose SGSH--a simple and effective framework to Stimulate GPT-3.5 with Skeleton Heuristics to enhance KBQG. The framework incorporates\"skeleton heuristics\", which provides more fine-grained guidance associated with each input to stimulate LLMs to generate optimal questions, encompassing essential elements like the question phrase and the auxiliary verb.More specifically, we devise an automatic data construction strategy leveraging ChatGPT to construct a skeleton training dataset, based on which we employ a soft prompting approach to train a BART model dedicated to generating the skeleton associated with each input. Subsequently, skeleton heuristics are encoded into the prompt to incentivize GPT-3.5 to generate desired questions. Extensive experiments demonstrate that SGSH derives the new state-of-the-art performance on the KBQG tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119113081",
                    "name": "Shasha Guo"
                },
                {
                    "authorId": "2294564280",
                    "name": "Lizi Liao"
                },
                {
                    "authorId": "2155700347",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "2108975817",
                    "name": "Yanling Wang"
                },
                {
                    "authorId": "2287979930",
                    "name": "Cuiping Li"
                },
                {
                    "authorId": "2191043357",
                    "name": "Hong Chen"
                }
            ]
        },
        {
            "paperId": "1d9e822ad395bbbfcc5505d5c7c27a84ca5777bc",
            "title": "Transformer Compression via Subspace Projection",
            "abstract": "We propose TCSP, a novel method for compressing a transformer model by focusing on reducing the hidden size of the model. By projecting the whole transform model into a subspace, we enable matrix operations between the weight matrices in the model and features in a reduced-dimensional space, leading to significant reductions in model parameters and computing resources. To establish this sub-space, we decompose the feature matrix, derived from different layers of sampled data instances, into a projection matrix. For evaluation, TCSP is applied to compress T5 and BERT models on the GLUE and SQuAD benchmarks. Experimental results demonstrate that TCSP achieves a compression ratio of 44% with at most 1.6% degradation in accuracy, surpassing or matching prior compression methods. Furthermore, TCSP exhibits compatibility with other methods targeting filter and attention head size compression.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2236902930",
                    "name": "Yuxuan Hu"
                },
                {
                    "authorId": "2155700347",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "50015989",
                    "name": "Chengliang Zhao"
                },
                {
                    "authorId": "2287979930",
                    "name": "Cuiping Li"
                },
                {
                    "authorId": "2273086226",
                    "name": "Hong Chen"
                }
            ]
        },
        {
            "paperId": "2f9221877030c28cf98f0847ff8b8e787377b9a6",
            "title": "A Generation-based Deductive Method for Math Word Problems",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2236902930",
                    "name": "Yuxuan Hu"
                },
                {
                    "authorId": "2155700347",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "2274084217",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "1625473962",
                    "name": "Cuiping Li"
                },
                {
                    "authorId": "2273086226",
                    "name": "Hong Chen"
                }
            ]
        }
    ]
}