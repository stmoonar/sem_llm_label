{
    "authorId": "2257230025",
    "papers": [
        {
            "paperId": "57a8333365bf99b65591e6b2176eacf8fd85d5da",
            "title": "Language Models as Science Tutors",
            "abstract": "NLP has recently made exciting progress toward training language models (LMs) with strong scientific problem-solving skills. However, model development has not focused on real-life use-cases of LMs for science, including applications in education that require processing long scientific documents. To address this, we introduce TutorEval and TutorChat. TutorEval is a diverse question-answering benchmark consisting of questions about long chapters from STEM textbooks, written by experts. TutorEval helps measure real-life usability of LMs as scientific assistants, and it is the first benchmark combining long contexts, free-form generation, and multi-disciplinary scientific knowledge. Moreover, we show that fine-tuning base models with existing dialogue datasets leads to poor performance on TutorEval. Therefore, we create TutorChat, a dataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to fine-tune Llemma models with 7B and 34B parameters. These LM tutors specialized in math have a 32K-token context window, and they excel at TutorEval while performing strongly on GSM8K and MATH. Our datasets build on open-source materials, and we release our models, data, and evaluations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284703052",
                    "name": "Alexis Chevalier"
                },
                {
                    "authorId": "2308103990",
                    "name": "Jiayi Geng"
                },
                {
                    "authorId": "2127066887",
                    "name": "Alexander Wettig"
                },
                {
                    "authorId": "2284724648",
                    "name": "Howard Chen"
                },
                {
                    "authorId": "2284686496",
                    "name": "Sebastian Mizera"
                },
                {
                    "authorId": "2284685543",
                    "name": "Toni Annala"
                },
                {
                    "authorId": "2284685834",
                    "name": "Max Jameson Aragon"
                },
                {
                    "authorId": "2284682678",
                    "name": "Arturo Rodr'iguez Fanlo"
                },
                {
                    "authorId": "2127069744",
                    "name": "Simon Frieder"
                },
                {
                    "authorId": "2284683132",
                    "name": "Simon Machado"
                },
                {
                    "authorId": "2309244668",
                    "name": "Akshara Prabhakar"
                },
                {
                    "authorId": "103432855",
                    "name": "Ellie Thieu"
                },
                {
                    "authorId": "2284732304",
                    "name": "Jiachen T. Wang"
                },
                {
                    "authorId": "2260308709",
                    "name": "Zirui Wang"
                },
                {
                    "authorId": "2284683822",
                    "name": "Xindi Wu"
                },
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "2284681788",
                    "name": "Wenhan Jia"
                },
                {
                    "authorId": "2257230025",
                    "name": "Jiatong Yu"
                },
                {
                    "authorId": "2284821642",
                    "name": "Jun-Jie Zhu"
                },
                {
                    "authorId": "2153387689",
                    "name": "Z. Ren"
                },
                {
                    "authorId": "2283134097",
                    "name": "Sanjeev Arora"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                }
            ]
        },
        {
            "paperId": "6f217d984f36499d88ab8a3d89572171552e6f3f",
            "title": "Evaluating Large Language Models at Evaluating Instruction Following",
            "abstract": "As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these ``LLM evaluators'', particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150468823",
                    "name": "Zhiyuan Zeng"
                },
                {
                    "authorId": "2257230025",
                    "name": "Jiatong Yu"
                },
                {
                    "authorId": "4800645",
                    "name": "Tianyu Gao"
                },
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                },
                {
                    "authorId": "2257034822",
                    "name": "Tanya Goyal"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                }
            ]
        }
    ]
}