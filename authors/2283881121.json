{
    "authorId": "2283881121",
    "papers": [
        {
            "paperId": "4dcefca05cd194746e4e5813636fc54ecffd186f",
            "title": "ChemVLM: Exploring the Power of Multimodal Large Language Models in Chemistry Area",
            "abstract": "Large Language Models (LLMs) have achieved remarkable success and have been applied across various scientific fields, including chemistry. However, many chemical tasks require the processing of visual information, which cannot be successfully handled by existing chemical LLMs. This brings a growing need for models capable of integrating multimodal information in the chemical domain. In this paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal large language model specifically designed for chemical applications. ChemVLM is trained on a carefully curated bilingual multimodal dataset that enhances its ability to understand both textual and visual chemical information, including molecular structures, reactions, and chemistry examination questions. We develop three datasets for comprehensive evaluation, tailored to Chemical Optical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and Multimodal Molecule Understanding tasks. We benchmark ChemVLM against a range of open-source and proprietary multimodal large language models on various tasks. Experimental results demonstrate that ChemVLM achieves competitive performance across all evaluated tasks. Our model can be found at https://huggingface.co/AI4Chem/ChemVLM-26B.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109383573",
                    "name": "Junxian Li"
                },
                {
                    "authorId": "2283952275",
                    "name": "Di Zhang"
                },
                {
                    "authorId": "2316089170",
                    "name": "Xunzhi Wang"
                },
                {
                    "authorId": "2316114521",
                    "name": "Zeying Hao"
                },
                {
                    "authorId": "2316236029",
                    "name": "Jingdi Lei"
                },
                {
                    "authorId": "2283844222",
                    "name": "Qian Tan"
                },
                {
                    "authorId": "2316088348",
                    "name": "Cai Zhou"
                },
                {
                    "authorId": "2283881121",
                    "name": "Wei Liu"
                },
                {
                    "authorId": "2190474418",
                    "name": "Weiyun Wang"
                },
                {
                    "authorId": "2305731793",
                    "name": "Zhe Chen"
                },
                {
                    "authorId": "2257133501",
                    "name": "Wenhai Wang"
                },
                {
                    "authorId": "2316055701",
                    "name": "Wei Li"
                },
                {
                    "authorId": "2249003500",
                    "name": "Shufei Zhang"
                },
                {
                    "authorId": "2248285678",
                    "name": "Mao Su"
                },
                {
                    "authorId": "2283918605",
                    "name": "Wanli Ouyang"
                },
                {
                    "authorId": "2248091979",
                    "name": "Yuqiang Li"
                },
                {
                    "authorId": "2116324147",
                    "name": "Dongzhan Zhou"
                }
            ]
        },
        {
            "paperId": "6eb23df05166c772e4c2fbfb0113de0beabd1a43",
            "title": "Large Language Models are In-Context Molecule Learners",
            "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid Context Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. Additionally, we also propose Post-retrieval Re-ranking with Sequence Reversal and Random Walk to further improve the quality of retrieval results. Finally, In-Context Molecule Tuning unlocks the in-context molecule learning capability of LLMs with retrieved examples and adapts the parameters of LLMs for the molecule-caption translation task. Experimental results demonstrate that ICMT can empower LLMs to achieve state-of-the-art or comparable performance without extra training corpora and intricate structures, showing that LLMs are inherently in-context molecule learners.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2259858493",
                    "name": "Jiatong Li"
                },
                {
                    "authorId": "2283881121",
                    "name": "Wei Liu"
                },
                {
                    "authorId": "2290248755",
                    "name": "Zhihao Ding"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2248091979",
                    "name": "Yuqiang Li"
                },
                {
                    "authorId": "2290317675",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "9f0bac228c616236c9fb8c25fbee817b1599a929",
            "title": "ChemLLM: A Chemical Large Language Model",
            "abstract": "Large language models (LLMs) have made impressive progress in chemistry applications. However, the community lacks an LLM specifically designed for chemistry. The main challenges are two-fold: firstly, most chemical data and scientific knowledge are stored in structured databases, which limits the model's ability to sustain coherent dialogue when used directly. Secondly, there is an absence of objective and fair benchmark that encompass most chemistry tasks. Here, we introduce ChemLLM, a comprehensive framework that features the first LLM dedicated to chemistry. It also includes ChemData, a dataset specifically designed for instruction tuning, and ChemBench, a robust benchmark covering nine essential chemistry tasks. ChemLLM is adept at performing various tasks across chemical disciplines with fluid dialogue interaction. Notably, ChemLLM achieves results comparable to GPT-4 on the core chemical tasks and demonstrates competitive performance with LLMs of similar size in general scenarios. ChemLLM paves a new path for exploration in chemical studies, and our method of incorporating structured chemical knowledge into dialogue systems sets a new standard for developing LLMs in various scientific fields. Codes, Datasets, and Model weights are publicly accessible at https://hf.co/AI4Chem",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283952275",
                    "name": "Di Zhang"
                },
                {
                    "authorId": "2283881121",
                    "name": "Wei Liu"
                },
                {
                    "authorId": "2283844222",
                    "name": "Qian Tan"
                },
                {
                    "authorId": "2284033855",
                    "name": "Jingdan Chen"
                },
                {
                    "authorId": "2283885582",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2283880395",
                    "name": "Yuliang Yan"
                },
                {
                    "authorId": "2198621140",
                    "name": "Jiatong Li"
                },
                {
                    "authorId": "8007867",
                    "name": "Weiran Huang"
                },
                {
                    "authorId": "2283845000",
                    "name": "Xiangyu Yue"
                },
                {
                    "authorId": "2116324147",
                    "name": "Dongzhan Zhou"
                },
                {
                    "authorId": "2249003500",
                    "name": "Shufei Zhang"
                },
                {
                    "authorId": "2248285678",
                    "name": "Mao Su"
                },
                {
                    "authorId": "2284252048",
                    "name": "Han-sen Zhong"
                },
                {
                    "authorId": "2248091979",
                    "name": "Yuqiang Li"
                },
                {
                    "authorId": "2283918605",
                    "name": "Wanli Ouyang"
                }
            ]
        }
    ]
}