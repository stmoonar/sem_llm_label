{
    "authorId": "2468810",
    "papers": [
        {
            "paperId": "95b7e3e3275f4af6d612177df7a22ba529ba4d58",
            "title": "Adaptive Indexing for In-situ Visual Exploration and Analytics",
            "abstract": "In-situ processing has received a great deal of attention in recent years. In in-situ scenarios, big raw data files which do not fit in main memory, must be efficiently handled using commodity hardware, without the overhead of a preprocessing phase or the loading of data into a database. In this work, we present an adaptive indexing scheme that enables efficient visual exploration and analytics over big raw data files. Beyond visual exploration and statistics, the scheme enables categorical-based analytics using group-by and filter operations. The proposed scheme combines a tile-based structure that offers efficient exploratory operations over the 2D space, with a tree-based structure that organizes a tile\u2019s objects based on their categorical values, enabling efficient visual analytics and the support of advanced visualization meth-ods. The index resides in main memory and is built progressively as the user explores parts of the raw file, whereas its structure and level of granularity are adjusted to the user\u2019s exploration areas and type of analysis. We conduct experiments using real and synthetic datasets, and demonstrate that the proposed approach, is in most cases more than 40 \u00d7 faster compared to the existing solutions, and performs around 3 orders of magnitude less I/O operations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51133690",
                    "name": "Stavros Maroulis"
                },
                {
                    "authorId": "2269685",
                    "name": "Nikos Bikakis"
                },
                {
                    "authorId": "2325154",
                    "name": "George Papastefanatos"
                },
                {
                    "authorId": "144996383",
                    "name": "Panos Vassiliadis"
                },
                {
                    "authorId": "2468810",
                    "name": "Y. Vassiliou"
                }
            ]
        },
        {
            "paperId": "b66758e1a3392c1543ba9ed22b6484d35ba5260e",
            "title": "RawVis: A System for Efficient In-situ Visual Analytics",
            "abstract": "In-situ processing has received a great deal of attention in recent years. In in-situ scenarios, big raw data files which do not fit in main memory, must be efficiently handled on-the-fly using commodity hardware, without the overhead of a preprocessing phase or the loading of data into a database system. This paper presents RawVis, an open source data visualization system for in-situ visual exploration and analytics over big raw data. RawVis implements novel indexing schemes and adaptive processing techniques allowing users to perform efficient visual and analytics operations directly over the data files. RawVis provides real-time interaction, reporting low response time, over large data files, using commodity hardware.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51133690",
                    "name": "Stavros Maroulis"
                },
                {
                    "authorId": "2269685",
                    "name": "Nikos Bikakis"
                },
                {
                    "authorId": "2325154",
                    "name": "George Papastefanatos"
                },
                {
                    "authorId": "144996383",
                    "name": "Panos Vassiliadis"
                },
                {
                    "authorId": "2468810",
                    "name": "Y. Vassiliou"
                }
            ]
        },
        {
            "paperId": "f09efa0bbebda72d2d8730e8aeb0c4eb38cc0b00",
            "title": "Evo-Path: Querying Data Evolution through Complex Changes",
            "abstract": "Evo-graph is a model for data evolution that captures data versions and treats changes as first-class citizens. A change in evo-graph can be compound, comprising disparate changes, and is associated with the data items it affects. In previous work, we specified how an evo-graph can be reduced to a snapshot holding under a specific time instance, we presented an XML representation of evo-graph called evoXML, we defined how evo-graph is constructed as the current snapshot evolves, as well as presented and evaluated the C2D framework that implements these concepts using XML technologies. In this paper, we formally define evo-path, an XPath extension for querying the data history and change structure in a uniform way over evograph. We specify the evo-path syntax, semantics and implementation, and present several query categories.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3333840",
                    "name": "Theodora Galani"
                },
                {
                    "authorId": "1794648",
                    "name": "Y. Stavrakas"
                },
                {
                    "authorId": "2325154",
                    "name": "George Papastefanatos"
                },
                {
                    "authorId": "2468810",
                    "name": "Y. Vassiliou"
                }
            ]
        },
        {
            "paperId": "3a6634ca3fda9c009bfa5389178dad2344739836",
            "title": "Ranking Papers by their Short-Term Scientific Impact",
            "abstract": "The constantly increasing rate at which scientific papers are published makes it difficult for researchers to identify papers that currently impact the research field of their interest. In this work, we present a method that ranks papers based on their estimated short-term impact, as measured by the number of citations received in the near future. Our method models a researcher exploring the paper citation network, and introduces an attention-based mechanism, akin to a time-restricted version of preferential attachment, that explicitly captures the researcher\u2019s preference to read papers which received a lot of attention recently. A detailed experimental evaluation on real citation datasets across disciplines, shows that our approach is more effective than previous work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1637421061",
                    "name": "Ilias Kanellos"
                },
                {
                    "authorId": "1768540",
                    "name": "Thanasis Vergoulis"
                },
                {
                    "authorId": "1760642",
                    "name": "Dimitris Sacharidis"
                },
                {
                    "authorId": "7974521",
                    "name": "Theodore Dalamagas"
                },
                {
                    "authorId": "2468810",
                    "name": "Y. Vassiliou"
                }
            ]
        },
        {
            "paperId": "81b44bcca6b6a7819fc5668cf3fa143fc452dd5b",
            "title": "Efficient Representation of Very Large Linked Datasets as Graphs",
            "abstract": "Large linked datasets are nowadays available on many scientific topics of interest and offer invaluable knowledge. These datasets are of interest to a wide audience, people with limited or no knowledge about the Semantic Web, that want to explore and analyse this information in a user-friendly way. Aiming to support such usage, systems have been developed that support such exploration they impose however many limitations as they provide to users access to a limited part of the input dataset either by aggregating information or by exploiting data formats, such as hierarchies. As more linked datasets are becoming available and more people are interested to explore them, it is imperative to provide an user-friendly way to access and explore diverse and very large datasets in an intuitive way, as graphs. We present here an off-line pre-processing technique, divided in three phases, that can transform any linked dataset, independently of size and characteristics to one continuous graph in the two-dimensional space. We store the spatial information of the graph, add the needed indices and provide the graphical information through a dedicated API to support the exploration of the information. Finally, we conduct an experimental analysis to show that our technique can process and represent as one continuous graph large and diverse datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1564589559",
                    "name": "Maria Krommyda"
                },
                {
                    "authorId": "1713548",
                    "name": "Verena Kantere"
                },
                {
                    "authorId": "2468810",
                    "name": "Y. Vassiliou"
                }
            ]
        },
        {
            "paperId": "4730b3380f7095877fa07fe6adb28558c2cd10bc",
            "title": "Divide and Conquer Technique for Large Linked Datasets",
            "abstract": "Significant effort has been dedicated in recent years to the exploitation of very large linked datasets due to the importance of the information they contain and the increase of their availability. Some techniques have been developed that handle the volume of these datasets by aggregating their information based on the data structure or model. Other approaches exploit specific characteristics of the datasets, such as semantic annotations, to present the information to the users in semantically defined ways. In an era that the volume and diversity of the available information increases exponentially and more users are interested in exploring it, it is crucial to provide a technique that will allow the exploitation of diverse and very large linked datasets in a scalable way independent of any characteristics of the input dataset. We present here a generic divide and conquer technique that can offer the required scalability to exploit any input dataset regardless its size and characteristics. The proposed technique has been tested in the context of interactive representation of very large linked datasets as graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1564589559",
                    "name": "Maria Krommyda"
                },
                {
                    "authorId": "1713548",
                    "name": "Verena Kantere"
                },
                {
                    "authorId": "2468810",
                    "name": "Y. Vassiliou"
                }
            ]
        },
        {
            "paperId": "4b1c6b942adfd5f771032bb79f1c5ab7f21311fd",
            "title": "Impact-Based Ranking of Scientific Publications: A Survey and Experimental Evaluation",
            "abstract": "As the rate at which scientific work is published continues to increase, so does the need to discern high-impact publications. In recent years, there have been several approaches that seek to rank publications based on their expected citation-based impact. Despite this level of attention, this research area has not been systematically studied. Past literature often fails to distinguish between short-term impact, the current popularity of an article, and long-term impact, the overall influence of an article. Moreover, the evaluation methodologies applied vary widely and are inconsistent. In this work, we aim to fill these gaps, studying impact-based ranking theoretically and experimentally. First, we provide explicit definitions for short-term and long-term impact, and introduce the associated ranking problems. Then, we identify and classify the most important ideas employed by state-of-the-art methods. After studying various evaluation methodologies of the literature, we propose a specific benchmark framework that can help us better differentiate effectiveness across impact aspects. Using this framework we investigate: (1) the practical difference between ranking by short- and long-term impact, and (2) the effectiveness and efficiency of ranking methods in different settings. To avoid reporting results that are discipline-dependent, we perform our experiments using four datasets from different scientific disciplines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1637421061",
                    "name": "Ilias Kanellos"
                },
                {
                    "authorId": "1768540",
                    "name": "Thanasis Vergoulis"
                },
                {
                    "authorId": "1760642",
                    "name": "Dimitris Sacharidis"
                },
                {
                    "authorId": "7974521",
                    "name": "Theodore Dalamagas"
                },
                {
                    "authorId": "2468810",
                    "name": "Y. Vassiliou"
                }
            ]
        },
        {
            "paperId": "e015086503650e655be932a9d20a4c668470660d",
            "title": "IVLG: Interactive Visualization of Large Graphs",
            "abstract": "There has been significant effort in recent years to explore and navigate very large linked datasets, due to the increase of their availability. Many techniques have been developed that extract the information from such datasets and present it to the user as diagrams, while others take advantage of the hierarchies of the datasets to filter and aggregate them, allowing the users to access specific information. In order to overcome the limitations regarding the volume of the presented information, we have developed a novel technique that enables the interactive visualization as one continuous graph of datasets with millions of elements. IVLG is a fully fledged prototype system that implements this technique based on a client-server architecture, enabling many users to have concurrently access to the information through a user-friendly interface. It allows the user to navigate the dataset through different levels of abstraction and locate information using innovative exploration techniques. A carefully designed storage schema along with an API that takes advantage of the appropriate indexing handles datasets with millions of elements without raising any performance issues, even when accessed from devices with limited computational resources. The proposed demonstration showcases the advantages and technical features of IVLG. A series of demonstration scenarios will show how IVLG can adapt accordingly and handle diverse real and synthetic datasets that vary on size and average node degree and how the system functionalities support the user experience and the exploration of the information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1564589559",
                    "name": "Maria Krommyda"
                },
                {
                    "authorId": "1713548",
                    "name": "Verena Kantere"
                },
                {
                    "authorId": "2468810",
                    "name": "Y. Vassiliou"
                }
            ]
        },
        {
            "paperId": "d0dcb5c5991c6c2fb76749c8d927caad2e412337",
            "title": "Evaluation of Diversification Techniques for Legal Information Retrieval",
            "abstract": "\u201cPublic legal information from all countries and international institutions is part of the common heritage of humanity. Maximizing access to this information promotes justice and the rule of law\u201d. In accordance with the aforementioned declaration on free access to law by legal information institutes of the world, a plethora of legal information is available through the Internet, while the provision of legal information has never before been easier. Given that law is accessed by a much wider group of people, the majority of whom are not legally trained or qualified, diversification techniques should be employed in the context of legal information retrieval, as to increase user satisfaction. We address the diversification of results in legal search by adopting several state of the art methods from the web search, network analysis and text summarization domains. We provide an exhaustive evaluation of the methods, using a standard dataset from the common law domain that we objectively annotated with relevance judgments for this purpose. Our results: (i) reveal that users receive broader insights across the results they get from a legal information retrieval system; (ii) demonstrate that web search diversification techniques outperform other approaches (e.g., summarization-based, graph-based methods) in the context of legal diversification; and (iii) offer balance boundaries between reinforcing relevant documents or sampling the information space around the legal query.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2019631",
                    "name": "Marios Koniaris"
                },
                {
                    "authorId": "2000554088",
                    "name": "Ioannis Anagnostopoulos"
                },
                {
                    "authorId": "2468810",
                    "name": "Y. Vassiliou"
                }
            ]
        },
        {
            "paperId": "360ce6b9cf8079859370d992c5d95a5072992b82",
            "title": "Towards Automatic Structuring and Semantic Indexing of Legal Documents",
            "abstract": "Over the last years there has been a great increase on the number of freely available legal resources. Portals that allow users to search for legislation, using keywords are now a common place. However, in the vast majority of those portals, legal documents are not stored in a structured format with a rich set of meta data, but in presentation oriented manifestation, making impossible for the end users to inquiry semantics about the documents, such as date of enactment, date of repeal, jurisdiction, etc. or to reuse information and establish an interconnection with similar repositories. In this paper, we present an approach for extracting a machine readable semantic representation of legislation, from unstructured document formats. Our method exploits common formats of legal documents to identify blocks of structural and semantic information and models them according to a popular legal meta-schema. Our proposed method is highly extensible and achieves high accuracy for a variety of legal and para legal documents, especially legislation. Our evaluation results reveal that our methodology can be of great assistance for the automatic structuring and semantic indexing of legal resources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2019631",
                    "name": "Marios Koniaris"
                },
                {
                    "authorId": "2325154",
                    "name": "George Papastefanatos"
                },
                {
                    "authorId": "2468810",
                    "name": "Y. Vassiliou"
                }
            ]
        }
    ]
}