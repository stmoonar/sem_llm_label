{
    "authorId": "1712027",
    "papers": [
        {
            "paperId": "3a55568a1d1a7e37faad1ca4e259a98e3f97027f",
            "title": "3MVRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding",
            "abstract": "This paper presents a groundbreaking multimodal, multi-task, multi-teacher joint-grained knowledge distillation model for visually-rich form document understanding. The model is designed to leverage insights from both fine-grained and coarse-grained levels by facilitating a nuanced correlation between token and entity representations, addressing the complexities inherent in form documents. Additionally, we introduce new inter-grained and cross-grained loss functions to further refine diverse multi-teacher knowledge distillation transfer process, presenting distribution gaps and a harmonised understanding of form documents. Through a comprehensive evaluation across publicly available form document understanding datasets, our proposed model consistently outperforms existing baselines, showcasing its efficacy in handling the intricate structures and content of visually complex form documents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2166956722",
                    "name": "Yihao Ding"
                },
                {
                    "authorId": "30079913",
                    "name": "Lorenzo Vaiani"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                },
                {
                    "authorId": "2282598754",
                    "name": "Jean Lee"
                },
                {
                    "authorId": "1712027",
                    "name": "P. Garza"
                },
                {
                    "authorId": "144179461",
                    "name": "Josiah Poon"
                },
                {
                    "authorId": "2237783897",
                    "name": "Luca Cagliero"
                }
            ]
        },
        {
            "paperId": "d7516e72c34705968582476ab14a7d66c5308984",
            "title": "Density-Based Clustering by Means of Bridge Point Identification",
            "abstract": "Density-based clustering focuses on defining clusters consisting of contiguous regions characterized by similar densities of points. Traditional approaches identify core points first, whereas more recent ones initially identify the cluster borders and then propagate cluster labels within the delimited regions. Both strategies encounter issues in presence of multi-density regions or when clusters are characterized by noisy borders. To overcome the above issues, we present a new clustering algorithm that relies on the concept of bridge point. A bridge point is a point whose neighborhood includes points of different clusters. The key idea is to use bridge points, rather than border points, to partition points into clusters. We have proved that a correct bridge point identification yields a cluster separation consistent with the expectation. To correctly identify bridge points in absence of a priori cluster information we leverage an established unsupervised outlier detection algorithm. Specifically, we empirically show that, in most cases, the detected outliers are actually a superset of the bridge point set. Therefore, to define clusters we spread cluster labels like a wildfire until an outlier, acting as a candidate bridge point, is reached. The proposed algorithm performs statistically better than state-of-the-art methods on a large set of benchmark datasets and is particularly robust to the presence of intra-cluster multiple densities and noisy borders.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2006458457",
                    "name": "Luca Colomba"
                },
                {
                    "authorId": "1692878",
                    "name": "Luca Cagliero"
                },
                {
                    "authorId": "1712027",
                    "name": "P. Garza"
                }
            ]
        },
        {
            "paperId": "efc91996e4722c5b5e81885a5b91026e05354531",
            "title": "PoliTo at SemEval-2023 Task 1: CLIP-based Visual-Word Sense Disambiguation Based on Back-Translation",
            "abstract": "Visual-Word Sense Disambiguation (V-WSD) entails resolving the linguistic ambiguity in a text by selecting a clarifying image from a set of (potentially misleading) candidates. In this paper, we address V-WSD using a state-of-the-art Image-Text Retrieval system, namely CLIP. We propose to alleviate the linguistic ambiguity across multiple domains and languages via text and image augmentation. To augment the textual content we rely on back-translation with the aid of a variety of auxiliary languages. The approach based on finetuning CLIP on the full phrases is effective in accurately disambiguating words and incorporating back-translation enhance the system\u2019s robustness and performance on the test samples written in Indo-European languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30079913",
                    "name": "Lorenzo Vaiani"
                },
                {
                    "authorId": "1692878",
                    "name": "Luca Cagliero"
                },
                {
                    "authorId": "1712027",
                    "name": "P. Garza"
                }
            ]
        },
        {
            "paperId": "08e7d7d63fa9e1ab4f99883142aa8e48f3582be1",
            "title": "How Much Attention Should we Pay to Mosquitoes?",
            "abstract": "Mosquitoes are a major global health problem. They are responsible for the transmission of diseases and can have a large impact on local economies. Monitoring mosquitoes is therefore helpful in preventing the outbreak of mosquito-borne diseases. In this paper, we propose a novel data-driven approach that leverages Transformer-based models for the identification of mosquitoes in audio recordings. The task aims at detecting the time intervals corresponding to the acoustic mosquito events in an audio signal. We formulate the problem as a sequence tagging task and train a Transformer-based model using a real-world dataset collecting mosquito recordings. By leveraging the sequential nature of mosquito recordings, we formulate the training objective so that the input recordings do not require fine-grained annotations. We show that our approach is able to outperform baseline methods using standard evaluation metrics, albeit suffering from unexpectedly high false negatives detection rates. In view of the achieved results, we propose future directions for the design of more effective mosquito detection models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "147574346",
                    "name": "Moreno La Quatra"
                },
                {
                    "authorId": "30079913",
                    "name": "Lorenzo Vaiani"
                },
                {
                    "authorId": "1909140421",
                    "name": "Alkis Koudounas"
                },
                {
                    "authorId": "1692878",
                    "name": "Luca Cagliero"
                },
                {
                    "authorId": "1712027",
                    "name": "P. Garza"
                },
                {
                    "authorId": "1750105",
                    "name": "Elena Baralis"
                }
            ]
        },
        {
            "paperId": "183d20b5ef29f7c389b395d607efb42fe6463105",
            "title": "A Dataset for Burned Area Delineation and Severity Estimation from Satellite Imagery",
            "abstract": "The ability to correctly identify areas damaged by forest wildfires is essential to plan and monitor the restoration process and estimate the environmental damages after such catastrophic events. The wide availability of satellite data, combined with the recent development of machine learning and deep learning methodologies applied to the computer vision field, makes it extremely interesting to apply the aforementioned techniques to the field of automatic burned area detection. One of the main issues in such a context is the limited amount of labeled data, especially in the context of semantic segmentation. In this paper, we introduce a publicly available dataset for the burned area detection problem for semantic segmentation. The dataset contains 73 satellite images of different forests damaged by wildfires across Europe with a resolution of up to 10m per pixel. Data were collected from the Sentinel-2 L2A satellite mission and the target labels were generated from the Copernicus Emergency Management Service (EMS) annotations, with five different severity levels, ranging from undamaged to completely destroyed. Finally, we report the benchmark values obtained by applying a Convolutional Neural Network on the proposed dataset to address the burned area identification problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2006458457",
                    "name": "Luca Colomba"
                },
                {
                    "authorId": "67262517",
                    "name": "Alessandro Farasin"
                },
                {
                    "authorId": "2059009044",
                    "name": "Simone Monaco"
                },
                {
                    "authorId": "2149942497",
                    "name": "Salvatore Greco"
                },
                {
                    "authorId": "1712027",
                    "name": "P. Garza"
                },
                {
                    "authorId": "2585766",
                    "name": "D. Apiletti"
                },
                {
                    "authorId": "1750105",
                    "name": "Elena Baralis"
                },
                {
                    "authorId": "1684646",
                    "name": "T. Cerquitelli"
                }
            ]
        },
        {
            "paperId": "429b0a009e18d7568d8dcc4dd574bd2be0dc661c",
            "title": "Vision Transformers for Burned Area Delineation",
            "abstract": "The automatic identification of burned areas is an important task that was mainly managed manually or semi-automatically in the past. In the last years, thanks to the availability of novel deep neural network architectures, automatic segmentation solutions have been proposed also in the emergency management domain. The most recent works in burned area delineation leverage on Convolutional Neural Networks (CNNs) to automatically identify regions that were previously affected by forest wildfires. A largely adopted segmentation model, U-Net, demonstrated good performances for the task under analysis, but in some cases a high overestimation of burned areas is given, leading to low precision scores. Given the recent advances in the field of NLP and the first successes also in the vision domain, in this paper we investigate the adoption of vision transformers for semantic segmentation to address the burned area identification task. In particular, we explore the SegFormer architecture with two of its variants: the smallest MiT-B0 and the intermediate one MiT-B3. The experimental results show that SegFormer provides better predictions, with higher precision and F1 score, but also better performance in terms of the number of parameters with respect to CNNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2205886540",
                    "name": "Daniele Rege Cambrin"
                },
                {
                    "authorId": "2006458457",
                    "name": "Luca Colomba"
                },
                {
                    "authorId": "1712027",
                    "name": "P. Garza"
                }
            ]
        },
        {
            "paperId": "a0037427f50003e3b799101106247b156d682563",
            "title": "Leveraging Explainable AI to Support Cryptocurrency Investors",
            "abstract": "In the last decade, cryptocurrency trading has attracted the attention of private and professional traders and investors. To forecast the financial markets, algorithmic trading systems based on Artificial Intelligence (AI) models are becoming more and more established. However, they suffer from the lack of transparency, thus hindering domain experts from directly monitoring the fundamentals behind market movements. This is particularly critical for cryptocurrency investors, because the study of the main factors influencing cryptocurrency prices, including the characteristics of the blockchain infrastructure, is crucial for driving experts\u2019 decisions. This paper proposes a new visual analytics tool to support domain experts in the explanation of AI-based cryptocurrency trading systems. To describe the rationale behind AI models, it exploits an established method, namely SHapley Additive exPlanations, which allows experts to identify the most discriminating features and provides them with an interactive and easy-to-use graphical interface. The simulations carried out on 21 cryptocurrencies over a 8-year period demonstrate the usability of the proposed tool.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1750925166",
                    "name": "Jacopo Fior"
                },
                {
                    "authorId": "1692878",
                    "name": "Luca Cagliero"
                },
                {
                    "authorId": "1712027",
                    "name": "P. Garza"
                }
            ]
        },
        {
            "paperId": "b70e5de5694fa7572a71b29de91300b844d04dff",
            "title": "Mining spatiotemporally invariant patterns",
            "abstract": "Discovering patterns that represent key spatial or temporal dependencies among data is a well-known exploratory data mining task. However, prior works either separately analyze spatial and temporal dependencies or discover joint spatiotemporal properties of specific trajectories observed over a region of interest. With the goal of generalizing the information provided by spatiotemporal patterns, in this paper we extract sequences of discrete events showing spatiotemporally invariant properties. We seek patterns whose corresponding instances in the source data differ only due to an invariant spatiotemporal transformation. We denote such a new type of patterns as SpatioTemporally Invariant. We also propose an efficient algorithm to mine STInvs and validate its efficiency and effectiveness on real data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2006458457",
                    "name": "Luca Colomba"
                },
                {
                    "authorId": "1692878",
                    "name": "Luca Cagliero"
                },
                {
                    "authorId": "1712027",
                    "name": "P. Garza"
                }
            ]
        },
        {
            "paperId": "c7874323c4b463e9a9399c0a57f5969fd3f0b272",
            "title": "Transformer-based Non-Verbal Emotion Recognition: Exploring Model Portability across Speakers' Genders",
            "abstract": "Recognizing emotions in non-verbal audio tracks requires a deep understanding of their underlying features. Traditional classifiers relying on excitation, prosodic, and vocal traction features are not always capable of effectively generalizing across speakers' genders. In the ComParE 2022 vocalisation sub-challenge we explore the use of a Transformer architecture trained on contrastive audio examples. We leverage augmented data to learn robust non-verbal emotion classifiers. We also investigate the impact of different audio transformations, including neural voice conversion, on the classifier capability to generalize across speakers' genders. The empirical findings indicate that neural voice conversion is beneficial in the pretraining phase, yielding an improved model generality, whereas is harmful at the finetuning stage as hinders model specialization for the task of non-verbal emotion recognition.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30079913",
                    "name": "Lorenzo Vaiani"
                },
                {
                    "authorId": "1909140421",
                    "name": "Alkis Koudounas"
                },
                {
                    "authorId": "147574346",
                    "name": "Moreno La Quatra"
                },
                {
                    "authorId": "1692878",
                    "name": "Luca Cagliero"
                },
                {
                    "authorId": "1712027",
                    "name": "P. Garza"
                },
                {
                    "authorId": "1750105",
                    "name": "Elena Baralis"
                }
            ]
        },
        {
            "paperId": "dcc0a7cd8894de0ccfd7a8f28c38858b7b946075",
            "title": "Leveraging multimodal content for podcast summarization",
            "abstract": "Podcasts are becoming an increasingly popular way to share streaming audio content. Podcast summarization aims at improving the accessibility of podcast content by automatically generating a concise summary consisting of text/audio extracts. Existing approaches either extract short audio snippets by means of speech summarization techniques or produce abstractive summaries of the speech transcription disregarding the podcast audio. To leverage the multimodal information hidden in podcast episodes we propose an end-to-end architecture for extractive summarization that encodes both acoustic and textual contents. It learns how to attend relevant multimodal features using an ad hoc, deep feature fusion network. The experimental results achieved on a real benchmark dataset show the benefits of integrating audio encodings into the extractive summarization process. The quality of the generated summaries is superior to those achieved by existing extractive methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30079913",
                    "name": "Lorenzo Vaiani"
                },
                {
                    "authorId": "147574346",
                    "name": "Moreno La Quatra"
                },
                {
                    "authorId": "1692878",
                    "name": "Luca Cagliero"
                },
                {
                    "authorId": "1712027",
                    "name": "P. Garza"
                }
            ]
        }
    ]
}