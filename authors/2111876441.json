{
    "authorId": "2111876441",
    "papers": [
        {
            "paperId": "a0173909e4515d6bd8de58752f17a457ba003667",
            "title": "Seeing Is Not Always Believing: Invisible Collision Attack and Defence on Pre-Trained Models",
            "abstract": "Large-scale pre-trained models (PTMs) such as BERT and GPT have achieved great success in diverse fields. The typical paradigm is to pre-train a big deep learning model on large-scale data sets, and then fine-tune the model on small task-specific data sets for downstream tasks. Although PTMs have rapidly progressed with wide real-world applications, they also pose significant risks of potential attacks. Existing backdoor attacks or data poisoning methods often build up the assumption that the attacker invades the computers of victims or accesses the target data, which is challenging in real-world scenarios. In this paper, we propose a novel framework for an invisible attack on PTMs with enhanced MD5 collision. The key idea is to generate two equal-size models with the same MD5 checksum by leveraging the MD5 chosen-prefix collision. Afterwards, the two ``same\"models will be deployed on public websites to induce victims to download the poisoned model. Unlike conventional attacks on deep learning models, this new attack is flexible, covert, and model-independent. Additionally, we propose a simple defensive strategy for recognizing the MD5 chosen-prefix collision and provide a theoretical justification for its feasibility. We extensively validate the effectiveness and stealthiness of our proposed attack and defensive method on different models and data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152100364",
                    "name": "Minghan Deng"
                },
                {
                    "authorId": "103010588",
                    "name": "Zhong Zhang"
                },
                {
                    "authorId": "2111876441",
                    "name": "Junming Shao"
                }
            ]
        },
        {
            "paperId": "bf8c7ca2c4f66607b7d99e3598a6b03397af764b",
            "title": "Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models",
            "abstract": "Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs. Motivated by the observation, in this paper, we study the problem of re-parameterizing and fine-tuning PLMs from a new perspective: Discovery of intrinsic task-specific subspace. Specifically, by exploiting the dynamics of the fine-tuning process for a given task, the parameter optimization trajectory is learned to uncover its intrinsic task-specific subspace. A key finding is that PLMs can be effectively fine-tuned in the subspace with a small number of free parameters. Beyond, we observe some outlier dimensions emerging during fine-tuning in the subspace. Disabling these dimensions degrades the model performance significantly. This suggests that these dimensions are crucial to induce task-specific knowledge to downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "103010588",
                    "name": "Zhong Zhang"
                },
                {
                    "authorId": "2116441692",
                    "name": "Bang Liu"
                },
                {
                    "authorId": "2111876441",
                    "name": "Junming Shao"
                }
            ]
        },
        {
            "paperId": "68e2e56ca6c317639047c5d542f3438d04706766",
            "title": "Enhancing Sindhi Word Segmentation using Subword Representation Learning and Position-aware Self-attention",
            "abstract": "Sindhi word segmentation is a challenging task due to space omission and insertion issues. The Sindhi language itself adds to this complexity. It's cursive and consists of characters with inherent joining and non-joining properties, independent of word boundaries. Existing Sindhi word segmentation methods rely on designing and combining hand-crafted features. However, these methods have limitations, such as difficulty handling out-of-vocabulary words, limited robustness for other languages, and inefficiency with large amounts of noisy or raw text. Neural network-based models, in contrast, can automatically capture word boundary information without requiring prior knowledge. In this paper, we propose a Subword-Guided Neural Word Segmenter (SGNWS) that addresses word segmentation as a sequence labeling task. The SGNWS model incorporates subword representation learning through a bidirectional long short-term memory encoder, position-aware self-attention, and a conditional random field. Our empirical results demonstrate that the SGNWS model achieves state-of-the-art performance in Sindhi word segmentation on six datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "115681118",
                    "name": "Wazir Ali"
                },
                {
                    "authorId": "144296649",
                    "name": "J. Kumar"
                },
                {
                    "authorId": "1683510",
                    "name": "Zenglin Xu"
                },
                {
                    "authorId": "1830449995",
                    "name": "Congjian Luo"
                },
                {
                    "authorId": null,
                    "name": "Junyu Lu"
                },
                {
                    "authorId": "2111876441",
                    "name": "Junming Shao"
                },
                {
                    "authorId": "2247212194",
                    "name": "Rajesh Kumar"
                },
                {
                    "authorId": "2041980",
                    "name": "Yazhou Ren"
                }
            ]
        }
    ]
}