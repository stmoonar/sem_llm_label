{
    "authorId": "2462276",
    "papers": [
        {
            "paperId": "0202576db14ba2b09c1df9669b240e1204dd2cd5",
            "title": "Mastering Context-to-Label Representation Transformation for Event Causality Identification with Diffusion Models",
            "abstract": "To understand event structures of documents, event causality identification (ECI) emerges as a crucial task, aiming to discern causal relationships among event mentions. The latest approach for ECI has introduced advanced deep learning models where transformer-based encoding models, complemented by enriching components, are typically leveraged to learn effective event context representations for causality prediction. As such, an important step for ECI models is to transform the event context representations into causal label representations to perform logits score computation for training and inference purposes. Within this framework, event context representations might encapsulate numerous complicated and noisy structures due to the potential long context between the input events while causal label representations are intended to capture pure information about the causal relations to facilitate score estimation. Nonetheless, a notable drawback of existing ECI models stems from their reliance on simple feed-forward networks to handle the complex context-to-label representation transformation process, which might require drastic changes in the representations to hinder the learning process. To overcome this issue, our work introduces a novel method for ECI where, instead abrupt transformations, event context representations are gradually updated to achieve effective label representations. This process will be done incrementally to allow filtering of irrelevant structures at varying levels of granularity for causal relations. To realize this, we present a diffusion model to learn gradual representation transition processes between context and causal labels. It operates through a forward pass for causal label representation noising and a reverse pass for reconstructing label representations from random noise. Our experiments on different datasets across multiple languages demonstrate the advantages of the diffusion model with state-of-the-art performance for ECI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2027979466",
                    "name": "Hieu Man"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2293544012",
                    "name": "Thien Huu Nguyen"
                }
            ]
        },
        {
            "paperId": "09d75de9475a516e0d24ca09931b4269480a9453",
            "title": "BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English",
            "abstract": "In this study, we introduce BEnQA, a dataset comprising parallel Bengali and English exam questions for middle and high school levels in Bangladesh. Our dataset consists of approximately 5K questions covering several subjects in science with different types of questions, including factual, application, and reasoning-based questions. We benchmark several Large Language Models (LLMs) with our parallel dataset and observe a notable performance disparity between the models in Bengali and English. We also investigate some prompting methods, and find that Chain-of-Thought prompting is beneficial mostly on reasoning questions, but not so much on factual ones. We also find that appending English translation helps to answer questions in Bengali. Our findings point to promising future research directions for improving the performance of LLMs in Bengali and more generally in low-resource languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292040447",
                    "name": "H. M. Q. H. Sheikh Shafayat"
                },
                {
                    "authorId": "2292090396",
                    "name": "Minhajur Rahman"
                },
                {
                    "authorId": "2292040215",
                    "name": "Chowdhury Mahim"
                },
                {
                    "authorId": "2292034518",
                    "name": "Rifki Afina"
                },
                {
                    "authorId": "2292036957",
                    "name": "James Putri"
                },
                {
                    "authorId": "2292036951",
                    "name": "Alice Thorne"
                },
                {
                    "authorId": "2292033561",
                    "name": "Oh"
                },
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "35831406",
                    "name": "Rishav Hada"
                },
                {
                    "authorId": "104014275",
                    "name": "Millicent Ochieng"
                },
                {
                    "authorId": "2292042313",
                    "name": "Prachi Jain"
                },
                {
                    "authorId": "2135014756",
                    "name": "Harshita Diddee"
                },
                {
                    "authorId": "2212460959",
                    "name": "Samuel Maina"
                },
                {
                    "authorId": "1785978",
                    "name": "T. Ganu"
                },
                {
                    "authorId": "2242814985",
                    "name": "Sameer Segal"
                },
                {
                    "authorId": "2212463954",
                    "name": "Maxamed Axmed"
                },
                {
                    "authorId": "3086996",
                    "name": "Kalika Bali"
                },
                {
                    "authorId": "2279986783",
                    "name": "Jonathan H. Choi"
                },
                {
                    "authorId": "2279921858",
                    "name": "Kristin E. Hickman"
                },
                {
                    "authorId": "2247943406",
                    "name": "Amy Monahan"
                },
                {
                    "authorId": "2265493391",
                    "name": "Peter Clark"
                },
                {
                    "authorId": "3390191",
                    "name": "Isaac Cowhey"
                },
                {
                    "authorId": "2282542651",
                    "name": "O. Etzioni"
                },
                {
                    "authorId": "2236429",
                    "name": "Tushar Khot"
                },
                {
                    "authorId": "48229640",
                    "name": "Ashish Sabharwal"
                },
                {
                    "authorId": "3393851",
                    "name": "Carissa Schoenick"
                },
                {
                    "authorId": "2292036520",
                    "name": "Oyvind Tafjord. 2018"
                },
                {
                    "authorId": "6062736",
                    "name": "K. Cobbe"
                },
                {
                    "authorId": "13622184",
                    "name": "V. Kosaraju"
                },
                {
                    "authorId": "2275251620",
                    "name": "Mo Bavarian"
                },
                {
                    "authorId": "2292054802",
                    "name": "Mark Chen"
                },
                {
                    "authorId": "35450887",
                    "name": "Heewoo Jun"
                },
                {
                    "authorId": "2275230678",
                    "name": "Lukasz Kaiser"
                },
                {
                    "authorId": "3407285",
                    "name": "Matthias Plappert"
                },
                {
                    "authorId": "2065005836",
                    "name": "Jerry Tworek"
                },
                {
                    "authorId": "2286540856",
                    "name": "Jacob Hilton"
                },
                {
                    "authorId": "7406311",
                    "name": "Reiichiro Nakano"
                },
                {
                    "authorId": "2789148",
                    "name": "Fajri Koto"
                },
                {
                    "authorId": "2256987672",
                    "name": "Nurul Aisyah"
                },
                {
                    "authorId": "2282714065",
                    "name": "Haonan Li"
                },
                {
                    "authorId": "2076041174",
                    "name": "Tiffany H. Kung"
                },
                {
                    "authorId": "40315563",
                    "name": "Morgan Cheatham"
                },
                {
                    "authorId": "2036654064",
                    "name": "Arielle Medenilla"
                },
                {
                    "authorId": "2205202770",
                    "name": "Czarina Sillos"
                },
                {
                    "authorId": "2292036703",
                    "name": "Lorie De Leon"
                },
                {
                    "authorId": "2205195617",
                    "name": "Camille Elepa\u00f1o"
                },
                {
                    "authorId": "2205195620",
                    "name": "Maria Madriaga"
                },
                {
                    "authorId": "2205195054",
                    "name": "Rimel Aggabao"
                },
                {
                    "authorId": "2205233740",
                    "name": "Giezel Diaz-Candido"
                },
                {
                    "authorId": "2205202767",
                    "name": "James Maningo"
                },
                {
                    "authorId": "2292035884",
                    "name": "V. Dac"
                },
                {
                    "authorId": "2292022310",
                    "name": "Trung Ngo"
                },
                {
                    "authorId": "2285453048",
                    "name": "Amir Pouran"
                },
                {
                    "authorId": "2292035577",
                    "name": "Ben Vey-seh"
                },
                {
                    "authorId": "2062898274",
                    "name": "Hieu Man"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2265648617",
                    "name": "Trung Bui"
                },
                {
                    "authorId": "2165171109",
                    "name": "T. Huu"
                },
                {
                    "authorId": "2272980805",
                    "name": "ChatGPT"
                }
            ]
        },
        {
            "paperId": "1f34873286e81605c5629b7abb6b873f36ba39e6",
            "title": "Retrieval Augmented Generation for Domain-specific Question Answering",
            "abstract": "Question answering (QA) has become an important application in the advanced development of large language models. General pre-trained large language models for question-answering are not trained to properly understand the knowledge or terminology for a specific domain, such as finance, healthcare, education, and customer service for a product. To better cater to domain-specific understanding, we build an in-house question-answering system for Adobe products. We propose a novel framework to compile a large question-answer database and develop the approach for retrieval-aware finetuning of a Large Language model. We showcase that fine-tuning the retriever leads to major improvements in the final generation. Our overall approach reduces hallucinations during generation while keeping in context the latest retrieval information for contextual grounding.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109948508",
                    "name": "Sanat Sharma"
                },
                {
                    "authorId": "2297848239",
                    "name": "David Seunghyun Yoon"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "27097973",
                    "name": "Dewang Sultania"
                },
                {
                    "authorId": "2297846982",
                    "name": "Karishma Bagga"
                },
                {
                    "authorId": "2298190476",
                    "name": "Mengjiao Zhang"
                },
                {
                    "authorId": "2265648617",
                    "name": "Trung Bui"
                },
                {
                    "authorId": "2297847280",
                    "name": "Varun Kotte"
                }
            ]
        },
        {
            "paperId": "27cc05d2ad2f48123db8fb6b38690862b34ac75c",
            "title": "Large Generative Graph Models",
            "abstract": "Large Generative Models (LGMs) such as GPT, Stable Diffusion, Sora, and Suno are trained on a huge amount of language corpus, images, videos, and audio that are extremely diverse from numerous domains. This training paradigm over diverse well-curated data lies at the heart of generating creative and sensible content. However, all previous graph generative models (e.g., GraphRNN, MDVAE, MoFlow, GDSS, and DiGress) have been trained only on one dataset each time, which cannot replicate the revolutionary success achieved by LGMs in other fields. To remedy this crucial gap, we propose a new class of graph generative model called Large Graph Generative Model (LGGM) that is trained on a large corpus of graphs (over 5000 graphs) from 13 different domains. We empirically demonstrate that the pre-trained LGGM has superior zero-shot generative capability to existing graph generative models. Furthermore, our pre-trained LGGM can be easily fine-tuned with graphs from target domains and demonstrate even better performance than those directly trained from scratch, behaving as a solid starting point for real-world customization. Inspired by Stable Diffusion, we further equip LGGM with the capability to generate graphs given text prompts (Text-to-Graph), such as the description of the network name and domain (i.e.,\"The power-1138-bus graph represents a network of buses in a power distribution system.\"), and network statistics (i.e.,\"The graph has a low average degree, suitable for modeling social media interactions.\"). This Text-to-Graph capability integrates the extensive world knowledge in the underlying language model, offering users fine-grained control of the generated graphs. We release the code, the model checkpoint, and the datasets at https://lggm-lg.github.io/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284900711",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2238208116",
                    "name": "Ryan Rossi"
                },
                {
                    "authorId": "2268675415",
                    "name": "Namyong Park"
                },
                {
                    "authorId": "2305588553",
                    "name": "Huiyuan Chen"
                },
                {
                    "authorId": "144741751",
                    "name": "Nesreen K. Ahmed"
                },
                {
                    "authorId": "30440868",
                    "name": "Puja Trivedi"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2479152",
                    "name": "Danai Koutra"
                },
                {
                    "authorId": "2290558635",
                    "name": "Tyler Derr"
                }
            ]
        },
        {
            "paperId": "3ea087a9a409bf16e49e0b7f8d444e7d91870f2d",
            "title": "An Analysis of Multilingual FActScore",
            "abstract": "FActScore has gained popularity as a metric to estimate the factuality of long-form texts generated by Large Language Models (LLMs) in English. However, there has not been any work in studying the behavior of FActScore in other languages. This paper studies the limitations of each component in the four-component pipeline of FActScore in the multilingual setting. We introduce a new dataset for FActScore on texts generated by strong multilingual LLMs. Our evaluation shows that LLMs exhibit distinct behaviors in both fact extraction and fact scoring tasks. No LLM produces consistent and reliable FActScore across languages with varying levels of resources. We also find that the knowledge source plays an important role in the quality of the estimated FActScore. Using Wikipedia as the knowledge source may hinder the true FActScore of long-form text due to its limited coverage in medium- and low-resource languages. We also incorporate three mitigations to our knowledge source that ultimately improve FActScore estimation across all languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2309004134",
                    "name": "Kim Trong Vu"
                },
                {
                    "authorId": "18171842",
                    "name": "Michael Krumdick"
                },
                {
                    "authorId": "2266430123",
                    "name": "Varshini Reddy"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2266397553",
                    "name": "V. Lai"
                }
            ]
        },
        {
            "paperId": "414b69509dfeb8449f6dcd7039cf4aeac2604065",
            "title": "Hierarchical Selection of Important Context for Generative Event Causality Identification with Optimal Transports",
            "abstract": "We study the problem of Event Causality Identification (ECI) that seeks to predict causal relation between event mentions in the text. In contrast to previous classification-based models, a few recent ECI methods have explored generative models to deliver state-of-the-art performance. However, such generative models cannot handle document-level ECI where long context between event mentions must be encoded to secure correct predictions. In addition, previous generative ECI methods tend to rely on external toolkits or human annotation to obtain necessary training signals. To address these limitations, we propose a novel generative framework that leverages Optimal Transport (OT) to automatically select the most important sentences and words from full documents. Specifically, we introduce hierarchical OT alignments between event pairs and the document to extract pertinent contexts. The selected sentences and words are provided as input and output to a T5 encoder-decoder model which is trained to generate both the causal relation label and salient contexts. This allows richer supervision without external tools. We conduct extensive evaluations on different datasets with multiple languages to demonstrate the benefits and state-of-the-art performance of ECI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2027979466",
                    "name": "Hieu Man"
                },
                {
                    "authorId": "2186540882",
                    "name": "Chien Van Nguyen"
                },
                {
                    "authorId": "1692755523",
                    "name": "Nghia Trung Ngo"
                },
                {
                    "authorId": "2301582159",
                    "name": "L. Ngo"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2293544012",
                    "name": "Thien Huu Nguyen"
                }
            ]
        },
        {
            "paperId": "4a2a1a107964c19a8b4a523a7fcd78e166e85f21",
            "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes",
            "abstract": "Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mitigation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237806749",
                    "name": "Isabel O. Gallegos"
                },
                {
                    "authorId": "2066337266",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "2284591479",
                    "name": "Joe Barrow"
                },
                {
                    "authorId": "35631602",
                    "name": "Md. Mehrab Tanjim"
                },
                {
                    "authorId": "1500399016",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "1787977",
                    "name": "Hanieh Deilamsalehy"
                },
                {
                    "authorId": "2283147661",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2261424174",
                    "name": "Sungchul Kim"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                }
            ]
        },
        {
            "paperId": "4b9967c741d26a3e497236cee898d1bfaf24c3b5",
            "title": "Scaling Up Video Summarization Pretraining with Large Language Models",
            "abstract": "Long-form video content constitutes a significant portion of internet traffic, making automated video summarization an essential research problem. However, existing video summarization datasets are notably limited in their size, constraining the effectiveness of state-of-the-art methods for generalization. Our work aims to overcome this limitation by capitalizing on the abundance of long-form videos with dense speech-to-video alignment and the remarkable capabilities of recent large language models (LLMs) in summarizing long text. We introduce an automated and scalable pipeline for generating a large-scale video summarization dataset using LLMs as Oracle summarizers. By leveraging the generated dataset, we analyze the limitations of existing approaches and propose a new video summarization model that effectively addresses them. To facilitate further research in the field, our work also presents a new benchmark dataset that contains 1200 long videos each with high-quality summaries annotated by professionals. Extensive experiments clearly indicate that our proposed approach sets a new state-of-the-art in video summarization across several benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1453599592",
                    "name": "Dawit Mureja Argaw"
                },
                {
                    "authorId": "2110654003",
                    "name": "Seunghyun Yoon"
                },
                {
                    "authorId": "3175258",
                    "name": "Fabian Caba Heilbron"
                },
                {
                    "authorId": "1787977",
                    "name": "Hanieh Deilamsalehy"
                },
                {
                    "authorId": "2265648617",
                    "name": "Trung Bui"
                },
                {
                    "authorId": "2269449137",
                    "name": "Zhaowen Wang"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2295092082",
                    "name": "Joon Son Chung"
                }
            ]
        },
        {
            "paperId": "56b92527cba93fc7b178960eaa2cee670e44b19f",
            "title": "Fine-tuning CLIP Text Encoders with Two-step Paraphrasing",
            "abstract": "Contrastive language-image pre-training (CLIP) models have demonstrated considerable success across various vision-language tasks, such as text-to-image retrieval, where the model is required to effectively process natural language input to produce an accurate visual output. However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications. In this study, we introduce a straightforward fine-tuning approach to enhance the representations of CLIP models for paraphrases. Our approach involves a two-step paraphrase generation process, where we automatically create two categories of paraphrases from web-scale image captions by leveraging large language models. Subsequently, we fine-tune the CLIP text encoder using these generated paraphrases while freezing the image encoder. Our resulting model, which we call ParaCLIP, exhibits significant improvements over baseline CLIP models across various tasks, including paraphrased retrieval (with rank similarity scores improved by up to 7.6% and 9.6%), Visual Genome Relation and Attribution, as well as seven semantic textual similarity tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3087706",
                    "name": "Hyunjae Kim"
                },
                {
                    "authorId": "2110654003",
                    "name": "Seunghyun Yoon"
                },
                {
                    "authorId": "2265648617",
                    "name": "Trung Bui"
                },
                {
                    "authorId": "2286357082",
                    "name": "Handong Zhao"
                },
                {
                    "authorId": "2265648457",
                    "name": "Q. Tran"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2261449256",
                    "name": "Jaewoo Kang"
                }
            ]
        },
        {
            "paperId": "5ccdc2ae2e8db7dd84d23f2de088d64fd2993713",
            "title": "KaPQA: Knowledge-Augmented Product Question-Answering",
            "abstract": "Question-answering for domain-specific applications has recently attracted much interest due to the latest advancements in large language models (LLMs). However, accurately assessing the performance of these applications remains a challenge, mainly due to the lack of suitable benchmarks that effectively simulate real-world scenarios. To address this challenge, we introduce two product question-answering (QA) datasets focused on Adobe Acrobat and Photoshop products to help evaluate the performance of existing models on domain-specific product QA tasks. Additionally, we propose a novel knowledge-driven RAG-QA framework to enhance the performance of the models in the product QA task. Our experiments demonstrated that inducing domain knowledge through query reformulation allowed for increased retrieval and generative performance when compared to standard RAG-QA methods. This improvement, however, is slight, and thus illustrates the challenge posed by the datasets introduced.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2312395859",
                    "name": "Swetha Eppalapally"
                },
                {
                    "authorId": "2312397639",
                    "name": "Daksh Dangi"
                },
                {
                    "authorId": "2312398606",
                    "name": "Chaithra Bhat"
                },
                {
                    "authorId": "2313102073",
                    "name": "Ankita Gupta"
                },
                {
                    "authorId": "2303636546",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2311640933",
                    "name": "Shubham Agarwal"
                },
                {
                    "authorId": "2297846982",
                    "name": "Karishma Bagga"
                },
                {
                    "authorId": "2110654003",
                    "name": "Seunghyun Yoon"
                },
                {
                    "authorId": "1793409",
                    "name": "Nedim Lipka"
                },
                {
                    "authorId": "2299780933",
                    "name": "Ryan Rossi"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                }
            ]
        }
    ]
}