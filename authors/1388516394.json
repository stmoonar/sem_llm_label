{
    "authorId": "1388516394",
    "papers": [
        {
            "paperId": "550891af899eb38af1896aa1f8add8a32e98733a",
            "title": "FLAIM: AIM-based Synthetic Data Generation in the Federated Setting",
            "abstract": "Preserving individual privacy while enabling collaborative data sharing is crucial for organizations. Synthetic data generation is one solution, producing artificial data that mirrors the statistical properties of private data. While numerous techniques have been devised under differential privacy, they predominantly assume data is centralized. However, data is often distributed across multiple clients in a federated manner. In this work, we initiate the study of federated synthetic tabular data generation. Building upon a SOTA central method known as AIM, we present DistAIM and FLAIM. We first show that it is straightforward to distribute AIM, extending a recent approach based on secure multi-party computation which necessitates additional overhead, making it less suited to federated scenarios. We then demonstrate that naively federating AIM can lead to substantial degradation in utility under the presence of heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach that maintains a private proxy of heterogeneity. We simulate our methods across a range of benchmark datasets under different degrees of heterogeneity and show we can improve utility while reducing overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388516394",
                    "name": "Samuel Maddock"
                },
                {
                    "authorId": "2254587151",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2262445634",
                    "name": "Carsten Maple"
                }
            ]
        },
        {
            "paperId": "0de69dfe766375061ab0a267d281da8a6ce4bbe6",
            "title": "Federated Boosted Decision Trees with Differential Privacy",
            "abstract": "There is great demand for scalable, secure, and efficient privacy-preserving machine learning models that can be trained over distributed data. While deep learning models typically achieve the best results in a centralized non-secure setting, different models can excel when privacy and communication constraints are imposed. Instead, tree-based approaches such as XGBoost have attracted much attention for their high performance and ease of use; in particular, they often achieve state-of-the-art results on tabular data. Consequently, several recent works have focused on translating Gradient Boosted Decision Tree (GBDT) models like XGBoost into federated settings, via cryptographic mechanisms such as Homomorphic Encryption (HE) and Secure Multi-Party Computation (MPC). However, these do not always provide formal privacy guarantees, or consider the full range of hyperparameters and implementation settings. In this work, we implement the GBDT model under Differential Privacy (DP). We propose a general framework that captures and extends existing approaches for differentially private decision trees. Our framework of methods is tailored to the federated setting, and we show that with a careful choice of techniques it is possible to achieve very high utility while maintaining strong levels of privacy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388516394",
                    "name": "Samuel Maddock"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "49980880",
                    "name": "Tianhao Wang"
                },
                {
                    "authorId": "2091059383",
                    "name": "C. Maple"
                },
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                }
            ]
        },
        {
            "paperId": "37fc0cb77dfaf6cced0082f498912e19dc5666b5",
            "title": "CANIFE: Crafting Canaries for Empirical Privacy Measurement in Federated Learning",
            "abstract": "Federated Learning (FL) is a setting for training machine learning models in distributed environments where the clients do not share their raw data but instead send model updates to a server. However, model updates can be subject to attacks and leak private information. Differential Privacy (DP) is a leading mitigation strategy which involves adding noise to clipped model updates, trading off performance for strong theoretical privacy guarantees. Previous work has shown that the threat model of DP is conservative and that the obtained guarantees may be vacuous or may overestimate information leakage in practice. In this paper, we aim to achieve a tighter measurement of the model exposure by considering a realistic threat model. We propose a novel method, CANIFE, that uses canaries - carefully crafted samples by a strong adversary to evaluate the empirical privacy of a training round. We apply this attack to vision models trained on CIFAR-10 and CelebA and to language models trained on Sent140 and Shakespeare. In particular, in realistic FL scenarios, we demonstrate that the empirical per-round epsilon obtained with CANIFE is 4-5x lower than the theoretical bound.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388516394",
                    "name": "Samuel Maddock"
                },
                {
                    "authorId": "2256994781",
                    "name": "Alexandre Sablayrolles"
                },
                {
                    "authorId": "37502184",
                    "name": "Pierre Stock"
                }
            ]
        },
        {
            "paperId": "7fa33ebff6ea6a382b8b078a7850cd9c0465709f",
            "title": "Frequency Estimation under Local Differential Privacy",
            "abstract": "Private collection of statistics from a large distributed population is an important problem, and has led to large scale deployments from several leading technology companies. The dominant approach requires each user to randomly perturb their input, leading to guarantees in the local differential privacy model. In this paper, we place the various approaches that have been suggested into a common framework, and perform an extensive series of experiments to understand the tradeoffs between different implementation choices. Our conclusion is that for the core problems of frequency estimation and heavy hitter identification, careful choice of algorithms can lead to very effective solutions that scale to millions of users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1388516394",
                    "name": "Samuel Maddock"
                },
                {
                    "authorId": "152981613",
                    "name": "C. Maple"
                }
            ]
        }
    ]
}