{
    "authorId": "17730355",
    "papers": [
        {
            "paperId": "520ec1b21db7aeda19a56325e7942655300bd06c",
            "title": "MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification",
            "abstract": "We introduce MAFALDA, a benchmark for fallacy classification that merges and unites previous fallacy datasets. It comes with a taxonomy that aligns, refines, and unifies existing classifications of fallacies. We further provide a manual annotation of a part of the dataset together with manual explanations for each annotation. We propose a new annotation scheme tailored for subjective NLP tasks, and a new evaluation method designed to handle subjectivity. We then evaluate several language models under a zero-shot learning setting and human performances on MAFALDA to assess their capability to detect and classify fallacies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "17730355",
                    "name": "Chadi Helwe"
                },
                {
                    "authorId": "2266841617",
                    "name": "Tom Calamai"
                },
                {
                    "authorId": "36560957",
                    "name": "Pierre-Henri Paris"
                },
                {
                    "authorId": "2266841646",
                    "name": "Chlo\u00e9 Clavel"
                },
                {
                    "authorId": "2266842840",
                    "name": "Fabian M. Suchanek"
                }
            ]
        },
        {
            "paperId": "21ba26e871560d7101613125628af98cf32a78c8",
            "title": "TINA: Textual Inference with Negation Augmentation",
            "abstract": "Transformer-based language models achieve state-of-the-art results on several natural language processing tasks. One of these is textual entailment , i.e., the task of determining whether a premise logically entails a hypothesis. However, the models perform poorly on this task when the examples contain nega-tions. In this paper, we propose a new definition of textual entailment that captures also negation. This allows us to develop TINA (Textual Inference with Negation Augmentation), a principled technique for negated data augmentation that can be combined with the un-likelihood loss function. Our experiments with different transformer-based models show that our method can significantly improve the performance of the models on textual entailment datasets with negation \u2013 without sacrificing performance on datasets without negation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "17730355",
                    "name": "Chadi Helwe"
                },
                {
                    "authorId": "2119004939",
                    "name": "Simon Coumes"
                },
                {
                    "authorId": "2049106",
                    "name": "C. Clavel"
                },
                {
                    "authorId": "1679784",
                    "name": "Fabian M. Suchanek"
                }
            ]
        },
        {
            "paperId": "37b37b58c19f55c899e99eaa851f9fab0b19277e",
            "title": "LogiTorch: A PyTorch-based library for logical reasoning on natural language",
            "abstract": "Logical reasoning on natural language is one of the most challenging tasks for deep learning models. There has been an increasing interest in developing new benchmarks to evaluate the reasoning capabilities of language models such as BERT. In parallel, new models based on transformers have emerged to achieve ever better performance on these datasets. However, there is currently no library for logical reasoning that includes such benchmarks and models. This paper introduces LogiTorch, a PyTorch-based library that includes different logical reasoning benchmarks, different models, as well as utility functions such as co-reference resolution. This makes it easy to directly use the preprocessed datasets, to run the models, or to finetune them with different hyperparameters. LogiTorch is open source and can be found on GitHub 1 .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "17730355",
                    "name": "Chadi Helwe"
                },
                {
                    "authorId": "2049106",
                    "name": "C. Clavel"
                },
                {
                    "authorId": "1679784",
                    "name": "Fabian M. Suchanek"
                }
            ]
        },
        {
            "paperId": "8424082e3bf4792462eb112d7ebcecf5b0dc3613",
            "title": "Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning",
            "abstract": "Recent years have seen impressive performance of transformer-based models on di\ufb00erent natural language processing tasks. However, it is not clear to what degree the transformers can reason on natural language. To shed light on this question, this survey paper discusses the performance of transformers on di\ufb00erent reasoning tasks, including mathematical reasoning, commonsense reasoning, and logical reasoning. We point out successes and limitations, of both empirical and theoretical nature",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "17730355",
                    "name": "Chadi Helwe"
                },
                {
                    "authorId": "2049106",
                    "name": "C. Clavel"
                },
                {
                    "authorId": "1679784",
                    "name": "Fabian M. Suchanek"
                }
            ]
        },
        {
            "paperId": "84af469848b4432a31a2d16cfcab78d2c6a2344e",
            "title": "A Semi-Supervised BERT Approach for Arabic Named Entity Recognition",
            "abstract": "Named entity recognition (NER) plays a significant role in many applications such as information extraction, information retrieval, question answering, and even machine translation. Most of the work on NER using deep learning was done for non-Arabic languages like English and French, and only few studies focused on Arabic. This paper proposes a semi-supervised learning approach to train a BERT-based NER model using labeled and semi-labeled datasets. We compared our approach against various baselines, and state-of-the-art Arabic NER tools on three datasets: AQMAR, NEWS, and TWEETS. We report a significant improvement in F-measure for the AQMAR and the NEWS datasets, which are written in Modern Standard Arabic (MSA), and competitive results for the TWEETS dataset, which contains tweets that are mostly in the Egyptian dialect and contain many mistakes or misspellings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "17730355",
                    "name": "Chadi Helwe"
                },
                {
                    "authorId": "2029670242",
                    "name": "Ghassan Dib"
                },
                {
                    "authorId": "2029670134",
                    "name": "Mohsen Shamas"
                },
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                }
            ]
        },
        {
            "paperId": "0127dcd127d66dbfd11cce1a6c7302d206dde705",
            "title": "Assessing Arabic Weblog Credibility via Deep Co-learning",
            "abstract": "Assessing the credibility of online content has garnered a lot of attention lately. We focus on one such type of online content, namely weblogs or blogs for short. Some recent work attempted the task of automatically assessing the credibility of blogs, typically via machine learning. However, in the case of Arabic blogs, there are hardly any datasets available that can be used to train robust machine learning models for this difficult task. To overcome the lack of sufficient training data, we propose deep co-learning, a semi-supervised end-to-end deep learning approach to assess the credibility of Arabic blogs. In deep co-learning, multiple weak deep neural network classifiers are trained using a small labeled dataset, and each using a different view of the data. Each one of these classifiers is then used to classify unlabeled data, and its prediction is used to train the other classifiers in a semi-supervised fashion. We evaluate our deep co-learning approach on an Arabic blogs dataset, and we report significant improvements in performance compared to many baselines including fully-supervised deep learning models as well as ensemble models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "17730355",
                    "name": "Chadi Helwe"
                },
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "3448480",
                    "name": "A. Zaatari"
                },
                {
                    "authorId": "1402224224",
                    "name": "W. El-Hajj"
                }
            ]
        },
        {
            "paperId": "6da09d93aedb79ae9121e37489820fe331da84ad",
            "title": "CCS Coding of Discharge Diagnoses via Deep Neural Networks",
            "abstract": "A standard procedure in the medical domain is to code discharge diagnoses into a set of manageable categories known as the CCS codes. This is typically done by first manually coding the discharge diagnoses into the standard ICD codes and then using a one-to-one mapping between ICD and CCS codes. In this paper, we study the applicability of deep learning to perform automatic coding of discharge diagnoses into CCS codes. In particular, we build an LSTM network combined with a dense neural network that uses medically-trained word embeddings to code discharge diagnoses into single-level CCS codes. We also investigate the advantage of mapping discharge diagnoses into UMLS concepts before coding is carried out. Experimental results based on a large dataset of manually coded discharge diagnoses show that our deep-learning model outperforms the state-of-the-art automatic coding approaches and that the mapping to UMLS concepts consistently results in significant improvement in the coding accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "17730355",
                    "name": "Chadi Helwe"
                },
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "41231046",
                    "name": "Mirabelle Geha"
                },
                {
                    "authorId": "2955618",
                    "name": "Eveline Hitti"
                },
                {
                    "authorId": "8006974",
                    "name": "C. Obermeyer"
                }
            ]
        },
        {
            "paperId": "7225a11f81650fdbd630363cf498ccb7825efb54",
            "title": "Reconfigurable and Adaptive Spark Applications",
            "abstract": "The contribution of this paper is two-fold. First, we propose a Domain Specific Language (DSL) to easily reconfigure and compose Spark applications. For each Spark application we define its input and output interfaces. Then, given a set of connections that map outputs of some Spark applications to free inputs of other Spark applications, we automatically embed Spark applications with the required synchronization and communication to properly run them according to the user-defined mapping. Second, we present an adaptive quality management/selection method for Spark applications. The method takes as input a pipeline of parameterized Spark applications, where the execution time of each Spark application is an unknown increasing function of quality level parameters. The method builds a controller that automatically computes adequate quality for each Spark application to meet a user-defined deadline. Consequently, users can submit a pipeline of Spark applications and a deadline, our method automatically runs all the Spark applications with the maximum quality while respecting the deadline specified by the user. We present experimental results showing the effectiveness of our method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34609940",
                    "name": "M. Jaber"
                },
                {
                    "authorId": "144712197",
                    "name": "M. Nassar"
                },
                {
                    "authorId": "17737197",
                    "name": "Wael Al Orabi"
                },
                {
                    "authorId": "2161009",
                    "name": "B. Farraj"
                },
                {
                    "authorId": "17719925",
                    "name": "M. Kayali"
                },
                {
                    "authorId": "17730355",
                    "name": "Chadi Helwe"
                }
            ]
        },
        {
            "paperId": "aff3ca0d0ac92cc33d60314bdef479f1fdee68a6",
            "title": "Methodical Evaluation of Arabic Word Embeddings",
            "abstract": "Many unsupervised learning techniques have been proposed to obtain meaningful representations of words from text. In this study, we evaluate these various techniques when used to generate Arabic word embeddings. We first build a benchmark for the Arabic language that can be utilized to perform intrinsic evaluation of different word embeddings. We then perform additional extrinsic evaluations of the embeddings based on two NLP tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2103301102",
                    "name": "Mohammed Elrazzaz"
                },
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "1837936",
                    "name": "K. Shaban"
                },
                {
                    "authorId": "17730355",
                    "name": "Chadi Helwe"
                }
            ]
        }
    ]
}