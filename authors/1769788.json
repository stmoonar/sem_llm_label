{
    "authorId": "1769788",
    "papers": [
        {
            "paperId": "078f86c6a691806cc71bbef1e734f75690db0ffc",
            "title": "FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding",
            "abstract": "Although Domain Adaptation in Semantic Scene Segmentation has shown impressive improvement in recent years, the fairness concerns in the domain adaptation have yet to be well defined and addressed. In addition, fairness is one of the most critical aspects when deploying the segmentation models into human-related real-world applications, e.g., autonomous driving, as any unfair predictions could influence human safety. In this paper, we propose a novel Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation. In particular, from the proposed formulated fairness objective, a new adaptation framework will be introduced based on the fair treatment of class distributions. Moreover, to generally model the context of structural dependency, a new conditional structural constraint is introduced to impose the consistency of predicted segmentation. Thanks to the proposed Conditional Structure Network, the self-attention mechanism has sufficiently modeled the structural information of segmentation. Through the ablation studies, the proposed method has shown the performance improvement of the segmentation models and promoted fairness in the model predictions. The experimental results on the two standard benchmarks, i.e., SYNTHIA $\\rightarrow$ Cityscapes and GTA5 $\\rightarrow$ Cityscapes, have shown that our method achieved State-of-the-Art (SOTA) performance11The implementation of FREDOM is available at https://github.com/uark-cviu/FREDOM",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35659935",
                    "name": "Thanh-Dat Truong"
                },
                {
                    "authorId": "144556913",
                    "name": "Ngan T. H. Le"
                },
                {
                    "authorId": "1681921",
                    "name": "B. Raj"
                },
                {
                    "authorId": "145863239",
                    "name": "J. Cothren"
                },
                {
                    "authorId": "1769788",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "0a8d38686b18f28aae1222529e6b9e8a60cab1c2",
            "title": "UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation",
            "abstract": "Multiple Object Tracking (MOT) aims to find bounding boxes and identities of targeted objects in consecutive video frames. While fully-supervised MOT methods have achieved high accuracy on existing datasets, they cannot generalize well on a newly obtained dataset or a new unseen domain. In this work, we first address the MOT problem from the cross-domain point of view, imitating the process of new data acquisition in practice. Then, a new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects. It can also learn and update itself from the target data feedback. The intensive experiments are designed on four challenging settings, including MOTSynth to MOT17, MOT17 to MOT20, MOT17 to VisDrone, and MOT17 to DanceTrack. We then prove the adaptability of the proposed self-supervised learning strategy. The experiments also show superior performance on tracking metrics MOTA and IDF1, compared to fully supervised, unsupervised, and self-supervised state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2468571",
                    "name": "Pha Nguyen"
                },
                {
                    "authorId": "2687827",
                    "name": "Kha Gia Quach"
                },
                {
                    "authorId": "1701970",
                    "name": "J. Gauch"
                },
                {
                    "authorId": "1740261",
                    "name": "S. Khan"
                },
                {
                    "authorId": "1681921",
                    "name": "B. Raj"
                },
                {
                    "authorId": "1769788",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "276115bd92274541189b87cce0f6302b923cecf8",
            "title": "CoMaL: Conditional Maximum Likelihood Approach to Self-supervised Domain Adaptation in Long-tail Semantic Segmentation",
            "abstract": "The research in self-supervised domain adaptation in semantic segmentation has recently received considerable attention. Although GAN-based methods have become one of the most popular approaches to domain adaptation, they have suffered from some limitations. They are insufficient to model both global and local structures of a given image, especially in small regions of tail classes. Moreover, they perform bad on the tail classes containing limited number of pixels or less training samples. In order to address these issues, we present a new self-supervised domain adaptation approach to tackle long-tail semantic segmentation in this paper. Firstly, a new metric is introduced to formulate long-tail domain adaptation in the segmentation problem. Secondly, a new Conditional Maximum Likelihood (CoMaL) approach in an autoregressive framework is presented to solve the problem of long-tail domain adaptation. Although other segmentation methods work under the pixel independence assumption, the long-tailed pixel distributions in CoMaL are generally solved in the context of structural dependency, as that is more realistic. Finally, the proposed method is evaluated on popular large-scale semantic segmentation benchmarks, i.e.,\"SYNTHIA to Cityscapes\"and\"GTA to Cityscapes\", and outperforms the prior methods by a large margin in both the standard and the proposed evaluation protocols.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35659935",
                    "name": "Thanh-Dat Truong"
                },
                {
                    "authorId": "1876581",
                    "name": "C. Duong"
                },
                {
                    "authorId": "2165016875",
                    "name": "Pierce Helton"
                },
                {
                    "authorId": "2166046829",
                    "name": "Ashley Dowling"
                },
                {
                    "authorId": "48568672",
                    "name": "Xin Li"
                },
                {
                    "authorId": "1769788",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "334377132b587261d12734a43c94cdbe72d32899",
            "title": "Type-to-Track: Retrieve Any Object via Prompt-based Tracking",
            "abstract": "One of the recent trends in vision problems is to use natural language captions to describe the objects of interest. This approach can overcome some limitations of traditional methods that rely on bounding boxes or category annotations. This paper introduces a novel paradigm for Multiple Object Tracking called Type-to-Track, which allows users to track objects in videos by typing natural language descriptions. We present a new dataset for that Grounded Multiple Object Tracking task, called GroOT, that contains videos with various types of objects and their corresponding textual captions describing their appearance and action in detail. Additionally, we introduce two new evaluation protocols and formulate evaluation metrics specifically for this task. We develop a new efficient method that models a transformer-based eMbed-ENcoDE-extRact framework (MENDER) using the third-order tensor decomposition. The experiments in five scenarios show that our MENDER approach outperforms another two-stage design in terms of accuracy and efficiency, up to 14.7% accuracy and 4$\\times$ speed faster.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2468571",
                    "name": "Pha Nguyen"
                },
                {
                    "authorId": "2687827",
                    "name": "Kha Gia Quach"
                },
                {
                    "authorId": "144040368",
                    "name": "Kris Kitani"
                },
                {
                    "authorId": "1769788",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "3d17a509560f7275572037fca24cf72eb43f556c",
            "title": "Cross-view Action Recognition Understanding From Exocentric to Egocentric Perspective",
            "abstract": "Understanding action recognition in egocentric videos has emerged as a vital research topic with numerous practical applications. With the limitation in the scale of egocentric data collection, learning robust deep learning-based action recognition models remains difficult. Transferring knowledge learned from the large-scale exocentric data to the egocentric data is challenging due to the difference in videos across views. Our work introduces a novel cross-view learning approach to action recognition (CVAR) that effectively transfers knowledge from the exocentric to the selfish view. First, we present a novel geometric-based constraint into the self-attention mechanism in Transformer based on analyzing the camera positions between two views. Then, we propose a new cross-view self-attention loss learned on unpaired cross-view data to enforce the self-attention mechanism learning to transfer knowledge across views. Finally, to further improve the performance of our cross-view learning approach, we present the metrics to measure the correlations in videos and attention maps effectively. Experimental results on standard egocentric action recognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, and EPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-art performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35659935",
                    "name": "Thanh-Dat Truong"
                },
                {
                    "authorId": "1769788",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "4c798d4a4f0d20da9c37a4858cb0b2789b80d882",
            "title": "SoGAR: Self-supervised Spatiotemporal Attention-based Social Group Activity Recognition",
            "abstract": "This paper introduces a novel approach to Social Group Activity Recognition (SoGAR) using Self-supervised Transformers network that can effectively utilize unlabeled video data. To extract spatio-temporal information, we created local and global views with varying frame rates. Our self-supervised objective ensures that features extracted from contrasting views of the same video were consistent across spatio-temporal domains. Our proposed approach is efficient in using transformer-based encoders to alleviate the weakly supervised setting of group activity recognition. By leveraging the benefits of transformer models, our approach can model long-term relationships along spatio-temporal dimensions. Our proposed SoGAR method achieved state-of-the-art results on three group activity recognition benchmarks, namely JRDB-PAR, NBA, and Volleyball datasets, surpassing the current numbers in terms of F1-score, MCA, and MPCA metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2166045667",
                    "name": "N. V. R. Chappa"
                },
                {
                    "authorId": "2468571",
                    "name": "Pha Nguyen"
                },
                {
                    "authorId": "2187716611",
                    "name": "Alec Nelson"
                },
                {
                    "authorId": "2159536733",
                    "name": "Han-Seok Seo"
                },
                {
                    "authorId": "48568672",
                    "name": "Xin Li"
                },
                {
                    "authorId": "15064400",
                    "name": "P. Dobbs"
                },
                {
                    "authorId": "1769788",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "5f2ffecd734106f061dec51f6600dcec5b9a404f",
            "title": "Fairness in Visual Clustering: A Novel Transformer Clustering Approach",
            "abstract": "Promoting fairness for deep clustering models in unsupervised clustering settings to reduce demographic bias is a challenging goal. This is because of the limitation of large-scale balanced data with well-annotated labels for sensitive or protected attributes. In this paper, we first evaluate demographic bias in deep clustering models from the perspective of cluster purity, which is measured by the ratio of positive samples within a cluster to their correlation degree. This measurement is adopted as an indication of demographic bias. Then, a novel loss function is introduced to encourage a purity consistency for all clusters to maintain the fairness aspect of the learned clustering model. Moreover, we present a novel attention mechanism, Cross-attention, to measure correlations between multiple clusters, strengthening faraway positive samples and improving the purity of clusters during the learning process. Experimental results on a large-scale dataset with numerous attribute settings have demonstrated the effectiveness of the proposed approach on both clustering accuracy and fairness enhancement on several sensitive attributes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1959025244",
                    "name": "Xuan-Bac Nguyen"
                },
                {
                    "authorId": "1876581",
                    "name": "C. Duong"
                },
                {
                    "authorId": "1794486",
                    "name": "M. Savvides"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "1769788",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "9040fd3397fc89ac0a99d185c956189fc9931e1f",
            "title": "Z-GMOT: Zero-shot Generic Multiple Object Tracking",
            "abstract": "Despite recent significant progress, Multi-Object Tracking (MOT) faces limitations such as reliance on prior knowledge and predefined categories and struggles with unseen objects. To address these issues, Generic Multiple Object Tracking (GMOT) has emerged as an alternative approach, requiring less prior information. However, current GMOT methods often rely on initial bounding boxes and struggle to handle variations in factors such as viewpoint, lighting, occlusion, and scale, among others. Our contributions commence with the introduction of the \\textit{Referring GMOT dataset} a collection of videos, each accompanied by detailed textual descriptions of their attributes. Subsequently, we propose $\\mathtt{Z-GMOT}$, a cutting-edge tracking solution capable of tracking objects from \\textit{never-seen categories} without the need of initial bounding boxes or predefined categories. Within our $\\mathtt{Z-GMOT}$ framework, we introduce two novel components: (i) $\\mathtt{iGLIP}$, an improved Grounded language-image pretraining, for accurately detecting unseen objects with specific characteristics. (ii) $\\mathtt{MA-SORT}$, a novel object association approach that adeptly integrates motion and appearance-based matching strategies to tackle the complex task of tracking objects with high similarity. Our contributions are benchmarked through extensive experiments conducted on the Referring GMOT dataset for GMOT task. Additionally, to assess the generalizability of the proposed $\\mathtt{Z-GMOT}$, we conduct ablation studies on the DanceTrack and MOT20 datasets for the MOT task. Our dataset, code, and models are released at: https://fsoft-aic.github.io/Z-GMOT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218446910",
                    "name": "Kim Hoang Tran"
                },
                {
                    "authorId": "1845324774",
                    "name": "Tien-Phat Nguyen"
                },
                {
                    "authorId": "2218455862",
                    "name": "Anh Duy Le Dinh"
                },
                {
                    "authorId": "2468571",
                    "name": "Pha Nguyen"
                },
                {
                    "authorId": "1556429551",
                    "name": "Thinh Phan"
                },
                {
                    "authorId": "1769788",
                    "name": "Khoa Luu"
                },
                {
                    "authorId": "2197292410",
                    "name": "Don Adjeroh"
                },
                {
                    "authorId": "144556913",
                    "name": "Ngan T. H. Le"
                }
            ]
        },
        {
            "paperId": "ac856b6b7b3f32fb34320b7170526d3ab15ba5f3",
            "title": "Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments",
            "abstract": "Continual semantic segmentation aims to learn new classes while maintaining the information from the previous classes. Although prior studies have shown impressive progress in recent years, the fairness concern in the continual semantic segmentation needs to be better addressed. Meanwhile, fairness is one of the most vital factors in deploying the deep learning model, especially in human-related or safety applications. In this paper, we present a novel Fairness Continual Learning approach to the semantic segmentation problem. In particular, under the fairness objective, a new fairness continual learning framework is proposed based on class distributions. Then, a novel Prototypical Contrastive Clustering loss is proposed to address the significant challenges in continual learning, i.e., catastrophic forgetting and background shift. Our proposed loss has also been proven as a novel, generalized learning paradigm of knowledge distillation commonly used in continual learning. Moreover, the proposed Conditional Structural Consistency loss further regularized the structural constraint of the predicted segmentation. Our proposed approach has achieved State-of-the-Art performance on three standard scene understanding benchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC, and promoted the fairness of the segmentation model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35659935",
                    "name": "Thanh-Dat Truong"
                },
                {
                    "authorId": null,
                    "name": "Hoang-Quan Nguyen"
                },
                {
                    "authorId": "1681921",
                    "name": "B. Raj"
                },
                {
                    "authorId": "1769788",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "b52af6bc5e556405c95033de81dd95d9ff229a50",
            "title": "Micron-BERT: BERT-Based Facial Micro-Expression Recognition",
            "abstract": "Micro-expression recognition is one of the most challenging topics in affective computing. It aims to recognize tiny facial movements difficult for humans to perceive in a brief period, i.e., 0.25 to 0.5 seconds. Recent advances in pre-training deep Bidirectional Transformers (BERT) have significantly improved self-supervised learning tasks in computer vision. However, the standard BERT in vision problems is designed to learn only from full images or videos, and the architecture cannot accurately detect details of facial micro-expressions. This paper presents Micron-BERT ($(\\mu$-BERT), a novel approach to facial micro-expression recognition. The proposed method can automatically capture these movements in an unsupervised manner based on two key ideas. First, we employ Diagonal Micro-Attention (DMA) to detect tiny differences between two frames. Second, we introduce a new Patch of Interest (PoI) module to localize and highlight micro-expression interest regions and simultaneously reduce noisy backgrounds and distractions. By incorporating these components into an end-to-end deep network, the proposed $\\mu$-BERT significantly outperforms all previous work in various micro-expression tasks. $\\mu$-BERT can be trained on a large-scale unlabeled dataset, i.e., up to 8 million images, and achieves high accuracy on new unseen facial micro-expression datasets. Empirical experiments show $\\mu$-BERT consistently outperforms state-of-the-art performance on four micro-expression benchmarks, including SAMM, CASME II, SMIC, and CASME3, by significant margins. Code will be available at https://github.com/uark-cviu/Micron-BERT",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1959025244",
                    "name": "Xuan-Bac Nguyen"
                },
                {
                    "authorId": "1876581",
                    "name": "C. Duong"
                },
                {
                    "authorId": "48568672",
                    "name": "Xin Li"
                },
                {
                    "authorId": "1805343",
                    "name": "Susan Gauch"
                },
                {
                    "authorId": "2159536733",
                    "name": "Han-Seok Seo"
                },
                {
                    "authorId": "1769788",
                    "name": "Khoa Luu"
                }
            ]
        }
    ]
}