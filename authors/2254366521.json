{
    "authorId": "2254366521",
    "papers": [
        {
            "paperId": "246ced9521c93e9f267a8f6c370b0b8422757770",
            "title": "FashionReGen: LLM-Empowered Fashion Report Generation",
            "abstract": "Fashion analysis refers to the process of examining and evaluating trends, styles, and elements within the fashion industry to understand and interpret its current state, generating fashion reports. It is traditionally performed by fashion professionals based on their expertise and experience, which requires high labour cost and may also produce biased results for relying heavily on a small group of people. In this paper, to tackle the Fashion Report Generation (FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting system based the advanced Large Language Models (LLMs), debbed as GPT-FAR. Specifically, it tries to deliver FashionReGen based on effective catwalk analysis, the proposed GPT-FAR system is equipped with several key procedures, namely, catwalk understanding, collective organization and analysis, and report generation. By posing and exploring such an open-ended, complex and domain-specific task of FashionReGen, it is able to test the general capability of LLMs in fashion domain. It also inspires the explorations of more high-level tasks with industrial significance in other domains. Video illustration and more materials of GPT-FAR can be found in https://github.com/CompFashion/FashionReGen.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "65768925",
                    "name": "Yujuan Ding"
                },
                {
                    "authorId": "2109267019",
                    "name": "Yunshan Ma"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2290864893",
                    "name": "Yige Yao"
                },
                {
                    "authorId": "2270722858",
                    "name": "Tat-Seng Chua"
                },
                {
                    "authorId": "2254366521",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "250d0521a774da4db33f7b81e78d8d34592ce6cf",
            "title": "TokenRec: Learning to Tokenize ID for LLM-based Generative Recommendation",
            "abstract": "There is a growing interest in utilizing large-scale language models (LLMs) to advance next-generation Recommender Systems (RecSys), driven by their outstanding language understanding and in-context learning capabilities. In this scenario, tokenizing (i.e., indexing) users and items becomes essential for ensuring a seamless alignment of LLMs with recommendations. While several studies have made progress in representing users and items through textual contents or latent representations, challenges remain in efficiently capturing high-order collaborative knowledge into discrete tokens that are compatible with LLMs. Additionally, the majority of existing tokenization approaches often face difficulties in generalizing effectively to new/unseen users or items that were not in the training corpus. To address these challenges, we propose a novel framework called TokenRec, which introduces not only an effective ID tokenization strategy but also an efficient retrieval paradigm for LLM-based recommendations. Specifically, our tokenization strategy, Masked Vector-Quantized (MQ) Tokenizer, involves quantizing the masked user/item representations learned from collaborative filtering into discrete tokens, thus achieving a smooth incorporation of high-order collaborative knowledge and a generalizable tokenization of users and items for LLM-based RecSys. Meanwhile, our generative retrieval paradigm is designed to efficiently recommend top-$K$ items for users to eliminate the need for the time-consuming auto-regressive decoding and beam search processes used by LLMs, thus significantly reducing inference time. Comprehensive experiments validate the effectiveness of the proposed methods, demonstrating that TokenRec outperforms competitive benchmarks, including both traditional recommender systems and emerging LLM-based recommender systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2306950042",
                    "name": "Haohao Qu"
                },
                {
                    "authorId": "2291324376",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2186864321",
                    "name": "Zihuai Zhao"
                },
                {
                    "authorId": "2254366521",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "54ed1873f88feea868e6182d44b3c6ff8d69f71e",
            "title": "Backdoor Graph Condensation",
            "abstract": "Recently, graph condensation has emerged as a prevalent technique to improve the training efficiency for graph neural networks (GNNs). It condenses a large graph into a small one such that a GNN trained on this small synthetic graph can achieve comparable performance to a GNN trained on a large graph. However, while existing graph condensation studies mainly focus on the best trade-off between graph size and the GNNs' performance (model utility), the security issues of graph condensation have not been studied. To bridge this research gap, we propose the task of backdoor graph condensation. While graph backdoor attacks have been extensively explored, applying existing graph backdoor methods for graph condensation is not practical since they can undermine the model utility and yield low attack success rate. To alleviate these issues, we introduce two primary objectives for backdoor attacks against graph condensation: 1) the injection of triggers cannot affect the quality of condensed graphs, maintaining the utility of GNNs trained on them; and 2) the effectiveness of triggers should be preserved throughout the condensation process, achieving high attack success rate. To pursue the objectives, we devise the first backdoor attack against graph condensation, denoted as BGC. Specifically, we inject triggers during condensation and iteratively update the triggers to ensure effective attacks. Further, we propose a poisoned node selection module to minimize the influence of triggers on condensed graphs' quality. The extensive experiments demonstrate the effectiveness of our attack. BGC achieves a high attack success rate (close to 1.0) and good model utility in all cases. Furthermore, the results demonstrate our method's resilience against multiple defense methods. Finally, we conduct comprehensive studies to analyze the factors that influence the attack performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109164978",
                    "name": "Jiahao Wu"
                },
                {
                    "authorId": "2147128045",
                    "name": "Ning Lu"
                },
                {
                    "authorId": "2311579404",
                    "name": "Zeiyu Dai"
                },
                {
                    "authorId": "2291324376",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2152939552",
                    "name": "Shengcai Liu"
                },
                {
                    "authorId": "2254366521",
                    "name": "Qing Li"
                },
                {
                    "authorId": "2253405825",
                    "name": "Ke Tang"
                }
            ]
        },
        {
            "paperId": "d86b1f87863cd14d03f6de4ac1082dc047299a45",
            "title": "Advancing the Robustness of Large Language Models through Self-Denoised Smoothing",
            "abstract": "Although large language models (LLMs) have achieved significant success, their vulnerability to adversarial perturbations, including recent jailbreak attacks, has raised considerable concerns. However, the increasing size of these models and their limited access make improving their robustness a challenging task. Among various defense strategies, randomized smoothing has shown great potential for LLMs, as it does not require full access to the model\u2019s parameters or fine-tuning via adversarial training. However, randomized smoothing involves adding noise to the input before model prediction, and the final model\u2019s robustness largely depends on the model\u2019s performance on these noise-corrupted data. Its effectiveness is often limited by the model\u2019s sub-optimal performance on noisy data. To address this issue, we propose to leverage the multitasking nature of LLMs to first denoise the noisy inputs and then to make predictions based on these denoised versions. We call this procedure self-denoised smoothing. Unlike previous denoised smoothing techniques in computer vision, which require training a separate model to enhance the robustness of LLMs, our method offers significantly better efficiency and flexibility. Our experimental results indicate that our method surpasses existing methods in both empirical and certified robustness in defending against adversarial attacks for both downstream tasks and human alignments (i.e., jailbreak attacks). Our code is publicly available at https://github.com/UCSB-NLP-Chang/SelfDenoise.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111812169",
                    "name": "Jiabao Ji"
                },
                {
                    "authorId": "1955614986",
                    "name": "Bairu Hou"
                },
                {
                    "authorId": "2297277435",
                    "name": "Zhen Zhang"
                },
                {
                    "authorId": "46266569",
                    "name": "Guanhua Zhang"
                },
                {
                    "authorId": "2291324376",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2254366521",
                    "name": "Qing Li"
                },
                {
                    "authorId": "2266840596",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2253857078",
                    "name": "Gaowen Liu"
                },
                {
                    "authorId": "2297212494",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "2191955409",
                    "name": "Shiyu Chang"
                }
            ]
        },
        {
            "paperId": "077d0d96198252c1b8c9f256ed10edbe8e6ae7ad",
            "title": "Dataset Condensation for Recommendation",
            "abstract": "Training recommendation models on large datasets often requires significant time and computational resources. Consequently, an emergent imperative has arisen to construct informative, smaller-scale datasets for efficiently training. Dataset compression techniques explored in other domains show potential possibility to address this problem, via sampling a subset or synthesizing a small dataset. However, applying existing approaches to condense recommendation datasets is impractical due to following challenges: (i) sampling-based methods are inadequate in addressing the long-tailed distribution problem; (ii) synthesizing-based methods are not applicable due to discreteness of interactions and large size of recommendation datasets; (iii) neither of them fail to address the specific issue in recommendation of false negative items, where items with potential user interest are incorrectly sampled as negatives owing to insufficient exposure. To bridge this gap, we investigate dataset condensation for recommendation, where discrete interactions are continualized with probabilistic re-parameterization. To avoid catastrophically expensive computations, we adopt a one-step update strategy for inner model training and introducing policy gradient estimation for outer dataset synthesis. To mitigate amplification of long-tailed problem, we compensate long-tailed users in the condensed dataset. Furthermore, we propose to utilize a proxy model to identify false negative items. Theoretical analysis regarding the convergence property is provided. Extensive experiments on multiple datasets demonstrate the efficacy of our method. In particular, we reduce the dataset size by 75% while approximating over 98% of the original performance on Dianping and over 90% on other datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109164978",
                    "name": "Jiahao Wu"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2152939552",
                    "name": "Shengcai Liu"
                },
                {
                    "authorId": "2244776472",
                    "name": "Qijiong Liu"
                },
                {
                    "authorId": "2253403022",
                    "name": "Rui He"
                },
                {
                    "authorId": "2254366521",
                    "name": "Qing Li"
                },
                {
                    "authorId": "2253405825",
                    "name": "Ke Tang"
                }
            ]
        },
        {
            "paperId": "0790ffc8118c9ebba6a1d2b0e7f805f39faeae82",
            "title": "Leveraging Large Language Models (LLMs) to Empower Training-Free Dataset Condensation for Content-Based Recommendation",
            "abstract": "Modern techniques in Content-based Recommendation (CBR) leverage item content information to provide personalized services to users, but suffer from resource-intensive training on large datasets. To address this issue, we explore the dataset condensation for textual CBR in this paper. The goal of dataset condensation is to synthesize a small yet informative dataset, upon which models can achieve performance comparable to those trained on large datasets. While existing condensation approaches are tailored to classification tasks for continuous data like images or embeddings, direct application of them to CBR has limitations. To bridge this gap, we investigate efficient dataset condensation for content-based recommendation. Inspired by the remarkable abilities of large language models (LLMs) in text comprehension and generation, we leverage LLMs to empower the generation of textual content during condensation. To handle the interaction data involving both users and items, we devise a dual-level condensation method: content-level and user-level. At content-level, we utilize LLMs to condense all contents of an item into a new informative title. At user-level, we design a clustering-based synthesis module, where we first utilize LLMs to extract user interests. Then, the user interests and user embeddings are incorporated to condense users and generate interactions for condensed users. Notably, the condensation paradigm of this method is forward and free from iterative optimization on the synthesized dataset. Extensive empirical findings from our study, conducted on three authentic datasets, substantiate the efficacy of the proposed method. Particularly, we are able to approximate up to 97% of the original performance while reducing the dataset size by 95% (i.e., on dataset MIND).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109164978",
                    "name": "Jiahao Wu"
                },
                {
                    "authorId": "2244776472",
                    "name": "Qijiong Liu"
                },
                {
                    "authorId": "2258801777",
                    "name": "Hengchang Hu"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2152939552",
                    "name": "Shengcai Liu"
                },
                {
                    "authorId": "2254366521",
                    "name": "Qing Li"
                },
                {
                    "authorId": "2187512110",
                    "name": "Xiao-Ming Wu"
                },
                {
                    "authorId": "2253405825",
                    "name": "Ke Tang"
                }
            ]
        },
        {
            "paperId": "5bc29fba16ad5fd7f575154b84f09970dae92491",
            "title": "Multi-agent Attacks for Black-box Social Recommendations",
            "abstract": "The rise of online social networks has facilitated the evolution of social recommender systems, which incorporate social relations to enhance users' decision-making process. With the great success of Graph Neural Networks (GNNs) in learning node representations, GNN-based social recommendations have been widely studied to model user-item interactions and user-user social relations simultaneously. Despite their great successes, recent studies have shown that these advanced recommender systems are highly vulnerable to adversarial attacks, in which attackers can inject well-designed fake user profiles to disrupt recommendation performances. While most existing studies mainly focus on argeted attacks to promote target items on vanilla recommender systems, untargeted attacks to degrade the overall prediction performance are less explored on social recommendations under a black-box scenario. To perform untargeted attacks on social recommender systems, attackers can construct malicious social relationships for fake users to enhance the attack performance. However, the coordination of social relations and item profiles is challenging for attacking black-box social recommendations. To address this limitation, we first conduct several preliminary studies to demonstrate the effectiveness of cross-community connections and cold-start items in degrading recommendations performance. Specifically, we propose a novel framework MultiAttack based on multi-agent reinforcement learning to coordinate the generation of cold-start item profiles and cross-community social relations for conducting untargeted attacks on black-box social recommendations. Comprehensive experiments on various real-world datasets demonstrate the effectiveness of our proposed attacking framework under the black-box setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2266567589",
                    "name": "Shijie Wang"
                },
                {
                    "authorId": "2115493866",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "2221127240",
                    "name": "Xiaowei Mei"
                },
                {
                    "authorId": "2254366521",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "e47febdb734c38981d98c82081ffbe94905dd4a5",
            "title": "Fast Graph Condensation with Structure-based Neural Tangent Kernel",
            "abstract": "The rapid development of Internet technology has given rise to a vast amount of graph-structured data. Graph Neural Networks (GNNs), as an effective method for various graph mining tasks, incurs substantial computational resource costs when dealing with large-scale graph data. A data-centric manner solution is proposed to condense the large graph dataset into a smaller one without sacrificing the predictive performance of GNNs. However, existing efforts condense graph-structured data through a computational intensive bi-level optimization architecture also suffer from massive computation costs. In this paper, we propose reforming the graph condensation problem as a Kernel Ridge Regression (KRR) task instead of iteratively training GNNs in the inner loop of bi-level optimization. More specifically, We propose a novel dataset condensation framework (GC-SNTK) for graph-structured data, where a Structure-based Neural Tangent Kernel (SNTK) is developed to capture the topology of graph and serves as the kernel function in KRR paradigm. Comprehensive experiments demonstrate the effectiveness of our proposed model in accelerating graph condensation while maintaining high prediction performance. The source code is available on \\hrefhttps://github.com/WANGLin0126/GCSNTK https://github.com/WANGLin0126/GCSNTK.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2259791162",
                    "name": "Lin Wang"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2259858493",
                    "name": "Jiatong Li"
                },
                {
                    "authorId": "2260282105",
                    "name": "Yao Ma"
                },
                {
                    "authorId": "2254366521",
                    "name": "Qing Li"
                }
            ]
        }
    ]
}