{
    "authorId": "2145408511",
    "papers": [
        {
            "paperId": "196f05c99d66d174da86fb7b17d6654fc3d84e4f",
            "title": "FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion",
            "abstract": "Taxonomy Expansion, which models complex 001 concepts and their relations, can be formulated 002 as a set representation learning task. The gen-003 eralization of set, fuzzy set, incorporates uncer-004 tainty and measures the information within a 005 semantic concept, making it suitable for con-006 cept modeling. Existing works usually model 007 sets as vectors or geometric objects such as 008 boxes, which are not closed under set opera-009 tions. In this work, we propose a sound and 010 efficient formulation of set representation learn-011 ing based on its volume approximation as a 012 fuzzy set. The resulting embedding framework, 013 Fuzzy Set Embedding (FUSE), satisfies all set 014 operations and compactly approximates the un-015 derlying fuzzy set, hence preserving informa-016 tion while being efficient to learn, relying on 017 minimum neural architecture. We empirically 018 demonstrate the power of FUSE on the task of 019 taxonomy expansion, where FUSE achieves re-020 markable improvements up to 23% compared 021 with existing baselines. Our work marks the 022 first attempt to understand and efficiently com-023 pute the embeddings of fuzzy sets. 024",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2316855310",
                    "name": "Fred Xu"
                },
                {
                    "authorId": "2108719385",
                    "name": "Song Jiang"
                },
                {
                    "authorId": "12318198",
                    "name": "Zijie Huang"
                },
                {
                    "authorId": "2220669584",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2145408511",
                    "name": "Shichang Zhang"
                },
                {
                    "authorId": "2257111854",
                    "name": "Yuanzhou Chen"
                },
                {
                    "authorId": "2260436235",
                    "name": "Yizhou Sun"
                }
            ]
        },
        {
            "paperId": "4a9fbd54d665fac70d8338a2b7f5cefed7ae9046",
            "title": "Automated Molecular Concept Generation and Labeling with Large Language Models",
            "abstract": "Artificial intelligence (AI) is significantly transforming scientific research. Explainable AI methods, such as concept-based models (CMs), are promising for driving new scientific discoveries because they make predictions based on meaningful concepts and offer insights into the prediction process. In molecular science, however, explainable CMs are not as common compared to black-box models like Graph Neural Networks (GNNs), primarily due to their requirement for predefined concepts and manual label for each instance, which demand domain knowledge and can be labor-intensive. This paper introduces a novel framework for Automated Molecular Concept (AutoMolCo) generation and labeling. AutoMolCo leverages the knowledge in Large Language Models (LLMs) to automatically generate predictive molecular concepts and label them for each molecule. Such procedures are repeated through iterative interactions with LLMs to refine concepts, enabling simple linear models on the refined concepts to outperform GNNs and LLM in-context learning on several benchmarks. The whole AutoMolCo framework is automated without any human knowledge inputs in either concept generation, labeling, or refinement, thereby surpassing the limitations of extant CMs while maintaining their explainability and allowing easy intervention. Through systematic experiments on MoleculeNet and High-Throughput Experimentation (HTE) datasets, we demonstrate that the AutoMolCo-induced explainable CMs are beneficial and promising for molecular science research.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2145408511",
                    "name": "Shichang Zhang"
                },
                {
                    "authorId": "2306781777",
                    "name": "Botao Xia"
                },
                {
                    "authorId": "2307004466",
                    "name": "Zimin Zhang"
                },
                {
                    "authorId": "2306869573",
                    "name": "Qianli Wu"
                },
                {
                    "authorId": "2307404605",
                    "name": "Fang Sun"
                },
                {
                    "authorId": "3407296",
                    "name": "Ziniu Hu"
                },
                {
                    "authorId": "2279734673",
                    "name": "Yizhou Sun"
                }
            ]
        },
        {
            "paperId": "86454a0383d8b59ae2843116f960a1c99551f5ed",
            "title": "Parameter-Efficient Tuning Large Language Models for Graph Representation Learning",
            "abstract": "Text-rich graphs, which exhibit rich textual information on nodes and edges, are prevalent across a wide range of real-world business applications. Large Language Models (LLMs) have demonstrated remarkable abilities in understanding text, which also introduced the potential for more expressive modeling in text-rich graphs. Despite these capabilities, efficiently applying LLMs to representation learning on graphs presents significant challenges. Recently, parameter-efficient fine-tuning methods for LLMs have enabled efficient new task generalization with minimal time and memory consumption. Inspired by this, we introduce Graph-aware Parameter-Efficient Fine-Tuning - GPEFT, a novel approach for efficient graph representation learning with LLMs on text-rich graphs. Specifically, we utilize a graph neural network (GNN) to encode structural information from neighboring nodes into a graph prompt. This prompt is then inserted at the beginning of the text sequence. To improve the quality of graph prompts, we pre-trained the GNN to assist the frozen LLM in predicting the next token in the node text. Compared with existing joint GNN and LMs, our method directly generate the node embeddings from large language models with an affordable fine-tuning cost. We validate our approach through comprehensive experiments conducted on 8 different text-rich graphs, observing an average improvement of 2% in hit@1 and Mean Reciprocal Rank (MRR) in link prediction evaluations. Our results demonstrate the efficacy and efficiency of our model, showing that it can be smoothly integrated with various large language models, including OPT, LLaMA and Falcon.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2299062897",
                    "name": "Qi Zhu"
                },
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "2118943843",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "2145408511",
                    "name": "Shichang Zhang"
                },
                {
                    "authorId": "2057050247",
                    "name": "Bowen Jin"
                },
                {
                    "authorId": "2279734673",
                    "name": "Yizhou Sun"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "a6d2eb2e7bf518e93595e80e61127176f36204f5",
            "title": "Hierarchical Compression of Text-Rich Graphs via Large Language Models",
            "abstract": "Text-rich graphs, prevalent in data mining contexts like e-commerce and academic graphs, consist of nodes with textual features linked by various relations. Traditional graph machine learning models, such as Graph Neural Networks (GNNs), excel in encoding the graph structural information, but have limited capability in handling rich text on graph nodes. Large Language Models (LLMs), noted for their superior text understanding abilities, offer a solution for processing the text in graphs but face integration challenges due to their limitation for encoding graph structures and their computational complexities when dealing with extensive text in large neighborhoods of interconnected nodes. This paper introduces ``Hierarchical Compression'' (HiCom), a novel method to align the capabilities of LLMs with the structure of text-rich graphs. HiCom processes text in a node's neighborhood in a structured manner by organizing the extensive textual information into a more manageable hierarchy and compressing node text step by step. Therefore, HiCom not only preserves the contextual richness of the text but also addresses the computational challenges of LLMs, which presents an advancement in integrating the text processing power of LLMs with the structural complexities of text-rich graphs. Empirical results show that HiCom can outperform both GNNs and LLM backbones for node classification on e-commerce and citation graphs. HiCom is especially effective for nodes from a dense region in a graph, where it achieves a 3.48% average performance improvement on five datasets while being more efficient than LLM backbones.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145408511",
                    "name": "Shichang Zhang"
                },
                {
                    "authorId": "2283934850",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "2258747730",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "2299062897",
                    "name": "Qi Zhu"
                },
                {
                    "authorId": "2284037254",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "2263543517",
                    "name": "Christos Faloutsos"
                },
                {
                    "authorId": "2064547804",
                    "name": "George Karypis"
                },
                {
                    "authorId": "2279734673",
                    "name": "Yizhou Sun"
                }
            ]
        },
        {
            "paperId": "4008f607e29cfe6c0cce0b5ae119827380b99031",
            "title": "PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous Link Prediction",
            "abstract": "Transparency and accountability have become major concerns for black-box machine learning (ML) models. Proper explanations for the model behavior increase model transparency and help researchers develop more accountable models. Graph neural networks (GNN) have recently shown superior performance in many graph ML problems than traditional methods, and explaining them has attracted increased interest. However, GNN explanation for link prediction (LP) is lacking in the literature. LP is an essential GNN task and corresponds to web applications like recommendation and sponsored search on web. Given existing GNN explanation methods only address node/graph-level tasks, we propose Path-based GNN Explanation for heterogeneous Link prediction (PaGE-Link) that generates explanations with connection interpretability, enjoys model scalability, and handles graph heterogeneity. Qualitatively, PaGE-Link can generate explanations as paths connecting a node pair, which naturally captures connections between the two nodes and easily transfer to human-interpretable explanations. Quantitatively, explanations generated by PaGE-Link improve AUC for recommendation on citation and user-item graphs by 9 - 35% and are chosen as better by 78.79% of responses in human evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145408511",
                    "name": "Shichang Zhang"
                },
                {
                    "authorId": "73329314",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "2118943843",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "2109461904",
                    "name": "Yizhou Sun"
                }
            ]
        },
        {
            "paperId": "4993258852711c4e3d0011325ac3db680eae84f4",
            "title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
            "abstract": "Most of the existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that the current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2224092326",
                    "name": "Xiaoxuan Wang"
                },
                {
                    "authorId": "3407296",
                    "name": "Ziniu Hu"
                },
                {
                    "authorId": "2887562",
                    "name": "Pan Lu"
                },
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "47540245",
                    "name": "Jieyu Zhang"
                },
                {
                    "authorId": "2224018745",
                    "name": "Satyen Subramaniam"
                },
                {
                    "authorId": "2224017080",
                    "name": "Arjun R. Loomba"
                },
                {
                    "authorId": "2145408511",
                    "name": "Shichang Zhang"
                },
                {
                    "authorId": "2109461904",
                    "name": "Yizhou Sun"
                },
                {
                    "authorId": "2158624285",
                    "name": "Wei Wang"
                }
            ]
        },
        {
            "paperId": "894d61c709ec6f61899703458d90b09c663d7b11",
            "title": "A Survey on Graph Neural Network Acceleration: Algorithms, Systems, and Customized Hardware",
            "abstract": "Graph neural networks (GNNs) are emerging for machine learning research on graph-structured data. GNNs achieve state-of-the-art performance on many tasks, but they face scalability challenges when it comes to real-world applications that have numerous data and strict latency requirements. Many studies have been conducted on how to accelerate GNNs in an effort to address these challenges. These acceleration techniques touch on various aspects of the GNN pipeline, from smart training and inference algorithms to efficient systems and customized hardware. As the amount of research on GNN acceleration has grown rapidly, there lacks a systematic treatment to provide a unified view and address the complexity of relevant works. In this survey, we provide a taxonomy of GNN acceleration, review the existing approaches, and suggest future research directions. Our taxonomic treatment of GNN acceleration connects the existing works and sets the stage for further development in this area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145408511",
                    "name": "Shichang Zhang"
                },
                {
                    "authorId": "2242000525",
                    "name": "Atefeh Sohrabizadeh"
                },
                {
                    "authorId": "2158998114",
                    "name": "Cheng Wan"
                },
                {
                    "authorId": "12318198",
                    "name": "Zijie Huang"
                },
                {
                    "authorId": "3407296",
                    "name": "Ziniu Hu"
                },
                {
                    "authorId": "27035995",
                    "name": "Yewen Wang"
                },
                {
                    "authorId": "3138925",
                    "name": "Yingyan Lin"
                },
                {
                    "authorId": "2066032921",
                    "name": "J. Cong"
                },
                {
                    "authorId": "2109461904",
                    "name": "Yizhou Sun"
                }
            ]
        },
        {
            "paperId": "bfd7d84e19e4b486cf1a318ef9a994e55c1aebae",
            "title": "Predicting and Interpreting Energy Barriers of Metallic Glasses with Graph Neural Networks",
            "abstract": "Metallic Glasses (MGs) are widely used materials that are stronger than steel while being shapeable as plastic. While understanding the structure-property relationship of MGs remains a challenge in materials science, studying their energy barriers (EBs) as an intermediary step shows promise. In this work, we utilize Graph Neural Networks (GNNs) to model MGs and study EBs. We contribute a new dataset for EB prediction and a novel Symmetrized GNN (SymGNN) model that is E(3)-invariant in expectation. SymGNN handles invariance by aggregating over orthogonal transformations of the graph structure. When applied to EB prediction, SymGNN are more accurate than molecular dynamics (MD) local-sampling methods and other machine-learning models. Compared to precise MD simulations, SymGNN reduces the inference time on new MGs from roughly 41 days to less than one second. We apply explanation algorithms to reveal the relationship between structures and EBs. The structures that we identify through explanations match the medium-range order (MRO) hypothesis and possess unique topological properties. Our work enables effective prediction and interpretation of MG EBs, bolstering material science research.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2279827977",
                    "name": "Haoyu Li"
                },
                {
                    "authorId": "2145408511",
                    "name": "Shichang Zhang"
                },
                {
                    "authorId": "82853137",
                    "name": "Longwen Tang"
                },
                {
                    "authorId": "2279753206",
                    "name": "Mathieu Bauchy"
                },
                {
                    "authorId": "2279734673",
                    "name": "Yizhou Sun"
                }
            ]
        },
        {
            "paperId": "2be6804469b99d9c336ef0fe146f9a9ffaa75282",
            "title": "Linkless Link Prediction via Relational Distillation",
            "abstract": "Graph Neural Networks (GNNs) have shown exceptional performance in the task of link prediction. Despite their effectiveness, the high latency brought by non-trivial neighborhood data dependency limits GNNs in practical deployments. Conversely, the known efficient MLPs are much less effective than GNNs due to the lack of relational knowledge. In this work, to combine the advantages of GNNs and MLPs, we start with exploring direct knowledge distillation (KD) methods for link prediction, i.e., predicted logit-based matching and node representation-based matching. Upon observing direct KD analogs do not perform well for link prediction, we propose a relational KD framework, Linkless Link Prediction (LLP), to distill knowledge for link prediction with MLPs. Unlike simple KD methods that match independent link logits or node representations, LLP distills relational knowledge that is centered around each (anchor) node to the student MLP. Specifically, we propose rank-based matching and distribution-based matching strategies that complement each other. Extensive experiments demonstrate that LLP boosts the link prediction performance of MLPs with significant margins, and even outperforms the teacher GNNs on 7 out of 8 benchmarks. LLP also achieves a 70.68x speedup in link prediction inference compared to GNNs on the large-scale OGB dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109411071",
                    "name": "Zhichun Guo"
                },
                {
                    "authorId": "2094369655",
                    "name": "William Shiao"
                },
                {
                    "authorId": "2145408511",
                    "name": "Shichang Zhang"
                },
                {
                    "authorId": "152891495",
                    "name": "Yozen Liu"
                },
                {
                    "authorId": "144539424",
                    "name": "N. Chawla"
                },
                {
                    "authorId": "145474474",
                    "name": "Neil Shah"
                },
                {
                    "authorId": "1742573",
                    "name": "Tong Zhao"
                }
            ]
        },
        {
            "paperId": "699a946c7e519e4f288baae422bae8920070bff3",
            "title": "Explaining Graph-level Predictions with Communication Structure-Aware Cooperative Games",
            "abstract": "Explaining predictions made by machine learning models is important and have attracted an increased interest. The Shapley value from cooperative game theory has been proposed as a prime approach to compute feature importances towards predictions, especially for images, text, tabular data, and recently graph neural networks (GNNs) on graphs. In this work, we revisit the appropriateness of the Shapley value for graph explanation, where the task is to identify the most important subgraph and constituent nodes for graph-level predictions. We purport that the Shapley value is a no-ideal choice for graph data because it is by definition not structure-aware. We propose a Graph Structure-aware eXplanation (GStarX) method to leverage the critical graph structure information to improve the explanation. Speci\ufb01cally, we propose a scoring function based on a new structure-aware value from the cooperative game theory called the HN value. When used to score node importance, the HN value utilizes graph structures to attribute cooperation surplus between neighbor nodes, re-sembling message passing in GNNs, so that node importance scores re\ufb02ect not only the node feature importance, but also the structural roles. We demonstrate that GstarX produces qualitatively more intuitive explanations, and quantitatively improves over strong baselines on chemical graph property prediction and text graph sentiment clas-si\ufb01cation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145408511",
                    "name": "Shichang Zhang"
                },
                {
                    "authorId": "145474474",
                    "name": "Neil Shah"
                },
                {
                    "authorId": "152891495",
                    "name": "Yozen Liu"
                },
                {
                    "authorId": "2109461904",
                    "name": "Yizhou Sun"
                }
            ]
        }
    ]
}