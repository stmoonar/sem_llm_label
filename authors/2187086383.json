{
    "authorId": "2187086383",
    "papers": [
        {
            "paperId": "06a032fbfb5511f65ab23c61ec946e9f2d1d4ab0",
            "title": "Understand Data Preprocessing for Effective End-to-End Training of Deep Neural Networks",
            "abstract": "In this paper, we primarily focus on understanding the data preprocessing pipeline for DNN Training in the public cloud. First, we run experiments to test the performance implications of the two major data preprocessing methods using either raw data or record files. The preliminary results show that data preprocessing is a clear bottleneck, even with the most efficient software and hardware configuration enabled by NVIDIA DALI, a high-optimized data preprocessing library. Second, we identify the potential causes, exercise a variety of optimization methods, and present their pros and cons. We hope this work will shed light on the new co-design of ``data storage, loading pipeline'' and ``training framework'' and flexible resource configurations between them so that the resources can be fully exploited and performance can be maximized.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2134719313",
                    "name": "Ping Gong"
                },
                {
                    "authorId": "2317038420",
                    "name": "Yuxin Ma"
                },
                {
                    "authorId": "2041687170",
                    "name": "Cheng-rong Li"
                },
                {
                    "authorId": "2187086383",
                    "name": "Xiaosong Ma"
                },
                {
                    "authorId": "1719212",
                    "name": "S. Noh"
                }
            ]
        },
        {
            "paperId": "41dc4b7eee87541f1d829951970a8c335bf35201",
            "title": "Towards Unbiased Training in Federated Open-world Semi-supervised Learning",
            "abstract": "Federated Semi-supervised Learning (FedSSL) has emerged as a new paradigm for allowing distributed clients to collaboratively train a machine learning model over scarce labeled data and abundant unlabeled data. However, existing works for FedSSL rely on a closed-world assumption that all local training data and global testing data are from seen classes observed in the labeled dataset. It is crucial to go one step further: adapting FL models to an open-world setting, where unseen classes exist in the unlabeled data. In this paper, we propose a novel Federatedopen-world Semi-Supervised Learning (FedoSSL) framework, which can solve the key challenge in distributed and open-world settings, i.e., the biased training process for heterogeneously distributed unseen classes. Specifically, since the advent of a certain unseen class depends on a client basis, the locally unseen classes (exist in multiple clients) are likely to receive differentiated superior aggregation effects than the globally unseen classes (exist only in one client). We adopt an uncertainty-aware suppressed loss to alleviate the biased training between locally unseen and globally unseen classes. Besides, we enable a calibration module supplementary to the global aggregation to avoid potential conflicting knowledge transfer caused by inconsistent data distribution among different clients. The proposed FedoSSL can be easily adapted to state-of-the-art FL methods, which is also validated via extensive experiments on benchmarks and real-world datasets (CIFAR-10, CIFAR-100 and CINIC-10).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40539618",
                    "name": "J. Zhang"
                },
                {
                    "authorId": "2187086383",
                    "name": "Xiaosong Ma"
                },
                {
                    "authorId": "2162793936",
                    "name": "Song Guo"
                },
                {
                    "authorId": "50232004",
                    "name": "Wenchao Xu"
                }
            ]
        },
        {
            "paperId": "7687dcf2495c3d70ff0309592a3697d358f209d4",
            "title": "Ten Years after ImageNet: A 360{\\deg} Perspective on AI",
            "abstract": "It is ten years since neural networks made their spectacular comeback. Prompted by this anniversary, we take a holistic perspective on Artificial Intelligence (AI). Supervised Learning for cognitive tasks is effectively solved - provided we have enough high-quality labeled data. However, deep neural network models are not easily interpretable, and thus the debate between blackbox and whitebox modeling has come to the fore. The rise of attention networks, self-supervised learning, generative modeling, and graph neural networks has widened the application space of AI. Deep Learning has also propelled the return of reinforcement learning as a core building block of autonomous decision making systems. The possible harms made possible by new AI technologies have raised socio-technical issues such as transparency, fairness, and accountability. The dominance of AI by Big-Tech who control talent, computing resources, and most importantly, data may lead to an extreme AI divide. Failure to meet high expectations in high profile, and much heralded flagship projects like self-driving vehicles could trigger another AI winter.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50793091",
                    "name": "S. Chawla"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2141768778",
                    "name": "Ahmed Ali"
                },
                {
                    "authorId": "152252333",
                    "name": "Wendy Hall"
                },
                {
                    "authorId": "2186981636",
                    "name": "Issa M. Khalil"
                },
                {
                    "authorId": "2187086383",
                    "name": "Xiaosong Ma"
                },
                {
                    "authorId": "1772206",
                    "name": "H. Sencar"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                },
                {
                    "authorId": "2059979016",
                    "name": "Michael Wooldridge"
                },
                {
                    "authorId": "2151684774",
                    "name": "Tingyue Yu"
                }
            ]
        }
    ]
}