{
    "authorId": "49095886",
    "papers": [
        {
            "paperId": "18a9aea9f947017b524a7eff1f3a18ec7acc23ac",
            "title": "Sustainable AI Regulation",
            "abstract": "Current proposals for AI regulation, in the EU and beyond, aim to spur AI that is trustworthy (e.g., AI Act) and accountable (e.g., AI Liability) What is missing, however, is a robust regulatory discourse and roadmap to make AI, and technology more broadly, environmentally sustainable. This paper aims to take first steps to fill this gap. The ICT sector contributes up to 3.9 percent of global greenhouse gas (GHG) emissions-more than global air travel at 2.5 percent. The carbon footprint and water consumption of AI, especially large-scale generative models like GPT-4, raise significant sustainability concerns. The paper is the first to assess how current and proposed technology regulations, including EU environmental law, the General Data Protection Regulation (GDPR), and the AI Act, could be adjusted to better account for environmental sustainability. The GDPR, for instance, could be interpreted to limit certain individual rights like the right to erasure if these rights significantly conflict with broader sustainability goals. In a second step, the paper suggests a multi-faceted approach to achieve sustainable AI regulation. It advocates for transparency mechanisms, such as disclosing the GHG footprint of AI systems, as laid out in the proposed EU AI Act. However, sustainable AI regulation must go beyond mere transparency. The paper proposes a regulatory toolkit comprising co-regulation, sustainability-by-design principles, restrictions on training data, and consumption caps, including integration into the EU Emissions Trading Scheme. Finally, the paper argues that this regulatory toolkit could serve as a blueprint for regulating other high-emission technologies and infrastructures like blockchain, Metaverse applications, and data centers. The framework aims to cohesively address the crucial dual challenges of our era: digital transformation and climate change mitigation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49095886",
                    "name": "P. Hacker"
                }
            ]
        },
        {
            "paperId": "4fb2223814a300631932a719ed731437d398d261",
            "title": "Fairness and Bias in Algorithmic Hiring",
            "abstract": "Employers are adopting algorithmic hiring technology throughout the recruitment pipeline. Algorithmic fairness is especially applicable in this domain due to its high stakes and structural inequalities. Unfortunately, most work in this space provides partial treatment, often constrained by two competing narratives, optimistically focused on replacing biased recruiter decisions or pessimistically pointing to the automation of discrimination. Whether, and more importantly what types of , algorithmic hiring can be less biased and more beneficial to society than low-tech alternatives currently remains unanswered, to the detriment of trustworthiness. This multidisciplinary survey caters to practitioners and researchers with a balanced and integrated coverage of systems, biases, measures, mitigation strategies, datasets, and legal aspects of algorithmic hiring and fairness. Our work supports a contextualized understanding and governance of this technology by highlighting current opportunities and limitations, providing recommendations for future work to ensure shared benefits for all stakeholders.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35340264",
                    "name": "Alessandro Fabris"
                },
                {
                    "authorId": "2103520330",
                    "name": "N. Baranowska"
                },
                {
                    "authorId": "34588657",
                    "name": "M. Dennis"
                },
                {
                    "authorId": "49095886",
                    "name": "P. Hacker"
                },
                {
                    "authorId": "2788991",
                    "name": "Jorge Saldivar"
                },
                {
                    "authorId": "3219305",
                    "name": "F. Z. Borgesius"
                },
                {
                    "authorId": "2133238109",
                    "name": "Asia J. Biega"
                }
            ]
        },
        {
            "paperId": "bd166912fa16cec04e3ee044a0d0fb6e3ac6fe33",
            "title": "Regulating ChatGPT and other Large Generative AI Models",
            "abstract": "Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49095886",
                    "name": "P. Hacker"
                },
                {
                    "authorId": "2053831555",
                    "name": "A. Engel"
                },
                {
                    "authorId": "36216751",
                    "name": "M. Mauer"
                }
            ]
        },
        {
            "paperId": "c690d0f9d6d051b22481768302d62ff93ef16c31",
            "title": "The European AI Liability Directives - Critique of a Half-Hearted Approach and Lessons for the Future",
            "abstract": "As ChatGPT et al. conquer the world, the optimal liability framework for AI systems remains an unsolved problem across the globe. In a much-anticipated move, the European Commission advanced two proposals outlining the European approach to AI liability in September 2022: a novel AI Liability Directive and a revision of the Product Liability Directive. They constitute the final cornerstone of EU AI regulation. Crucially, the liability proposals and the EU AI Act are inherently intertwined: the latter does not contain any individual rights of affected persons, and the former lack specific, substantive rules on AI development and deployment. Taken together, these acts may well trigger a Brussels Effect in AI regulation, with significant consequences for the US and beyond. This paper makes three novel contributions. First, it examines in detail the Commission proposals and shows that, while making steps in the right direction, they ultimately represent a half-hearted approach: if enacted as foreseen, AI liability in the EU will primarily rest on disclosure of evidence mechanisms and a set of narrowly defined presumptions concerning fault, defectiveness and causality. Hence, second, the article suggests amendments, which are collected in an Annex at the end of the paper. Third, based on an analysis of the key risks AI poses, the final part of the paper maps out a road for the future of AI liability and regulation, in the EU and beyond. This includes: a comprehensive framework for AI liability; provisions to support innovation; an extension to non-discrimination/algorithmic fairness, as well as explainable AI; and sustainability. I propose to jump-start sustainable AI regulation via sustainability impact assessments in the AI Act and sustainable design defects in the liability regime. In this way, the law may help spur not only fair AI and XAI, but potentially also sustainable AI (SAI).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49095886",
                    "name": "P. Hacker"
                }
            ]
        },
        {
            "paperId": "e2cd590b3145305f86d86c96af354043b68f8aea",
            "title": "AI Compliance \u2013 Challenges of Bridging Data Science and Law",
            "abstract": "This vision article outlines the main building blocks of what we term AI Compliance, an effort to bridge two complementary research areas: computer science and the law. Such research has the goal to model, measure, and affect the quality of AI artifacts, such as data, models, and applications, to then facilitate adherence to legal standards.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49095886",
                    "name": "P. Hacker"
                },
                {
                    "authorId": "1488674669",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "2164140976",
                    "name": "Tobias Friedrich"
                },
                {
                    "authorId": "37138315",
                    "name": "Stefan Grundmann"
                },
                {
                    "authorId": "2164140639",
                    "name": "Anja Lehmann"
                },
                {
                    "authorId": "2085517465",
                    "name": "Herbert Zech"
                }
            ]
        },
        {
            "paperId": "e82278b5463f87b711886f46a22e1cb52f7b38f7",
            "title": "Regulating Gatekeeper AI and Data: Transparency, Access, and Fairness under the DMA, the GDPR, and beyond",
            "abstract": "Artificial intelligence is not only increasingly used in business and administration contexts, but a race for its regulation is also underway, with the EU spearheading the efforts. Contrary to existing literature, this article suggests, however, that the most far-reaching and effective EU rules for AI applications in the digital economy will not be contained in the proposed AI Act - but have just been enacted in the Digital Markets Act. We analyze the impact of the DMA and related EU acts on AI models and their underlying data across four key areas: disclosure requirements; the regulation of AI training data; access rules; and the regime for fair rankings. The paper demonstrates that fairness, in the sense of the DMA, goes beyond traditionally protected categories of non-discrimination law on which scholarship at the intersection of AI and law has so far largely focused on. Rather, we draw on competition law and the FRAND criteria known from intellectual property law to interpret and refine the DMA provisions on fair rankings. Moreover, we show how, based on CJEU jurisprudence, a coherent interpretation of the concept of non-discrimination in both traditional non-discrimination and competition law may be found. The final part sketches specific proposals for a comprehensive framework of transparency, access, and fairness under the DMA and beyond.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49095886",
                    "name": "P. Hacker"
                },
                {
                    "authorId": "2195521316",
                    "name": "Johann Cordes"
                },
                {
                    "authorId": "2195815029",
                    "name": "Janina Rochon"
                }
            ]
        },
        {
            "paperId": "59235ad3ed97bc093aa028853e48148c7ea028e6",
            "title": "A Legal Framework for AI Training Data",
            "abstract": "Building on the recently published White Paper of the EU Commission on Artificial Intelligence (AI), this article shows that training data for AI do not only play a key role in the development of AI applications, but are currently only inadequately captured by EU law. In this, I focus on three central risks of AI training data: risks of data quality, discrimination and innovation. Existing EU law, with the new copyright exception for text and data mining, only addresses a part of this risk profile adequately. Therefore, the article develops the foundations for a discrimination-sensitive quality regime for data sets and AI training, which emancipates itself from the controversial question of the applicability of data protection law to AI training data. Furthermore, it spells out concrete guidelines for the re-use of personal data for AI training purposes under the GDPR. Ultimately, the legislative and interpretive task rests in striking an appropriate balance between individual protection and the promotion of innovation. The law ought to provide support for the dynamic development of new AI models, but must also, where necessary, shape them in a socially desirable manner.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49095886",
                    "name": "P. Hacker"
                }
            ]
        },
        {
            "paperId": "ce6778e1171fc2c229e99e61c4b8ff089efc1a79",
            "title": "Towards a Flexible Framework for Algorithmic Fairness",
            "abstract": "Increasingly, scholars seek to integrate legal and technological insights to combat bias in AI systems. In recent years, many different definitions for ensuring non-discrimination in algorithmic decision systems have been put forward. In this paper, we first briefly describe the EU law framework covering cases of algorithmic discrimination. Second, we present an algorithm that harnesses optimal transport to provide a flexible framework to interpolate between different fairness definitions. Third, we show that important normative and legal challenges remain for the implementation of algorithmic fairness interventions in real-world scenarios. Overall, the paper seeks to contribute to the quest for flexible technical frameworks that can be adapted to varying legal and normative fairness constraints.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49095886",
                    "name": "P. Hacker"
                },
                {
                    "authorId": "1398540770",
                    "name": "Emil Wiedemann"
                },
                {
                    "authorId": "41154657",
                    "name": "Meike Zehlike"
                }
            ]
        },
        {
            "paperId": "5ca8301e5934d20687f44b4ecf130a04557076ae",
            "title": "Regulating Under Uncertainty About Rationality: From Decision Theory to Machine Learning and Complexity Theory",
            "abstract": "Theories of choice, and their legal consequences, dramatically differ based on whether they are premised on rational or boundedly rational actors. This chapter describes the interactions between, and regulatory implications of, three types of uncertainties that the selection of an adequate theory of choice requires. First, it suggests that Knightian uncertainty concerning the distribution of degrees of rationality between regulatees obtains in many regulatory areas. More recently, this has been described as a \u2018knowledge problem\u2019 of behavioural law and economics. This chapter argues that the best regulatory response to the knowledge problem is to frame regulation as a problem of decision making under uncertainty. Second, with the rise of machine learning, it is arguably becoming ever more possible to estimate the level of bias, or even entire rationality quotients, of individual regulatees. This opens the potential for, but also the pitfalls of, personalised law. Third, even Big Data analytics generally only offers a snapshot of a distribution of rationality at one moment in time. Recent economic analyses have suggested behavioural heterogeneity can evolve over time in unpredicted ways that may lead to unforeseen consequences, leading to economic complexity. This calls for a greater role of standards, as opposed to rules, in regulating environments with dynamic behavioural heterogeneity. The chapter focuses on the normative implications of different types of uncertainty. In the end, theories of choice can aid more transparent normative trade-offs but they cannot replace the value judgments, and normative discourses, that balance the involved interests.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49095886",
                    "name": "P. Hacker"
                }
            ]
        },
        {
            "paperId": "cdddbaedd46862ed465f402c78625caa5638cb66",
            "title": "Teaching fairness to artificial intelligence: Existing and novel strategies against algorithmic discrimination under EU law",
            "abstract": "Empirical evidence is mounting that artificial intelligence applications threaten to discriminate against legally protected groups. This raises intricate questions for EU law. The existing categories of EU anti-discrimination law do not provide an easy fit for algorithmic decision making. Furthermore, victims won\u2019t be able to prove their case without access to the data and the algorithmic models. Drawing on a growing computer science literature on algorithmic fairness, this article suggests an integrated vision of anti-discrimination and data protection law to enforce fairness in the digital age. It shows how the concepts of anti-discrimination law may be combined with algorithmic audits and data protection impact assessments in an effort to unlock the algorithmic black box.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49095886",
                    "name": "P. Hacker"
                }
            ]
        }
    ]
}