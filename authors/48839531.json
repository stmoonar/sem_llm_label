{
    "authorId": "48839531",
    "papers": [
        {
            "paperId": "a2223e85a744a61c508d1ab3b6c3901941960485",
            "title": "When Newer is Not Better: Does Deep Learning Really Benefit Recommendation From Implicit Feedback?",
            "abstract": "In recent years, neural models have been repeatedly touted to exhibit state-of-the-art performance in recommendation. Nevertheless, multiple recent studies have revealed that the reported state-of-the-art results of many neural recommendation models cannot be reliably replicated. A primary reason is that existing evaluations are performed under various inconsistent protocols. Correspondingly, these replicability issues make it difficult to understand how much benefit we can actually gain from these neural models. It then becomes clear that a fair and comprehensive performance comparison between traditional and neural models is needed. Motivated by these issues, we perform a large-scale, systematic study to compare recent neural recommendation models against traditional ones in top-n recommendation from implicit data. We propose a set of evaluation strategies for measuring memorization performance, generalization performance, and subgroup-specific performance of recommendation models. We conduct extensive experiments with 13 popular recommendation models (including two neural models and 11 traditional ones as baselines) on nine commonly used datasets. Our experiments demonstrate that even with extensive hyper-parameter searches, neural models do not dominate traditional models in all aspects, e.g., they fare worse in terms of average HitRate. We further find that there are areas where neural models seem to outperform non-neural models, for example, in recommendation diversity and robustness between different subgroups of users and items. Our work illuminates the relative advantages and disadvantages of neural models in recommendation and is therefore an important step towards building better recommender systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123918726",
                    "name": "Yushun Dong"
                },
                {
                    "authorId": "1737121128",
                    "name": "Jundong Li"
                },
                {
                    "authorId": "48839531",
                    "name": "Tobias Schnabel"
                }
            ]
        },
        {
            "paperId": "c856d1a955e185bfa5711bf2b0fe1aa64f3e9abc",
            "title": "Foreword for Workshop on Decision Making for Information Retrieval and Recommender Systems",
            "abstract": "1 FOREWORD Most of the recent progress in information retrieval (IR) and recommender systems has been fueled by deep learning: the success of neural networks has brought tremendous opportunities to model highly complex patterns for prediction. However, algorithmic advances on accurate predictions and improved user modeling are just a small part of designing considerations of a much larger system. IR and recommender systems difer from other machine learning domains because they are inherently part of an ecosystem \u2013 in the simplest case, a world of items and users. In these ecosystems, system designers face a broad range of decisions \u2013 e.g., how to balance popularity, which incentives should be given to which users, or what safeguards to put in place to ensure the platform thrives in the long-run. Some of the decision making problems are statistical and algorithmic in nature, such as coping with the uncertainty of data and models, while others can involve building systems for the satisfactory of multiple parties. In general, there are many complex decision-making challenges faced by real-world IR and recommender systems, but existing approaches often make oversimplifed assumptions about the environment, data, and human behavior. Ignoring those challenges or treating them simply as pattern recognition problems can cost the engagement, accessibility, fairness, inclusiveness, and ultimately the vitality of IR and recommendation systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118285164",
                    "name": "Da Xu"
                },
                {
                    "authorId": "48839531",
                    "name": "Tobias Schnabel"
                },
                {
                    "authorId": "2087078033",
                    "name": "Xiquan Cui"
                },
                {
                    "authorId": "2166476321",
                    "name": "Sarah Dean"
                },
                {
                    "authorId": "143736660",
                    "name": "Aniket Deshmukh"
                },
                {
                    "authorId": "2156653838",
                    "name": "Bo Yang"
                },
                {
                    "authorId": "2112273246",
                    "name": "Shipeng Yu"
                }
            ]
        },
        {
            "paperId": "2ff388eb4516519660eb9b4a006f90ed4d67c40b",
            "title": "HINT: Integration Testing for AI-based features with Humans in the Loop",
            "abstract": "The dynamic nature of AI technologies makes testing human-AI interaction and collaboration challenging \u2013 especially before such features are deployed in the wild. This presents a challenge for designers and AI practitioners as early feedback for iteration is often unavailable in the development phase. In this paper, we take inspiration from integration testing concepts in software development and present HINT (Human-AI INtegration Testing), a crowd-based framework for testing AI-based experiences integrated with a humans-in-the-loop workflow. HINT supports early testing of AI-based features within the context of realistic user tasks and makes use of successive sessions to simulate AI experiences that evolve over-time. Finally, it provides practitioners with reports to evaluate and compare aspects of these experiences. Through a crowd-based study, we demonstrate the need for over-time testing where user behaviors evolve as they interact with an AI system. We also show that HINT is able to capture and reveal these distinct user behavior patterns across a variety of common AI performance modalities using two AI-based feature prototypes. We further evaluated HINT\u2019s potential to support practitioners\u2019 evaluation of human-AI interaction experiences pre-deployment through semi-structured interviews with 13 practitioners.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3207763",
                    "name": "Quan Ze Chen"
                },
                {
                    "authorId": "48839531",
                    "name": "Tobias Schnabel"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                },
                {
                    "authorId": "1719124",
                    "name": "Saleema Amershi"
                }
            ]
        },
        {
            "paperId": "3bf03ca39f8d0ff0967a6dafcaa755fcf0e216bc",
            "title": "Where Do We Go From Here? Guidelines For Offline Recommender Evaluation",
            "abstract": "Various studies in recent years have pointed out large issues in the offline evaluation of recommender systems, making it difficult to assess whether true progress has been made. However, there has been little research into what set of practices should serve as a starting point during experimentation. In this paper, we examine four larger issues in recommender system research regarding uncertainty estimation, generalization, hyperparameter optimization and dataset pre-processing in more detail to arrive at a set of guidelines. We present a TrainRec, a lightweight and flexible toolkit for offline training and evaluation of recommender systems that implements these guidelines. Different from other frameworks, TrainRec is a toolkit that focuses on experimentation alone, offering flexible modules that can be can be used together or in isolation. Finally, we demonstrate TrainRec's usefulness by evaluating a diverse set of twelve baselines across ten datasets. Our results show that (i) many results on smaller datasets are likely not statistically significant, (ii) there are at least three baselines that perform well on most datasets and should be considered in future experiments, and (iii) improved uncertainty quantification (via nested CV and statistical testing) rules out some reported differences between linear and neural methods. Given these results, we advocate that future research should standardize evaluation using our suggested guidelines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48839531",
                    "name": "Tobias Schnabel"
                }
            ]
        },
        {
            "paperId": "81b7ee490cb91ae4dc936e7a3170b691f28d235a",
            "title": "EvalRS: a rounded evaluation of recommender systems",
            "abstract": "Much of the complexity of Recommender Systems (RSs) comes from the fact that they are used as part of more complex applications and affect user experience through a varied range of user interfaces. However, research focused almost exclusively on the ability of RSs to produce accurate item rankings while giving little attention to the evaluation of RS behavior in real-world scenarios. Such narrow focus has limited the capacity of RSs to have a lasting impact in the real world and makes them vulnerable to undesired behavior, such as reinforcing data biases. We propose EvalRS as a new type of challenge, in order to foster this discussion among practitioners and build in the open new methodologies for testing RSs\"in the wild\".",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1964040776",
                    "name": "Jacopo Tagliabue"
                },
                {
                    "authorId": "49224924",
                    "name": "Federico Bianchi"
                },
                {
                    "authorId": "48839531",
                    "name": "Tobias Schnabel"
                },
                {
                    "authorId": "1481857041",
                    "name": "Giuseppe Attanasio"
                },
                {
                    "authorId": "144868680",
                    "name": "C. Greco"
                },
                {
                    "authorId": "3077104",
                    "name": "G. Moreira"
                },
                {
                    "authorId": "2071967689",
                    "name": "P. Chia"
                }
            ]
        },
        {
            "paperId": "cebc7bc086f56bbee87237ad91c35d291c4bf7ac",
            "title": "Lightweight Compositional Embeddings for Incremental Streaming Recommendation",
            "abstract": "Most work in graph-based recommender systems considers a {\\em static} setting where all information about test nodes (i.e., users and items) is available upfront at training time. However, this static setting makes little sense for many real-world applications where data comes in continuously as a stream of new edges and nodes, and one has to update model predictions incrementally to reflect the latest state. To fully capitalize on the newly available data in the stream, recent graph-based recommendation models would need to be repeatedly retrained, which is infeasible in practice. In this paper, we study the graph-based streaming recommendation setting and propose a compositional recommendation model -- Lightweight Compositional Embedding (LCE) -- that supports incremental updates under low computational cost. Instead of learning explicit embeddings for the full set of nodes, LCE learns explicit embeddings for only a subset of nodes and represents the other nodes {\\em implicitly}, through a composition function based on their interactions in the graph. This provides an effective, yet efficient, means to leverage streaming graph data when one node type (e.g., items) is more amenable to static representation. We conduct an extensive empirical study to compare LCE to a set of competitive baselines on three large-scale user-item recommendation datasets with interactions under a streaming setting. The results demonstrate the superior performance of LCE, showing that it achieves nearly skyline performance with significantly fewer parameters than alternative graph-based models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51119032",
                    "name": "Mengyue Hang"
                },
                {
                    "authorId": "48839531",
                    "name": "Tobias Schnabel"
                },
                {
                    "authorId": "1683235",
                    "name": "Longqi Yang"
                },
                {
                    "authorId": "144050371",
                    "name": "Jennifer Neville"
                }
            ]
        },
        {
            "paperId": "f13c2033d90344c4d4a95d764bbff1f94fd1b54b",
            "title": "Situating Recommender Systems in Practice: Towards Inductive Learning and Incremental Updates",
            "abstract": "With information systems becoming larger scale, recommendation systems are a topic of growing interest in machine learning research and industry. Even though progress on improving model design has been rapid in research, we argue that many advances fail to translate into practice because of two limiting assumptions. First, most approaches focus on a transductive learning setting which cannot handle unseen users or items and second, many existing methods are developed for static settings that cannot incorporate new data as it becomes available. We argue that these are largely impractical assumptions on real-world platforms where new user interactions happen in real time. In this survey paper, we formalize both concepts and contextualize recommender systems work from the last six years. We then discuss why and how future work should move towards inductive learning and incremental updates for recommendation model design and evaluation. In addition, we present best practices and fundamental open challenges for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48839531",
                    "name": "Tobias Schnabel"
                },
                {
                    "authorId": "2127553",
                    "name": "Mengting Wan"
                },
                {
                    "authorId": "1683235",
                    "name": "Longqi Yang"
                }
            ]
        },
        {
            "paperId": "a22d0d8c87549e07cedf4ebd84a60ef9766a2b8e",
            "title": "Local Factor Models for Large-Scale Inductive Recommendation",
            "abstract": "In many domains, user preferences are similar locally within like-minded subgroups of users, but typically differ globally between those subgroups. Local recommendation models were shown to substantially improve top-K recommendation performance in such settings. However, existing local models do not scale to large-scale datasets with an increasing number of subgroups and do not support inductive recommendations for users not appearing in the training set. Key reasons for this are that subgroup detection and recommendation get implemented as separate steps in the model or that local models are explicitly instantiated for each subgroup. In this paper, we propose an End-to-end Local Factor Model (Elfm) which overcomes these limitations by combining both steps and incorporating local structures through an inductive bias. Our model can be optimized end-to-end and supports incremental inference, does not require a full separate model for each subgroup, and has overall small memory and computational costs for incorporating local structures. Empirical results show that our method substantially improves recommendation performance on large-scale datasets with millions of users and items with considerably smaller model size. Our user study also shows that our approach produces coherent item subgroups which could aid in the generation of explainable recommendations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1683235",
                    "name": "Longqi Yang"
                },
                {
                    "authorId": "48839531",
                    "name": "Tobias Schnabel"
                },
                {
                    "authorId": "144609235",
                    "name": "Paul N. Bennett"
                },
                {
                    "authorId": "1728602",
                    "name": "S. Dumais"
                }
            ]
        },
        {
            "paperId": "ee1ef7b70dc34adcc90c42cc28168165ea56501f",
            "title": "SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization",
            "abstract": "In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SummaCConv that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SummaC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SummaCConv obtains state-of-the-art results with a balanced accuracy of 74.4%, a 5% improvement compared with prior work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46180754",
                    "name": "Philippe Laban"
                },
                {
                    "authorId": "48839531",
                    "name": "Tobias Schnabel"
                },
                {
                    "authorId": "144609235",
                    "name": "Paul N. Bennett"
                },
                {
                    "authorId": "1716902",
                    "name": "Marti A. Hearst"
                }
            ]
        },
        {
            "paperId": "f07029549bdb29a7afce1acd824fbe4e3dfb25d5",
            "title": "Keep It Simple: Unsupervised Simplification of Multi-Paragraph Text",
            "abstract": "This work presents Keep it Simple (KiS), a new approach to unsupervised text simplification which learns to balance a reward across three properties: fluency, salience and simplicity. We train the model with a novel algorithm to optimize the reward (k-SCST), in which the model proposes several candidate simplifications, computes each candidate\u2019s reward, and encourages candidates that outperform the mean reward. Finally, we propose a realistic text comprehension task as an evaluation method for text simplification. When tested on the English news domain, the KiS model outperforms strong supervised baselines by more than 4 SARI points, and can help people complete a comprehension task an average of 18% faster while retaining accuracy, when compared to the original text.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46180754",
                    "name": "Philippe Laban"
                },
                {
                    "authorId": "48839531",
                    "name": "Tobias Schnabel"
                },
                {
                    "authorId": "144609235",
                    "name": "Paul N. Bennett"
                },
                {
                    "authorId": "1716902",
                    "name": "Marti A. Hearst"
                }
            ]
        }
    ]
}