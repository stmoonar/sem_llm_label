{
    "authorId": "1693038",
    "papers": [
        {
            "paperId": "018784502f6ad74c7ec3861b35e6367425f16f54",
            "title": "LCE: An Augmented Combination of Bagging and Boosting in Python",
            "abstract": "lcensemble is a high-performing, scalable and user-friendly Python package for the general tasks of classification and regression. The package implements Local Cascade Ensemble (LCE), a machine learning method that further enhances the prediction performance of the current state-of-the-art methods Random Forest and XGBoost. LCE combines their strengths and adopts a complementary diversification approach to obtain a better generalizing predictor. The package is compatible with scikit-learn, therefore it can interact with scikit-learn pipelines and model selection tools. It is distributed under the Apache 2.0 license, and its source code is available at https://github.com/LocalCascadeEnsemble/LCE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41035652",
                    "name": "Kevin Fauvel"
                },
                {
                    "authorId": "1687778",
                    "name": "\u00c9lisa Fromont"
                },
                {
                    "authorId": "2673540",
                    "name": "V\u00e9ronique Masson"
                },
                {
                    "authorId": "143806005",
                    "name": "P. Faverdin"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "3b5cc75654e1e4152f4063cdd59486044e89f065",
            "title": "Generating robust counterfactual explanations",
            "abstract": "Counterfactual explanations have become a mainstay of the XAI field. This particularly intuitive statement allows the user to understand what small but necessary changes would have to be made to a given situation in order to change a model prediction. The quality of a counterfactual depends on several criteria: realism, actionability, validity, robustness, etc. In this paper, we are interested in the notion of robustness of a counterfactual. More precisely, we focus on robustness to counterfactual input changes. This form of robustness is particularly challenging as it involves a trade-off between the robustness of the counterfactual and the proximity with the example to explain. We propose a new framework, CROCO, that generates robust counterfactuals while managing effectively this trade-off, and guarantees the user a minimal robustness. An empirical evaluation on tabular datasets confirms the relevance and effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155042352",
                    "name": "Victor Guyomard"
                },
                {
                    "authorId": "2197518922",
                    "name": "Franccoise Fessant"
                },
                {
                    "authorId": "1678805",
                    "name": "Thomas Guyet"
                },
                {
                    "authorId": "1817990",
                    "name": "Tassadit Bouadi"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "8bfa76f3cbb6cd47ddd95fabb168569b8d136d9c",
            "title": "Persistence-Based Discretization for Learning Discrete Event Systems from Time Series",
            "abstract": "To get a good understanding of a dynamical system, it is convenient to have an interpretable and versatile model of it. Timed discrete event systems are a kind of model that respond to these requirements. However, such models can be inferred from timestamped event sequences but not directly from numerical data. To solve this problem, a discretization step must be done to identify events or symbols in the time series. Persist is a discretization method that intends to create persisting symbols by using a score called persistence score. This allows to mitigate the risk of undesirable symbol changes that would lead to a too complex model. After the study of the persistence score, we point out that it tends to favor excessive cases making it miss interesting persisting symbols. To correct this behavior, we replace the metric used in the persistence score, the Kullback-Leibler divergence, with the Wasserstein distance. Experiments show that the improved persistence score enhances Persist's ability to capture the information of the original time series and that it makes it better suited for discrete event systems learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107085268",
                    "name": "L\u00e9na\u00efg Cornanguer"
                },
                {
                    "authorId": "2535084",
                    "name": "C. Largou\u00ebt"
                },
                {
                    "authorId": "145482257",
                    "name": "L. Roz\u00e9"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "15625ba091bdef1a617d542fbe170aef186f34fd",
            "title": "Diversity and Inclusion Activities in EGC - A 2022 Report",
            "abstract": "EGC (\"Extraction et Gestion des Connaissances\"1 in French) started in 2001 and is the reference conference for the french community in Knowledge Extraction and Management (equivalent to the French KDD).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1699192",
                    "name": "A. Bonifati"
                },
                {
                    "authorId": "2065394520",
                    "name": "C\u00e9cile Favre"
                },
                {
                    "authorId": "2262635584",
                    "name": "\u00c9. Fromont"
                },
                {
                    "authorId": "1714938",
                    "name": "Nicolas Labroche"
                },
                {
                    "authorId": "2482136",
                    "name": "G. Melan\u00e7on"
                },
                {
                    "authorId": "1804438",
                    "name": "F. S\u00e8des"
                },
                {
                    "authorId": "2955369",
                    "name": "Arnaud Soulet"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "9d284ee6ae05064ecbb5cb0fa5950d5546dc82d2",
            "title": "TAG: Learning Timed Automata from Logs",
            "abstract": "Event logs are often one of the main sources of information to understand the behavior of a system. While numerous approaches have extracted partial information from event logs, in this work, we aim at inferring a global model of a system from its event logs.\n We consider real-time systems, which can be modeled with Timed Automata: our approach is thus a Timed Automata learner. There is a handful of related work, however, they might require a lot of parameters or produce Timed Automata that either are undeterministic or lack precision. In contrast, our proposed approach, called TAG, requires only one parameter and learns a deterministic Timed Automaton having a good tradeoff between accuracy and complexity of the automata. This allows getting an interpretable and accurate global model of the real-time system considered. Our experiments compare our approach to the related work and demonstrate its merits.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107085268",
                    "name": "L\u00e9na\u00efg Cornanguer"
                },
                {
                    "authorId": "2535084",
                    "name": "C. Largou\u00ebt"
                },
                {
                    "authorId": "145482257",
                    "name": "L. Roz\u00e9"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "bf488564c901957f9cec1b34838af52e2c7ff711",
            "title": "VCNet: A self-explaining model for realistic counterfactual generation",
            "abstract": "Counterfactual explanation is a common class of methods to make local explanations of machine learning decisions. For a given instance, these methods aim to find the smallest modification of feature values that changes the predicted decision made by a machine learning model. One of the challenges of counterfactual explanation is the efficient generation of realistic counterfactuals. To address this challenge, we propose VCNet-Variational Counter Net-a model architecture that combines a predictor and a counterfactual generator that are jointly trained, for regression or classification tasks. VCNet is able to both generate predictions, and to generate counterfactual explanations without having to solve another minimisation problem. Our contribution is the generation of counterfactuals that are close to the distribution of the predicted class. This is done by learning a variational autoencoder conditionally to the output of the predictor in a join-training fashion. We present an empirical evaluation on tabular datasets and across several interpretability metrics. The results are competitive with the state-of-the-art method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155042352",
                    "name": "Victor Guyomard"
                },
                {
                    "authorId": "2197518922",
                    "name": "Franccoise Fessant"
                },
                {
                    "authorId": "1678805",
                    "name": "Thomas Guyet"
                },
                {
                    "authorId": "1817990",
                    "name": "Tassadit Bouadi"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "625059c90155468719578bcddb65a57a7bd2406b",
            "title": "Discovering Useful Compact Sets of Sequential Rules in a Long Sequence",
            "abstract": "We are interested in understanding the underlying generation process for long sequences of symbolic events. To do so, we propose COSSU, an algorithm to mine small and meaningful sets of sequential rules. The rules are selected using an MDL-inspired criterion that favors compactness and relies on a novel rule-based encoding scheme for sequences. Our evaluation shows that COSSU can successfully retrieve relevant sets of closed sequential rules from a long sequence. Such rules constitute an interpretable model that exhibits competitive accuracy for the tasks of next-element prediction and classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2127068779",
                    "name": "Erwan Bourrand"
                },
                {
                    "authorId": "2000363870",
                    "name": "Luis Gal'arraga"
                },
                {
                    "authorId": "1997577",
                    "name": "E. Galbrun"
                },
                {
                    "authorId": "1687778",
                    "name": "\u00c9lisa Fromont"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "d560ba593ea6542b756e42ab929c1bfe33d43ff9",
            "title": "Prediction-Based Fleet Relocation for Free Floating Car Sharing Services",
            "abstract": "The success of a free-floating car-sharing service depends on a good allocation of the vehicles across the city, i.e. where and when they are needed by citizens. This requires predicting the demand across the geographical regions and across time, which is challenging due to the sparsity and variability of the data. Furthermore, the purpose of these predictions is to help computing the best possible car positions for the next day, hence the need to model both the prediction task and the optimisation task in a compatible way. As the allocation optimisation involves reasoning about the number of cars to assign to geographical regions, we propose to predict the expected utilisation of a car when added to a region. We discuss the challenges in modeling both the machine learning and the relocation problem, and we propose a integer linear programming method that solves the relocation problem while taking into account the model predictions and relocation distances. We experiment with the dataset from a citywide car sharing company and show how our method can increase the allocation strategies and hence profitability of the service.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2147191122",
                    "name": "Gregory Martin"
                },
                {
                    "authorId": "101007779",
                    "name": "Matthieu Donain"
                },
                {
                    "authorId": "1687778",
                    "name": "\u00c9lisa Fromont"
                },
                {
                    "authorId": "1834512",
                    "name": "Tias Guns"
                },
                {
                    "authorId": "145482257",
                    "name": "L. Roz\u00e9"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "0ee88a7199ce5b0efc8871fc0db167dac3f66822",
            "title": "Local Cascade Ensemble for Multivariate Data Classification",
            "abstract": "We present LCE, a Local Cascade Ensemble for traditional (tabular) multivariate data classification, and its extension LCEM for Multivariate Time Series (MTS) classification. LCE is a new hybrid ensemble method that combines an explicit boosting-bagging approach to handle the usual bias-variance tradeoff faced by machine learning models and an implicit divide-and-conquer approach to individualize classifier errors on different parts of the training data. Our evaluation firstly shows that the hybrid ensemble method LCE outperforms the state-of-the-art classifiers on the UCI datasets and that LCEM outperforms the state-of-the-art MTS classifiers on the UEA datasets. Furthermore, LCEM provides explainability by design and manifests robust performance when faced with challenges arising from continuous data collection (different MTS length, missing data and noise).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41035652",
                    "name": "Kevin Fauvel"
                },
                {
                    "authorId": "1687778",
                    "name": "\u00c9lisa Fromont"
                },
                {
                    "authorId": "2673540",
                    "name": "V\u00e9ronique Masson"
                },
                {
                    "authorId": "143806005",
                    "name": "P. Faverdin"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "1161391f77287dfc2b1eb6bef8d8247533682d04",
            "title": "XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification",
            "abstract": "Multivariate Time Series (MTS) classification has gained importance over the past decade with the increase in the number of temporal datasets in multiple domains. The current state-of-the-art MTS classifier is a heavyweight deep learning approach, which outperforms the second-best MTS classifier only on large datasets. Moreover, this deep learning approach cannot provide faithful explanations as it relies on post hoc model-agnostic explainability methods, which could prevent its use in numerous applications. In this paper, we present XCM, an eXplainable Convolutional neural network for MTS classification. XCM is a new compact convolutional neural network which extracts information relative to the observed variables and time directly from the input data. Thus, XCM architecture enables a good generalization ability on both large and small datasets, while allowing the full exploitation of a faithful post hoc model-specific explainability method (Gradient-weighted Class Activation Mapping) by precisely identifying the observed variables and timestamps of the input data that are important for predictions. We first show that XCM outperforms the state-of-the-art MTS classifiers on both the large and small public UEA datasets. Then, we illustrate how XCM reconciles performance and explainability on a synthetic dataset and show that XCM enables a more precise identification of the regions of the input data that are important for predictions compared to the current deep learning MTS classifier also providing faithful explainability. Finally, we present how XCM can outperform the current most accurate state-of-the-art algorithm on a real-world application while enhancing explainability by providing faithful and more informative explanations.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "41035652",
                    "name": "Kevin Fauvel"
                },
                {
                    "authorId": "2109640391",
                    "name": "Tao Lin"
                },
                {
                    "authorId": "2673540",
                    "name": "V\u00e9ronique Masson"
                },
                {
                    "authorId": "1934168963",
                    "name": "'Elisa Fromont"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "497aee0f4b9d764099e6cfdec14d5042c30c3c09",
            "title": "A Distributed Multi-Sensor Machine Learning Approach to Earthquake Early Warning",
            "abstract": "Our research aims to improve the accuracy of Earthquake Early Warning (EEW) systems by means of machine learning. EEW systems are designed to detect and characterize medium and large earthquakes before their damaging effects reach a certain location. Traditional EEW methods based on seismometers fail to accurately identify large earthquakes due to their sensitivity to the ground motion velocity. The recently introduced high-precision GPS stations, on the other hand, are ineffective to identify medium earthquakes due to its propensity to produce noisy data. In addition, GPS stations and seismometers may be deployed in large numbers across different locations and may produce a significant volume of data consequently, affecting the response time and the robustness of EEW systems.In practice, EEW can be seen as a typical classification problem in the machine learning field: multi-sensor data are given in input, and earthquake severity is the classification result. In this paper, we introduce the Distributed Multi-Sensor Earthquake Early Warning (DMSEEW) system, a novel machine learning-based approach that combines data from both types of sensors (GPS stations and seismometers) to detect medium and large earthquakes. DMSEEW is based on a new stacking ensemble method which has been evaluated on a real-world dataset validated with geoscientists. The system builds on a geographically distributed infrastructure, ensuring an efficient computation in terms of response time and robustness to partial infrastructure failures. Our experiments show that DMSEEW is more accurate than the traditional seismometer-only approach and the combined-sensors (GPS and seismometers) approach that adopts the rule of relative strength.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41035652",
                    "name": "Kevin Fauvel"
                },
                {
                    "authorId": "1393670168",
                    "name": "Daniel Balouek-Thomert"
                },
                {
                    "authorId": "97318651",
                    "name": "D. Melgar"
                },
                {
                    "authorId": "2110591649",
                    "name": "Pedro Silva"
                },
                {
                    "authorId": "20477658",
                    "name": "Anthony Simonet"
                },
                {
                    "authorId": "1779139",
                    "name": "Gabriel Antoniu"
                },
                {
                    "authorId": "1726606",
                    "name": "Alexandru Costan"
                },
                {
                    "authorId": "2673540",
                    "name": "V\u00e9ronique Masson"
                },
                {
                    "authorId": "145203664",
                    "name": "M. Parashar"
                },
                {
                    "authorId": "1709070",
                    "name": "I. Rodero"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "96bd743e624b53f7b85f84616e913231b91d6a9b",
            "title": "Netspot: a simple Intrusion Detection System with statistical learning",
            "abstract": "Machine learning is nowadays increasingly used in cyber-security. While intrusion detection was mainly based on human expertise in the 1990s, learning models to predict attacks are now built from data. However, a large part of the developed learning algorithms hitherto has missed real-world issues, making them unpractical. Indeed, many supervised algorithms described in the literature have been trained and tuned only on the KDD99 dataset. Besides, these algorithms are often static and are unable to automatically adapt for detecting attacks depending on the network traffic. Consequently, we are far from detecting zero-day or more general Advanced Persistent Threats (APT) since only pre-registered and well-characterized attacks can be catched. Some recent systems use unsupervised ML algorithms, but the resulting tools are overly complex: many ML components are stacked with various tuning parameters, usually making the results hard to interpret. And finally, a strong ML/DM expertise is required to set up these systems on real networks. We present netspot, a very simple network intrusion detection system (NIDS) powered by SPOT, a recent streaming statistical anomaly detector. This statistical test uses Extreme Value Theory, which is a powerful method for detecting anomalies. Unlike all the previous works, it is not an end-to-end solution aimed to detect all cyber-attacks with packet resolution. It is rather a module providing a behavioral information which can be integrated in a more general monitoring system. netspot is simple: it has few (simple) parameters, it adapts along time to the monitored network and it is as fast as current rule-based methods. But most importantly, it is able to detect realworld cyber-attacks, making it a credible practical anomaly-based NIDS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "22657977",
                    "name": "Alban Siffer"
                },
                {
                    "authorId": "1740168",
                    "name": "Pierre-Alain Fouque"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "2535084",
                    "name": "C. Largou\u00ebt"
                }
            ]
        },
        {
            "paperId": "e01155989d3652bf11e87caedd144f6ee2ace1d4",
            "title": "XEM: An Explainable Ensemble Method for Multivariate Time Series Classification.",
            "abstract": "We present XEM, an eXplainable Ensemble method for Multivariate time series classification. XEM relies on a new hybrid ensemble method that combines an explicit boosting-bagging approach to handle the bias-variance trade-off faced by machine learning models and an implicit divide-and-conquer approach to individualize classifier errors on different parts of the training data. Our evaluation shows that XEM outperforms the state-of-the-art MTS classifiers on the UEA datasets. Furthermore, XEM provides faithful explainability by design and manifests robust performance when faced with challenges arising from continuous data collection (different MTS length, missing data and noise).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2154411319",
                    "name": "Kevin Fauvel"
                },
                {
                    "authorId": "1934168963",
                    "name": "'Elisa Fromont"
                },
                {
                    "authorId": "2673540",
                    "name": "V\u00e9ronique Masson"
                },
                {
                    "authorId": "143806005",
                    "name": "P. Faverdin"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "91362bec57d2b303e210471b020153dc154cec5b",
            "title": "Agnostic Local Explanation for Time Series Classification",
            "abstract": "Recent advances in Machine Learning (such as Deep Learning) have brought tremendous gains in classification accuracy. However, these approaches build complex non-linear models, making the resulting predictions difficult to interpret for humans. The field of model interpretability has therefore recently emerged, aiming to address this issue by designing methods to explain a posteriori the predictions of complex learners. Interpretability frameworks such as LIME and SHAP have been proposed for tabular, image and text data. Nowadays, with the advent of the Internet of Things and of pervasive monitoring, time-series have become ubiquitous and their classification is a crucial task in many application domains. Like in other data domains, state-of-the-art time-series classifiers rely on complex models and typically do not provide intuitive and easily interpretable outputs, yet no interpretability framework had so far been proposed for this type of data. In this paper, we propose the first agnostic Local Explainer For TIme Series classificaTion (LEFTIST). LEFTIST provides explanations for predictions made by any time series classifier. Our thorough experiments on synthetic and real-world datasets show that the explanations provided by LEFTIST are at once faithful to the classification model and understandable by human users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "29789713",
                    "name": "Ma\u00ebl Guillem\u00e9"
                },
                {
                    "authorId": "2673540",
                    "name": "V\u00e9ronique Masson"
                },
                {
                    "authorId": "145482257",
                    "name": "L. Roz\u00e9"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "cc3e7d451006cc7c774d686718779bca2ebf01f4",
            "title": "Accelerating Itemset Sampling using Satisfiability Constraints on FPGA",
            "abstract": "Finding recurrent patterns within a data stream is important for fields as diverse as cybersecurity or e-commerce. This requires to use pattern mining techniques. However, pattern mining suffers from two issues. The first one, known as \"pattern explosion\", comes from the large combinatorial space explored and is the result of too many patterns outputed to be analyzed. Recent techniques called output space sampling solve this problem by outputing only a sampled set of all the results, with a target size provided by the user. The second issue is that most algorithms are designed to operate on static datasets or low throughput streams. In this paper, we propose a contribution to tackle both issues, by designing an FPGA accelerator for pattern mining with output space sampling. We show that our accelerator can outperform a state-of-the-art implementation on a server class CPU using a modest FPGA product.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "89545047",
                    "name": "M. Gueguen"
                },
                {
                    "authorId": "1697143",
                    "name": "O. Sentieys"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "ef943ff114ebabdec3eea6426f7dcae1bd4731b6",
            "title": "Towards Sustainable Dairy Management - A Machine Learning Enhanced Method for Estrus Detection",
            "abstract": "Our research tackles the challenge of milk production resource use efficiency in dairy farms with machine learning methods. Reproduction is a key factor for dairy farm performance since cows milk production begin with the birth of a calf. Therefore, detecting estrus, the only period when the cow is susceptible to pregnancy, is crucial for farm efficiency. Our goal is to enhance estrus detection (performance, interpretability), especially on the currently undetected silent estrus (35% of total estrus), and allow farmers to rely on automatic estrus detection solutions based on affordable data (activity, temperature). In this paper, we first propose a novel approach with real-world data analysis to address both behavioral and silent estrus detection through machine learning methods. Second, we present LCE, a local cascade based algorithm that significantly outperforms a typical commercial solution for estrus detection, driven by its ability to detect silent estrus. Then, our study reveals the pivotal role of activity sensors deployment in estrus detection. Finally, we propose an approach relying on global and local (behavioral versus silent) algorithm interpretability (SHAP) to reduce the mistrust in estrus detection solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41035652",
                    "name": "Kevin Fauvel"
                },
                {
                    "authorId": "2673540",
                    "name": "V\u00e9ronique Masson"
                },
                {
                    "authorId": "1687778",
                    "name": "\u00c9lisa Fromont"
                },
                {
                    "authorId": "143806005",
                    "name": "P. Faverdin"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "a6bab22ccd7908bdaf25ae4168100d957fcb22b0",
            "title": "Are your data gathered?",
            "abstract": "Understanding data distributions is one of the most fundamental research topic in data analysis. The literature provides a great deal of powerful statistical learning algorithms to gain knowledge on the underlying distribution given multivariate observations. We are likely to find out a dependence between features, the appearance of clusters or the presence of outliers. Before such deep investigations, we propose the folding test of unimodality. As a simple statistical description, it allows to detect whether data are gathered or not (unimodal or multimodal). To the best of our knowledge, this is the first multivariate and purely statistical unimodality test. It makes no distribution assumption and relies only on a straightforward p-value. Through real world data experiments, we show its relevance and how it could be useful for clustering.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "22657977",
                    "name": "Alban Siffer"
                },
                {
                    "authorId": "1740168",
                    "name": "Pierre-Alain Fouque"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "2535084",
                    "name": "C. Largou\u00ebt"
                }
            ]
        },
        {
            "paperId": "e664ea67a72ef18ab1429373a4e39d0689781fdf",
            "title": "Data Science Techniques for Sustainable Dairy Management",
            "abstract": "A multi-disciplinary team of experts from Inria and INRA are working towards improving farmers\u2019 income and working conditions in an environmentally friendly manner.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "41035652",
                    "name": "Kevin Fauvel"
                },
                {
                    "authorId": "2673540",
                    "name": "V\u00e9ronique Masson"
                },
                {
                    "authorId": "143806005",
                    "name": "P. Faverdin"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "0cfe631a0f99242a751a5a8bfd28dfb18f37f23e",
            "title": "Anomaly Detection in Streams with Extreme Value Theory",
            "abstract": "Anomaly detection in time series has attracted considerable attention due to its importance in many real-world applications including intrusion detection, energy management and finance. Most approaches for detecting outliers rely on either manually set thresholds or assumptions on the distribution of data according to Chandola, Banerjee and Kumar. Here, we propose a new approach to detect outliers in streaming univariate time series based on Extreme Value Theory that does not require to hand-set thresholds and makes no assumption on the distribution: the main parameter is only the risk, controlling the number of false positives. Our approach can be used for outlier detection, but more generally for automatically setting thresholds, making it useful in wide number of situations. We also experiment our algorithms on various real-world datasets which confirm its soundness and efficiency.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "22657977",
                    "name": "Alban Siffer"
                },
                {
                    "authorId": "1740168",
                    "name": "Pierre-Alain Fouque"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "2535084",
                    "name": "C. Largou\u00ebt"
                }
            ]
        },
        {
            "paperId": "e300e7f345361d1dd40c7c6aca4fec3499588320",
            "title": "Topic Signatures in Political Campaign Speeches",
            "abstract": "Highlighting the recurrence of topics usage in candidates speeches is a key feature to identify the main ideas of each candidate during a political campaign. In this paper, we present a method combining standard topic modeling with signature mining for analyzing topic recurrence in speeches of Clinton and Trump during the 2016 American presidential campaign. The results show that the method extracts automatically the main ideas of each candidate and, in addition, provides information about the evolution of these topics during the campaign.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3383576",
                    "name": "Cl\u00e9ment Gautrais"
                },
                {
                    "authorId": "24194345",
                    "name": "Peggy Cellier"
                },
                {
                    "authorId": "1749221",
                    "name": "R. Quiniou"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "0d9489083167a3976bf938592d4f8287ceb452fa",
            "title": "Identifying Genetic Variant Combinations Using Skypatterns",
            "abstract": "Identifying variant combination association with disease is a bioinformatics challenge. This problem can be solved by discriminative pattern mining that use statistical function to evaluate the significance of individual biological patterns. There is a wide range of such measures. However, selecting an appropriate measure as well as a suitable threshold in some specific practical situations is a difficult task. In this article, we propose to use the skypattern technique which allows combinations of measures to be used to evaluate the importance of variant combinations without having to select a given measure and a fixed threshold. Experiments on several real variant datasets demonstrate that the skypattern method effectively identifies the risk variant combinations related to diseases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34982796",
                    "name": "Hoang-Son Pham"
                },
                {
                    "authorId": "1752007",
                    "name": "D. Lavenier"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "58a770548c945fb1461464800838c5601d13028e",
            "title": "TraceViz: a visualization framework for interactive analysis of execution traces",
            "abstract": "Hardware platforms of embedded systems are more powerful at each new generation thank to the integration of System-on-Chip (SoC). Developing streaming multimedia applications on embedded systems becomes an increasingly complex process. Modern applications are highly multi-threaded and have to decode the multimedia stream in real time to prevent the apparition of audio and video artifacts. Debugging this kind of issue cannot be done with traditional debuggers that interrupt the decoding and perturb the synchronization of the different threads. The solution is to record all the events that occurred during the decoding in a trace and perform the analysis post-mortem. There exists many visualization tools to analyze execution traces but they have reached their limits with the amount of data generated by modern applications. They either provide a too generalized representation to be useful, or they show too much details leading to a fastidious data exploration. We propose a novel interaction visualization framework to address these problems. In particular, our contribution is in two parts: (a) we present a new fast backend suitable for the interactive browsing of huge traces and (b) a new visualization tool to explore the trace at different level of details.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35427940",
                    "name": "R\u00e9my Dautriche"
                },
                {
                    "authorId": "2864180",
                    "name": "R. Blanch"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "145264159",
                    "name": "M. Santana"
                }
            ]
        },
        {
            "paperId": "71192e6f1335cbd7865b53ac08edefc7570f624b",
            "title": "Understanding Customer Attrition at an Individual Level: a New Model in Grocery Retail Context",
            "abstract": "This paper presents a new model to detect and explain customer defection in a grocery retail context. This new model analyzes the evolution of each customer basket content. It therefore provides actionable knowledge for the retailer at an individual scale. In addition, this model is able to identify customers that are likely to defect in the future months.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3383576",
                    "name": "Cl\u00e9ment Gautrais"
                },
                {
                    "authorId": "24194345",
                    "name": "Peggy Cellier"
                },
                {
                    "authorId": "1678805",
                    "name": "Thomas Guyet"
                },
                {
                    "authorId": "1749221",
                    "name": "R. Quiniou"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "8e4c0579a401201116c74acd766c86258ae0f9c9",
            "title": "Efficient local search for L1 and L2 binary matrix factorization",
            "abstract": "Rank K Binary Matrix Factorization (BMF) approximates a binary matrix by the product of two binary matrices of lower rank, K. Several researchers have addressed this problem, focusing on either approximations of rank 1 or higher, using either the L 1 or L 2-norms for measuring the quality of the approximation. The rank 1 problem (for which the L 1 and L 2-norms are equivalent) has been shown to be related to the Integer Linear Programming (ILP) problem. We first show here that the alternating strategy with the L 2-norm, at the core of several methods used to solve BMF, can be reformulated as an Unconstrained Binary Quadratic Programming (UBQP) problem. This reformulation allows us to use local search procedures designed for UBQP in order to improve the solutions of BMF. We then introduce a new local search dedicated to the BMF problem. We show in particular that this solution is in average faster than the previously proposed ones. We then assess its behavior on several collections and methods and show that it significantly improves methods targeting the L 2-norms on all the datasets considered; for the L 1-norm, the improvement is also significant for real, structured datasets and for the BMF problem without the binary reconstruction constraint.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2701917",
                    "name": "Seyed Hamid Mirisaee"
                },
                {
                    "authorId": "1732180",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "c0d321e5a49a2c0fc44dd6a4edf176de396fee5e",
            "title": "Towards Visualizing Hidden Structures",
            "abstract": "There is an increasing need to quickly understand the contents log data. A wide range of patterns can be computed and provide valuable information: for example existence of repeated sequences of events or periodic behaviors. However patternminingtechniquesoftenproducemanypatternsthathave to be examined one by one, which is time consuming for experts. On the other hand, visualization techniques are easier to understand, but cannot provide the in-depth understanding provided by pattern mining approaches. Our contribution is to propose a novel visualanalytics methodthat allows toimmediately visualize hidden structures such as repeated sets/sequences and periodicity, allowing to quickly gain a deep understanding of the log.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35427940",
                    "name": "R\u00e9my Dautriche"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "2864180",
                    "name": "R. Blanch"
                },
                {
                    "authorId": "145264159",
                    "name": "M. Santana"
                }
            ]
        },
        {
            "paperId": "f2e32b483719e3d51794e520dc3ea91ee4afb7e1",
            "title": "Steady Patterns",
            "abstract": "Skypatterns are an elegant answer to the pattern explosion issue, when a set of measures can be provided. Skypatterns for all possible measure combinations can be explored thanks to recent work on the skypattern cube. However, this leads to too many skypatterns, where it is difficult to quickly identify which ones are more important. First, we introduce a new notion of pattern steadiness which measures the conservation of the skypattern property across the skypattern cube, allowing to see which are the \"most universal\" skypatterns. Then, we extended this notion to partitions of the dataset, and show in our experiments that this both allows to discover especially stable skypatterns, and identify interesting differences between the partitions.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1962063",
                    "name": "W. Ugarte"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "145264159",
                    "name": "M. Santana"
                }
            ]
        },
        {
            "paperId": "124c6fa4e54514b3930f2c1bc79e4abd39d04d18",
            "title": "Improved Local Search for Binary Matrix Factorization",
            "abstract": "\n \n Rank K Binary Matrix Factorization (BMF) approximates a binary matrix by the product of two binary matrices of lower rank, K, using either L1 or L2 norm. In this paper, we first show that the BMF with L2 norm can be reformulated as an Unconstrained Binary Quadratic Programming (UBQP) problem. We then review several local search strategies that can be used to improve the BMF solutions obtained by previously proposed methods, before introducing a new local search dedicated to the BMF problem. We show in particular that the proposed solution is in general faster than the previously proposed ones. We then assess its behavior on several collections and methods and show that it significantly improves methods targeting the L2 norms on all the datasets considered; for the L1 norm, the improvement is also significant for real, structured datasets and for the BMF problem without the binary reconstruction constraint.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2701917",
                    "name": "Seyed Hamid Mirisaee"
                },
                {
                    "authorId": "1732180",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "2cd2654fbe6dc2aba9f6ade5796bde54d7350a57",
            "title": "Interactive User Group Analysis",
            "abstract": "User data is becoming increasingly available in multiple domains ranging from phone usage traces to data on the social Web. The analysis of user data is appealing to scientists who work on population studies, recommendations, and large-scale data analytics. We argue for the need for an interactive analysis to understand the multiple facets of user data and address different analytics scenarios. Since user data is often sparse and noisy, we propose to produce labeled groups that describe users with common properties and develop IUGA, an interactive framework based on group discovery primitives to explore the user space. At each step of IUGA, an analyst visualizes group members and may take an action on the group (add/remove members) and choose an operation (exploit/explore) to discover more groups and hence more users. Each discovery operation results in k most relevant and diverse groups. We formulate group exploitation and exploration as optimization problems and devise greedy algorithms to enable efficient group discovery. Finally, we design a principled validation methodology and run extensive experiments that validate the effectiveness of IUGA on large datasets for different user space analysis scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403851743",
                    "name": "Behrooz Omidvar-Tehrani"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "33586add5d3a923ce834a588d329cb66ca74f995",
            "title": "Interactive Data-Driven Research: the place where databases and data mining research meet",
            "abstract": "Data-driven research, or the science of letting data tell us what we are looking for, is in many areas, the only viable approach to research. In some domains like adaptive clinical trials and emerging research areas such as social computing, useful results are highly dependent on the ability to observe and interactively explore large volumes of real datasets. Database management is the science of efficiently storing and retrieving data. Data mining is the science of discovering hidden correlations in data. Interactive data-driven research is a natural meeting point that presents a new research opportunity. The ability to conduct effective data-driven research requires to combine efficient indexing and querying from databases and pattern mining and classification from data mining to help analysts understand what lies behind large data volumes. In this paper, we explore key challenges and new opportunities in building robust systems for interactive data-driven research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "3359814",
                    "name": "M. Kirchgessner"
                },
                {
                    "authorId": "1403851743",
                    "name": "Behrooz Omidvar-Tehrani"
                }
            ]
        },
        {
            "paperId": "9acb32ee248f7b61cce2f84acfe59c3dfaaa3b08",
            "title": "Reducing trace size in multimedia applications endurance tests",
            "abstract": "Proper testing of applications over embedded systems such as set-top boxes requires endurance tests, i.e. running applications for extended periods of times, typically several days. In order to understand bugs or poor performances, execution traces have to be analyzed, however current trace analysis methods are not designed to handle several days of execution traces due to the huge quantity of data generated. Our proposal, designed for regular applications such as multimedia decoding/encoding, is to monitor execution by analyzing trace on the fly in order to record trace only in time periods where a suspicious activity is detected. Our experiments show a significant reduction in the trace size compared to recording the whole trace.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3068361",
                    "name": "Serge Vladimir Emteu Tchagou"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "73210645",
                    "name": "Jean-Fran\u00e7ois M\u00e9haut"
                },
                {
                    "authorId": "2765359",
                    "name": "B. Videau"
                },
                {
                    "authorId": "145264159",
                    "name": "M. Santana"
                },
                {
                    "authorId": "1749221",
                    "name": "R. Quiniou"
                }
            ]
        },
        {
            "paperId": "f1c0a2f24eaf5b324ac68e104659133143bd5641",
            "title": "Selecting representative instances from datasets",
            "abstract": "We propose in this paper a new, alternative approach for the problem of finding a set of representative objects in large datasets. To do so, we first formulate the general Instance Selection Problem (ISP) and then study three variants of that in order to select instances from different regions of the data. These variants aim at finding the objects located in three very different locations of the data: the inner frontier, the central area and the outer frontier. Solutions to these problems have been discussed and their complexities have been studied. To illustrate the effectiveness of the proposed techniques, we first use a small, synthetic dataset for visualization purpose. We then study them on the Reuters dataset and show that the integration of instances selected by the ISP techniques is able to provide a good representation of the data and can be considered as a complementary approach for the state-of-the-art methods. Finally, we examine the quality of the selected objects by applying a topic-based analysis in order to show how well the selected documents cover the topics in the Reuters dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2701917",
                    "name": "Seyed Hamid Mirisaee"
                },
                {
                    "authorId": "1765141",
                    "name": "A. Chouakria"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "f215285a89fc3facbc5d02c574225a62b05c741c",
            "title": "Data mining approach to temporal debugging of embedded streaming applications",
            "abstract": "One of the greatest challenges in the embedded systems area is to empower software developers with tools that speed up the debugging of QoS properties in applications. Typical streaming applications, such as multimedia (audio/video) decoding, fulfill the QoS properties by respecting the real-time deadlines. A perfectly functional application, when missing these deadlines, may lead to cracks in the sound or perceptible artifacts in the image. We start from the premise that most of the streaming applications that run on embedded systems can be expressed under a data ow model of computation, where the application is represented as a directed graph of the data flowing through computational units called actors. It has been shown that in order to meet real-time constraints the actors should be scheduled in a periodic manner. We exploit this property to propose SATM - a novel approach based on data mining techniques that automatically analyzes execution traces of streaming applications, and discovers significant breaks in the periodicity of actors, as well as potential causes of these breaks. We show on a real use case that our debugging approach can uncover important defects and pinpoint their location to the application developer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2309544255",
                    "name": "Oleg Iegorov"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "73210645",
                    "name": "Jean-Fran\u00e7ois M\u00e9haut"
                },
                {
                    "authorId": "145264159",
                    "name": "M. Santana"
                }
            ]
        },
        {
            "paperId": "07d998273ec74a0774b66620c27129a52a5c6e15",
            "title": "Itemset approximation using Constrained Binary Matrix Factorization",
            "abstract": "We address in this paper the problem of efficiently finding a few number of representative frequent itemsets in transaction matrices. To do so, we propose to rely on matrix decomposition techniques, and more precisely on Constrained Binary Matrix Factorization (CBMF) which decomposes a given binary matrix into the product of two lower dimensional binary matrices, called factors. We first show, under binary constraints, that one can interpret the first factor as a transaction matrix operating on packets of items, whereas the second factor indicates which item belongs to which packet. We then formally prove that one can directly mine the CBMF factors in order to find (approximate) itemsets of a given size and support in the original transaction matrix. Then through a detailed experimental study, we show that the frequent itemsets produced by our method represent a significant portion of the set of all frequent itemsets according to existing metrics, while being up to several orders of magnitude less numerous.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2701917",
                    "name": "Seyed Hamid Mirisaee"
                },
                {
                    "authorId": "1732180",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "42b3fea5295b6f5179426d90a6b80ae339e3d4d6",
            "title": "Benchmarking of triple stores scalability for MPSoC trace analysis",
            "abstract": "A Multi Processor System-on-Chip (MPSoC) is a complex embedded system used in consumer electronic devices, such as smartphones, tablets and set-top boxes. In order to cope with the complexity of MPSoC architectures, software developers rely on post-mortem trace analysis for application de-bugging or optimization. The traces are explored to localize expected and unexpected programs behaviors. However, the low semantic value of low-level trace events make the trace exploration difficult. We propose to perform trace exploration through an ontology which adds semantics to events and provides a declarative language for querying data. Because traces can be huge, such an ontology contains a large number of instances stored as RDF triples. Because analysts need fast results on classical computer, an efficient system for query answering is preferred. Therefore, saturating , loading and querying those triples pose a scalabil-ity challenge to state-of-the-art knowledge base repositories (KBR). In this paper, we have conducted a benchmark of 7 KBRs: Jena, Sesame-native, Sesame-memory, tdb, sdb, rdf-3x and vertical-mdb, to test their scalability in a non-distributed environment close to analyst environment. We used these KBRs to analyze real traces through VIDECOM, an ontology we designed for trace analysis of applications on MPSoC. Results show that vertical-mdb has a loading rate 3 times faster than the others. It is the only KBR able to saturate the biggest trace of our dataset without exceeding system memory and to run complex queries on it in an acceptable time. Other approaches failed, due to memory limitation or inefficient join implementation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35438184",
                    "name": "L. Fopa"
                },
                {
                    "authorId": "2683468",
                    "name": "F. Jouanot"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "1779483",
                    "name": "M. Tchuent\u00e9"
                },
                {
                    "authorId": "2309544255",
                    "name": "Oleg Iegorov"
                }
            ]
        },
        {
            "paperId": "cc4baf33fe1b6775545cde46bb446327318b21aa",
            "title": "Scalability bottlenecks discovery in MPSoC platforms using data mining on simulation traces",
            "abstract": "Nowadays, a challenge faced by many developers is the profiling of parallel applications so that they can scale over more and more cores. This is especially critical for embedded systems powered by Multi-Processor System-on-Chip (MPSoC), where ever demanding applications have to run smoothly on numerous cores, each with modest power budget. The reasons for the lack of scalability of parallel applications are numerous, and it can be time consuming for a developer to pinpoint the correct one. In this paper, we propose a fully automatic method which detects the instructions of the code which lead to a lack of scalability. The method is based on data mining techniques exploiting low level execution traces produced by MPSoC simulators. Our experiments show the accuracy of the proposed technique on five different kinds of applications, and how the information reported can be exploited by application developers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2012545",
                    "name": "S. Lagraa"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "150103462",
                    "name": "F. P\u00e9trot"
                }
            ]
        },
        {
            "paperId": "8aee8b7a42469ab1d009d43c39a6a62578c92340",
            "title": "Pattern mining rock: more, faster, better",
            "abstract": "Pattern mining is the area of data mining concerned with finding regularities in data. This document presents my contributions to this domain along three axes: 1. The domain is young, and there are still some kinds of regularities that data analysts would like to discover in data but that are not handled. We contributed two new pattern definitions extending the reach of data analysis by pattern mining: gradual patterns and periodic patterns with unrestricted gaps. We also proposed ParaMiner, a pioneering algorithm for generic pattern mining, allowing practitioners to directly specify the patterns they are interested in. 2. Pattern mining is extremely demanding on computational resources. In order to reduce the mining time, we studied how to exploit the parallelism of multicore processors. Our results show that some well established techniques in pattern mining are ill-adapted for parallelism, and propose solutions. 3. Our ultimate goal is to make pattern mining easier to use by data analysts. There is a lot to do in this area, as currently they are presented with unusable lists of millions of patterns. We will present our first results in the context of mining execution traces of processors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "a5f36ee00dd518dac08aca7d79c9f84225450f53",
            "title": "Towards a Framework for Semantic Exploration of Frequent Patterns",
            "abstract": "Mining frequent patterns is an essential task in discovering hidden correlations in datasets. Although frequent patterns unveil valuable information, there are some challenges which limits their usability. First, the number of possible patterns is often very large which hinders their eff ective exploration. Second, patterns with many items are hard to read and the analyst may be unable to understand their meaning. In addition, the only available information about patterns is their support, a very coarse piece of information. In this paper, we are particularly interested in mining datasets that reflect usage patterns of users moving in space and time and for whom demographics attributes are available (age, occupation, etc). Such characteristics are typical of data collected from smart phones, whose analysis has critical business applications nowadays. We propose pattern exploration primitives, abstraction and refinement, that use hand-crafted taxonomies on time, space and user demographics. We show on two real datasets, Nokia and MovieLens, how the use of such taxonomies reduces the size of the pattern space and how demographics enable their semantic exploration. This work opens new perspectives in the semantic exploration of frequent patterns that reflect the behavior of di fferent user communities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403851743",
                    "name": "Behrooz Omidvar-Tehrani"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "3228199",
                    "name": "Aur\u00e9lie Bertaux"
                },
                {
                    "authorId": "1732180",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "b491bb7330c157eb3a7703a79969f8d0df95d763",
            "title": "Efficiently rewriting large multimedia application execution traces with few event sequences",
            "abstract": "The analysis of multimedia application traces can reveal important information to enhance program execution comprehension. However typical size of traces can be in gigabytes, which hinders their effective exploitation by application developers. In this paper, we study the problem of finding a set of sequences of events that allows a reduced-size rewriting of the original trace. These sequences of events, that we call blocks, can simplify the exploration of large execution traces by allowing application developers to see an abstraction instead of low-level events. The problem of computing such set of blocks is NP-hard and naive approaches lead to prohibitive running times that prevent analysing real world traces. We propose a novel algorithm that directly mines the set of blocks. Our experiments show that our algorithm can analyse real traces of up to two hours of video. We also show experimentally the quality of the set of blocks proposed, and the interest of the rewriting to understand actual trace data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2309554",
                    "name": "C. Kengne"
                },
                {
                    "authorId": "35438184",
                    "name": "L. Fopa"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "2052763654",
                    "name": "Noha Ibrahim"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1704749",
                    "name": "T. Washio"
                },
                {
                    "authorId": "145264159",
                    "name": "M. Santana"
                }
            ]
        },
        {
            "paperId": "e85b2b4f20b7a0aa62b13ed0c1d6b0f7973cd904",
            "title": "Data mining MPSoC simulation traces to identify concurrent memory access patterns",
            "abstract": "Due to a growing need for flexibility, massively parallel Multiprocessor SoC (MPSoC) architectures are currently being developed. This leads to the need for parallel software, but poses the problem of the efficient deployment of the software on these architectures. To address this problem, the execution of the parallel program with software traces enabled on the platform and the visualization of these traces to detect irregular timing behavior is the rule. This is error prone as it relies on software logs and human analysis, and requires an existing platform. To overcome these issues and automate the process, we propose the conjoint use of a virtual platform logging at hardware level the memory accesses and of a data-mining approach to automatically report unexpected instructions timings, and the context of occurrence of these instructions. We demonstrate the approach on a multiprocessor platform running a video decoding application.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2012545",
                    "name": "S. Lagraa"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "150103462",
                    "name": "F. P\u00e9trot"
                }
            ]
        },
        {
            "paperId": "6d5f635c388ef3b255d7a9c94bf89726f4eaf2bb",
            "title": "Debugging embedded multimedia application traces through periodic pattern mining",
            "abstract": "Increasing complexity in both the software and the underlying hardware, and ever tighter time-to-market pressures are some of the key challenges faced when designing multimedia embedded systems. Optimizing the debugging phase can help to reduce development time significantly. A powerful approach used extensively during this phase is the analysis of execution traces. However, huge trace volumes make manual trace analysis unmanageable. In such situations, Data Mining can help by automatically discovering interesting patterns in large amounts of data. In this paper, we are interested in discovering periodic behaviors in multimedia applications. Therefore, we propose a new pattern mining approach for automatically discovering all periodic patterns occurring in a multimedia application execution trace.\n Furthermore, gaps in the periodicity are of special interest since they can correspond to cracks or drop-outs in the stream. Existing periodic pattern definitions are too restrictive regarding the size of the gaps in the periodicity. So, in this paper, we specify a new definition of frequent periodic patterns that removes this limitation. Moreover, in order to simplify the analysis of the set of frequent periodic patterns we propose two complementary approaches: (a) a lossless representation that reduces the size of the set and facilitates its analysis, and (b) a tool to identify pairs of \"competitors\" where a pattern breaks the periodicity of another pattern. Several experiments were carried out on embedded video and audio decoding application traces, demonstrating that using these new patterns it is possible to identify abnormal behaviors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2487405",
                    "name": "Patricia L\u00f3pez Cueva"
                },
                {
                    "authorId": "3228199",
                    "name": "Aur\u00e9lie Bertaux"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "73210645",
                    "name": "Jean-Fran\u00e7ois M\u00e9haut"
                },
                {
                    "authorId": "145264159",
                    "name": "M. Santana"
                }
            ]
        },
        {
            "paperId": "7a7f99422669cf6a3dd6b27a399f0786a897aa57",
            "title": "Automatic congestion detection in MPSoC programs using data mining on simulation traces",
            "abstract": "The efficient deployment of parallel software, specifically legacy one, on Multiprocessor systems on chip (MPSoC) is a challenging task. In this paper, we introduce the use of a data-mining approach on traces of a functionally correct program to automatically identify recurring congestion points and their sources. Each memory transaction, i.e. instruction fetch, data load and data store, occurring in the system is logged, thanks to the use of a virtual platform of the system. The resulting trace is analyzed to discover memory access patterns that are occurring frequently and that feature high latencies. These patterns are sorted by order of decreasing occurrence and estimated congestion level, allowing the easy identification of the sources of inefficiency. We have simulated a MPSoC with 16 processors running multiple applications, and have been able to automatically detect congestion on resources and their sources in the parallel program using this technique by analyzing gigabytes of traces.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2012545",
                    "name": "S. Lagraa"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "150103462",
                    "name": "F. P\u00e9trot"
                }
            ]
        },
        {
            "paperId": "e489509a7643d1cfc1c6de79dcaff8ee36bc47ca",
            "title": "Enhancing the Analysis of Large Multimedia Applications Execution Traces with FrameMiner",
            "abstract": "The analysis of multimedia application traces can reveal important information to enhance program comprehension. However traces can be very large, which hinders their effective exploitation. In this paper, we study the problem of finding a k-golden set of blocks that best characterize data. Sequential pattern mining can help to automatically discover the blocks, and we called k-golden set, a set of k blocks that maximally covers the trace. These kind of blocks can simplify the exploration of large traces by allowing programmers to see an abstraction instead of low-level events. We propose an approach for mining golden blocks and finding coverage of frames. The experiments carried out on video and audio application decoding show very promising results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2309554",
                    "name": "C. Kengne"
                },
                {
                    "authorId": "35438184",
                    "name": "L. Fopa"
                },
                {
                    "authorId": "2052763654",
                    "name": "Noha Ibrahim"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1704749",
                    "name": "T. Washio"
                }
            ]
        },
        {
            "paperId": "0a690e585684f71d0ed27aa78efebafa5f48b332",
            "title": "PGP-mc : extraction parall\u00e8le efficace de motifs graduels",
            "abstract": "Initialement utilises pour les systemes de commande, les regles et motifs graduels (de la forme \"plus une personne est \u00e2gee, plus son salaire est ele-ve\") trouvent de tres nombreuses applications, par exemple dans les domaines de la biologie, des donnees en flots (e.g. issues de reseaux de capteurs), etc. Tres recemment, des algorithmes ont ete proposes pour extraire automatiquement de tels motifs. Cependant, meme si certains d'entre eux ont permis des gains de performance importants, les algorithmes restent couteux et ne permettent pas de traiter efficacement les bases de donnees reelles souvent tres volumi-neuses (en nombre de lignes et/ou nombre d'attributs). Nous proposons donc dans cet article une methode originale de recherche de ces motifs utilisant le multi-threading pour exploiter au mieux les multiples coeurs presents dans la plupart des ordinateurs et serveurs actuels. L'efficacite de cette approche est va-lidee par une etude experimentale.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145472032",
                    "name": "Anne Laurent"
                },
                {
                    "authorId": "2511985",
                    "name": "Benjamin N\u00e9grevergne"
                },
                {
                    "authorId": "32405274",
                    "name": "Nicolas Sicard"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "8f3d9583427f192f7e9a899f2fc3ebc3367008ac",
            "title": "Discovering closed frequent itemsets on multicore: Parallelizing computations and optimizing memory accesses",
            "abstract": "The problem of closed frequent itemset discovery is a fundamental problem of data mining, having applications in numerous domains. It is thus very important to have efficient parallel algorithms to solve this problem, capable of efficiently harnessing the power of multicore processors that exists in our computers (notebooks as well as desktops). In this paper we present PLCMQS, a parallel algorithm based on the LCM algorithm, recognized as the most efficient algorithm for sequential discovery of closed frequent itemsets. We also present a simple yet powerfull parallelism interface based on the concept of Tuple Space, which allows an efficient dynamic sharing of the work. Thanks to a detailed experimental study, we show that PLCMQS is efficient on both on sparse and dense databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2511985",
                    "name": "Benjamin N\u00e9grevergne"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "73210645",
                    "name": "Jean-Fran\u00e7ois M\u00e9haut"
                },
                {
                    "authorId": "1713302",
                    "name": "T. Uno"
                }
            ]
        },
        {
            "paperId": "9033d88f4694281ac4776a296d8f41d8f2a8eeec",
            "title": "HLCM: a first experiment on parallel data mining with Haskell",
            "abstract": "We present a parallel implementation in Haskell of the most efficient closed frequent itemset mining algorithm called LCM. We show that Haskell allows us to conveniently express the complex code of the LCM algorithm. We also present a thorough experimental study about the influence of run time system parameters on the parallel performance of our implementation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "2511985",
                    "name": "Benjamin N\u00e9grevergne"
                },
                {
                    "authorId": "1678221",
                    "name": "S. Marlow"
                },
                {
                    "authorId": "2108384134",
                    "name": "Satnam Singh"
                }
            ]
        },
        {
            "paperId": "b2f916d8231602fcfa123bb9b5720fa247c98583",
            "title": "ProbaMap: a scalable tool for discovering probabilistic mappings between taxonomies",
            "abstract": "In this paper, we investigate a principled approach for defining and discovering probabilistic mappings between two taxonomies. First, we compare two ways of modeling probabilistic mappings which are compatible with the logical constraints declared in each taxonomy. Then we describe a generate and test algorithm (called ProbaMap) which minimizes the number of calls to the probability estimator for determining those mappings whose probability exceeds a certain threshold. Finally, we provide an experimental analysis of this approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2129862",
                    "name": "R\u00e9mi Tournaire"
                },
                {
                    "authorId": "1852297",
                    "name": "Jean-Marc Petit"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "ae71aec87f39826a7a03118b7caf98228eb39acf",
            "title": "DryadeParent, An Efficient and Robust Closed Attribute Tree Mining Algorithm",
            "abstract": "In this paper, we present a new tree mining algorithm, DryadeParent, based on the hooking principle first introduced in DRYADE. In the experiments, we demonstrate that the branching factor and depth of the frequent patterns to find are key factors of complexity for tree mining algorithms, even if often overlooked in previous work. We show that DryadeParent outperforms the current fastest algorithm, CMTreeMiner, by orders of magnitude on data sets where the frequent tree patterns have a high branching factor.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "69343681",
                    "name": "M. Sebag"
                },
                {
                    "authorId": "1736451",
                    "name": "K. Ohara"
                },
                {
                    "authorId": "1704749",
                    "name": "T. Washio"
                },
                {
                    "authorId": "1748072",
                    "name": "H. Motoda"
                }
            ]
        },
        {
            "paperId": "22e9d4b67459ae360099c8d56e89fed5ae603851",
            "title": "Mining XML Documents",
            "abstract": "XML documents are becoming ubiquitous because of their rich and flexible format that can be used for a variety of applications. Giving the increasing size of XML collections as information sources, mining techniques that traditionally exist for text collections or databases need to be adapted and new methods to be invented to exploit the particular structure of XML documents. Basically XML documents can be seen as trees, which are well known to be complex structures. This chapter describes various ways of using and simplifying this tree structure to model documents and support efficient mining algorithms. We focus on three mining tasks: classification and clustering which are standard for text collections; discovering of frequent tree structure which is especially important for heterogeneous collection. This chapter presents some recent approaches and algorithms to support these tasks together with experimental evaluation on a variety of large XML collections.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1727520",
                    "name": "Laurent Candillier"
                },
                {
                    "authorId": "8905591",
                    "name": "Ludovic Denoyer"
                },
                {
                    "authorId": "1741426",
                    "name": "P. Gallinari"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "1685821",
                    "name": "A. Vercoustre"
                }
            ]
        },
        {
            "paperId": "7a4f8135f1076670157def2cea0f323b6cf7aa22",
            "title": "DIGDAG, a First Algorithm to Mine Closed Frequent Embedded Sub-DAGs",
            "abstract": "Although tree and graph mining have attracted a lot of attention, there are nearly no algorithms devoted to DAGmining, whereas many applications are in dire need of such algorithms. We present in this paper DIGDAG, the first algorithm capable of mining closed frequent embedded subDAGs. This algorithm combines efficient closed frequent itemset algorithms with novel techniques in order to scale up to complex input data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "3127675",
                    "name": "Y. Tamada"
                },
                {
                    "authorId": "1767657",
                    "name": "Kazuyuki Numata"
                },
                {
                    "authorId": "1794239",
                    "name": "S. Imoto"
                },
                {
                    "authorId": "1704749",
                    "name": "T. Washio"
                },
                {
                    "authorId": "92447805",
                    "name": "T. Higuchi"
                }
            ]
        },
        {
            "paperId": "4b2ad4bb735c1312b56213216478cc32cf4466b9",
            "title": "Efficient mining of high branching factor attribute trees",
            "abstract": "In this paper, we present a new tree mining algorithm, DryadeParent, based on the hooking principle first introduced in Dryade (Termier et al, 2004). In the experiments, we demonstrate that the branching factor and depth of the frequent patterns to find are key factor of complexity for tree mining algorithms. We show that DryadeParent outperforms the current fastest algorithm, CMTreeMiner, by orders of magnitude on datasets where the frequent patterns have a high branching factor.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "69343681",
                    "name": "M. Sebag"
                },
                {
                    "authorId": "1736451",
                    "name": "K. Ohara"
                },
                {
                    "authorId": "1704749",
                    "name": "T. Washio"
                },
                {
                    "authorId": "1748072",
                    "name": "H. Motoda"
                }
            ]
        },
        {
            "paperId": "8a3fb668771742c8d767298413c7d0626335083b",
            "title": "Dryade: a new approach for discovering closed frequent trees in heterogeneous tree databases",
            "abstract": "In this paper we present a novel algorithm for discovering tree patterns in a tree database. This algorithm uses a relaxed tree inclusion definition, making the problem more complex (checking tree inclusion is NP-complete), but allowing to mine highly heterogeneous databases. To obtain good performances, our DRYADE algorithm, discovers only closed frequent tree patterns.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "69343681",
                    "name": "M. Sebag"
                }
            ]
        },
        {
            "paperId": "c34348b22821fd20dd8260e7069e0de97ca166a6",
            "title": "Highlighting Latent Structure in Documents",
            "abstract": "Extensible Markup Language (XML) is playing an increasingly important role in the exchange of a wide variety of data on the Web and elsewhere. It is a simple, very flexible text format, used to annotate data by means of markup. XML documents can be checked for syntactic well-formedness and semantic coherence through DTD and schema validation which makes their processing easier. In particular, data with nested structure can be easily represented with embedded tags. This structured representation should be used in information retrieval models which take structure into account. As such, it is meta-data and therefore a contribution to the Semantic Web. However, nowadays, there exists huge quantities of raw texts and the issue is how to find an easy way to provide these texts with sensible XML structure. Here we present an automatic method to extract tree structure from raw texts. This work has been supported by the Paris XI University (BQR2002 project, Paris-XI University).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2085023006",
                    "name": "Helka Folch"
                },
                {
                    "authorId": "36722443",
                    "name": "B. Habert"
                },
                {
                    "authorId": "2432809",
                    "name": "M. Jardino"
                },
                {
                    "authorId": "1718164",
                    "name": "Nathalie Pernelle"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "1dfe4d4809bff855141572178876310f1f565b0a",
            "title": "TreeFinder: a first step towards XML data mining",
            "abstract": "In this paper we consider the problem of searching frequent trees from a collection of tree-structured data modeling XML data. The TreeFinder algorithm aims at finding trees, such that their exact or perturbed copies are frequent in a collection of labelled trees. To cope with complexity issues, TreeFinder is correct but not complete: it finds a subset of actually frequent trees. The default of completeness is experimentally investigated on artificial medium size datasets; it is shown that TreeFinder reaches completeness or falls short for a range of experimental settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "69343681",
                    "name": "M. Sebag"
                }
            ]
        },
        {
            "paperId": "8cf37a07871bec0a5c70a8294ed852141727a9d8",
            "title": "Combining Statistics and Semantics for Word and Document Clustering",
            "abstract": "A new approach for constructing pseudo-keywords, referred to as Sense Units, is proposed. Sense Units are obtained by a word clustering process, where the underlying similarity reflects both statistical and semantic properties, respectively detected through Latent Semantic Analysis and WordNet. Sense Units are used to recode documents and are evaluated from the performance increase they permit in classification tasks. \n \nExperimental results show that accounting for semantic information in fact decreases the performances compared to LSI standalone. \n \nThe main weakenesses of the current hybrid scheme are discussed and several tracks for improvement are sketched.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "69343681",
                    "name": "M. Sebag"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        }
    ]
}