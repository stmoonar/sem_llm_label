{
    "authorId": "145546470",
    "papers": [
        {
            "paperId": "7bf4ce81f7fcc99c576c0b364a0f1195a8d9a651",
            "title": "FDup framework: A General-purpose solution for Efficient Entity Deduplication of Record Collections",
            "abstract": "Deduplication is a technique aimed at identifying and resolving duplicate metadata records in a collection with a special focus on the performances of the approach. This paper describes FDup(Flat Collections Deduper), a general-purpose software framework supporting a complete deduplication workflow to manage big data record collections: metadata record data model definition, identification of candidate duplicates, identification of duplicates. FDup brings two main innovations: first, it delivers a full deduplication framework in a single easy-to-use software package based on Apache Spark Hadoop framework, where developers can customize the optimal and parallel workflow steps of blocking, sliding windows, and similarity matching function via an intuitive configuration file; second, it introduces a novel approach to improve performance, beyond the known techniques of \u201cblocking\u201d and \u201csliding window\u201d, by introducing a smart similarity-matching function T-match. T-match is engineered as a decision tree that drives the comparisons of the fields of two records as branches of predicates and allows for successful or unsuccessful early exit strategies. The efficacy of the approach is proved by experiments performed over big data collections of metadata records in the OpenAIRE Graph, a known open-access knowledge base in Scholarly communication.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145546470",
                    "name": "M. Bonis"
                },
                {
                    "authorId": "2589488",
                    "name": "Claudio Atzori"
                },
                {
                    "authorId": "3216767",
                    "name": "Sandro La Bruzzo"
                },
                {
                    "authorId": "2240178920",
                    "name": "Paolo Manghi"
                }
            ]
        },
        {
            "paperId": "32bcea8cfd7ebd8654e262987fa61c0b1f96dd0d",
            "title": "(Semi)automated disambiguation of scholarly repositories",
            "abstract": "The full exploitation of scholarly repositories is pivotal in modern Open Science, and scholarly repository registries are kingpins in enabling researchers and research infrastructures to list and search for suitable repositories. However, since multiple registries exist, repository managers are keen on registering multiple times the repositories they manage to maximise their traction and visibility across different research communities, disciplines, and applications. These multiple registrations ultimately lead to information fragmentation and redundancy on the one hand and, on the other, force registries' users to juggle multiple registries, profiles and identifiers describing the same repository. Such problems are known to registries, which claim equivalence between repository profiles whenever possible by cross-referencing their identifiers across different registries. However, as we will see, this ``claim set'' is far from complete and, therefore, many replicas slip under the radar, possibly creating problems downstream. In this work, we combine such claims to create duplicate sets and extend them with the results of an automated clustering algorithm run over repository metadata descriptions. Then we manually validate our results to produce an ``as accurate as possible'' de-duplicated dataset of scholarly repositories.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1824207",
                    "name": "Miriam Baglioni"
                },
                {
                    "authorId": "2043406",
                    "name": "Andrea Mannocci"
                },
                {
                    "authorId": "2093455368",
                    "name": "Gina Pavone"
                },
                {
                    "authorId": "145546470",
                    "name": "M. Bonis"
                },
                {
                    "authorId": "1799502",
                    "name": "P. Manghi"
                }
            ]
        },
        {
            "paperId": "0d9a5dcb5006a25e5bc97c68a3ad4f35e0208456",
            "title": "A Preliminary Assessment of the Article Deduplication Algorithm Used for the OpenAIRE Research Graph",
            "abstract": "In recent years, a large number of Scholarly Knowledge Graphs (SKGs) have been introduced in the literature. The communities behind these graphs strive to gather, clean, and integrate scholarly metadata from various sources to produce clean and easy-to-process knowledge graphs. In this context, a very important task of the respective cleaning and integration workflows is deduplication. In this paper, we briefly describe and evaluate the accuracy of the deduplication algorithm used for the OpenAIRE Research Graph. Our experiments show that the algorithm has an adequate performance producing a small number of false positives and an even smaller number of false negatives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2089033890",
                    "name": "Kleanthis Vichos"
                },
                {
                    "authorId": "145546470",
                    "name": "M. Bonis"
                },
                {
                    "authorId": "1637421061",
                    "name": "Ilias Kanellos"
                },
                {
                    "authorId": "40056258",
                    "name": "Serafeim Chatzopoulos"
                },
                {
                    "authorId": "2589488",
                    "name": "Claudio Atzori"
                },
                {
                    "authorId": "2877400",
                    "name": "Natalia Manola"
                },
                {
                    "authorId": "1799502",
                    "name": "P. Manghi"
                },
                {
                    "authorId": "1768540",
                    "name": "Thanasis Vergoulis"
                }
            ]
        },
        {
            "paperId": "1f1c3eee416be035accfcb77355ebe35438c45c4",
            "title": "A Literature Review of User Studies in Extended Reality Applications for Archaeology",
            "abstract": "In the present study we conducted a systematic review on user studies for Archaeology in eXtended Reality of the last 10 years. After a screening and selection process, 52 articles were selected for an in-depth analysis. Their classification follows different axes: devices, location dependency, type of users, interaction and collaboration. We also organised the existing user studies according to tasks, evaluation measurements, number of participants, and how the study was conducted (pre-test and/or post-test, formative and summative evaluation, quantitative and qualitative data). We found an intertwined relation between Archaeology and Cultural Heritage, which is reflected in the vast presence of applications for museum exhibitions and tours on archaeological sites. Similarities between systems developed for archaeologists and for general public were also investigated. Our purpose was to find a common ground between different user studies that could help designers of the next systems have a base on which they can build their system. We also highlighted which would be the preferred and most suitable evaluation techniques, when they are needed, with the type of users to address. The results show a heterogeneity of measurable variables and possible choices, but some guidelines could be derived.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145546470",
                    "name": "M. Bonis"
                },
                {
                    "authorId": "2110492709",
                    "name": "Huyen Nguyen"
                },
                {
                    "authorId": "1782179",
                    "name": "P. Bourdot"
                }
            ]
        },
        {
            "paperId": "9ad697897f63312f1747d042c8cd7381aab07b57",
            "title": "Towards Unsupervised Machine Learning Approaches for Knowledge Graphs",
            "abstract": "Nowadays, a lot of data is in the form of Knowledge Graphs aiming at representing information as a set of nodes and relationships between them. This paper proposes an efficient framework to create informative embeddings for node classification on large knowledge graphs. Such embeddings capture how a particular node of the graph interacts with his neighborhood and indicate if it is either isolated or part of a bigger clique. Since a homogeneous graph is necessary to perform this kind of analysis, the framework exploits the metapath approach to split the heterogeneous graph into multiple homogeneous graphs. The proposed pipeline includes an unsupervised attentive neural network to merge different metapaths and produce node embeddings suitable for classification. Preliminary experiments on the IMDb dataset demonstrate the validity of the proposed approach, which can defeat current state-of-the-art unsupervised methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087693003",
                    "name": "Filippo Minutella"
                },
                {
                    "authorId": "1846129",
                    "name": "F. Falchi"
                },
                {
                    "authorId": "1799502",
                    "name": "P. Manghi"
                },
                {
                    "authorId": "145546470",
                    "name": "M. Bonis"
                },
                {
                    "authorId": "67213349",
                    "name": "Nicola Messina"
                }
            ]
        },
        {
            "paperId": "9e99f43555ba60d3bf5b1d433bd2c59fca12116c",
            "title": "FDup: a framework for general-purpose and efficient entity deduplication of record collections",
            "abstract": "Deduplication is a technique aiming at identifying and resolving duplicate metadata records in a collection. This article describes FDup (Flat Collections Deduper), a general-purpose software framework supporting a complete deduplication workflow to manage big data record collections: metadata record data model definition, identification of candidate duplicates, identification of duplicates. FDup brings two main innovations: first, it delivers a full deduplication framework in a single easy-to-use software package based on Apache Spark Hadoop framework, where developers can customize the optimal and parallel workflow steps of blocking, sliding windows, and similarity matching function via an intuitive configuration file; second, it introduces a novel approach to improve performance, beyond the known techniques of \u201cblocking\u201d and \u201csliding window\u201d, by introducing a smart similarity matching function T-match. T-match is engineered as a decision tree that drives the comparisons of the fields of two records as branches of predicates and allows for successful or unsuccessful early-exit strategies. The efficacy of the approach is proved by experiments performed over big data collections of metadata records in the OpenAIRE Research Graph, a known open access knowledge base in Scholarly communication.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "145546470",
                    "name": "M. Bonis"
                },
                {
                    "authorId": "1799502",
                    "name": "P. Manghi"
                },
                {
                    "authorId": "2589488",
                    "name": "Claudio Atzori"
                }
            ]
        },
        {
            "paperId": "e5126a6c077d20edf4fcfa8c305577c23f35986d",
            "title": "Entity deduplication in big data graphs for scholarly communication",
            "abstract": "PurposeSeveral online services offer functionalities to access information from \u201cbig research graphs\u201d (e.g. Google Scholar, OpenAIRE, Microsoft Academic Graph), which correlate scholarly/scientific communication entities such as publications, authors, datasets, organizations, projects, funders, etc. Depending on the target users, access can vary from search and browse content to the consumption of statistics for monitoring and provision of feedback. Such graphs are populated over time as aggregations of multiple sources and therefore suffer from major entity-duplication problems. Although deduplication of graphs is a known and actual problem, existing solutions are dedicated to specific scenarios, operate on flat collections, local topology-drive challenges and cannot therefore be re-used in other contexts.Design/methodology/approachThis work presents GDup, an integrated, scalable, general-purpose system that can be customized to address deduplication over arbitrary large information graphs. The paper presents its high-level architecture, its implementation as a service used within the OpenAIRE infrastructure system and reports numbers of real-case experiments.FindingsGDup provides the functionalities required to deliver a fully-fledged entity deduplication workflow over a generic input graph. The system offers out-of-the-box Ground Truth management, acquisition of feedback from data curators and algorithms for identifying and merging duplicates, to obtain an output disambiguated graph.Originality/valueTo our knowledge GDup is the only system in the literature that offers an integrated and general-purpose solution for the deduplication graphs, while targeting big data scalability issues. GDup is today one of the key modules of the OpenAIRE infrastructure production system, which monitors Open Science trends on behalf of the European Commission, National funders and institutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1799502",
                    "name": "P. Manghi"
                },
                {
                    "authorId": "2589488",
                    "name": "Claudio Atzori"
                },
                {
                    "authorId": "145546470",
                    "name": "M. Bonis"
                },
                {
                    "authorId": "39565794",
                    "name": "A. Bardi"
                }
            ]
        },
        {
            "paperId": "9982fe5bab006220736bf893e6bebd6dde308038",
            "title": "Evaluation of the concrete characteristics by measurements of sonic wave velocity",
            "abstract": "(10 pt) The seismic vulnerability of the existing RC buildings and their retrofit are two important topics in modern seismic engineering (both search and professional practice). The related problems with assessment of material and structural characteristics have been extensively studied in last years and then incorporated in seismic codes. In particular, in order to define new effective methodologies and techniques many studies have been performed. Moreover, economic problems regarding these techniques must to be considered. Commonly, in situ tests on RC buildings are subdivided in Non Destructive Test (NDT) and Destructive Tests (DT). The advantage of NDTs tests is that no damage is caused to the structural elements and moreover provide a large amount of data with relatively low cost. In this paper we propose a fast NDT methodology based on the measure of the sonic wave velocity within the RC structural elements. This technique has been developed by means of a wide experimental campaign (carried out in Laboratory of University of Basilicata) on RC elements extract by existing RC buildings. An impulse is generated by a hammer on the surface of the considered elements. Then the average velocity of the sonic waves was estimated. Finally, the results were compared with those obtained by the classical ultrasonic method. From the analysis of the preliminary applications, the proposed technique seems to be able to describe the concrete characteristics with few economic costs due to unexisting damage on non structural components.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41067235",
                    "name": "R. Ditommaso"
                },
                {
                    "authorId": "3637873",
                    "name": "M. Mucciarelli"
                },
                {
                    "authorId": "37293255",
                    "name": "M. Vona"
                },
                {
                    "authorId": "47555139",
                    "name": "A. Masi"
                },
                {
                    "authorId": "145546470",
                    "name": "M. Bonis"
                }
            ]
        }
    ]
}