{
    "authorId": "2148906289",
    "papers": [
        {
            "paperId": "cd4af2a6fdfc85b9fb514989a41551186cf303dc",
            "title": "4DBInfer: A 4D Benchmarking Toolbox for Graph-Centric Predictive Modeling on Relational DBs",
            "abstract": "Although RDBs store vast amounts of rich, informative data spread across interconnected tables, the progress of predictive machine learning models as applied to such tasks arguably falls well behind advances in other domains such as computer vision or natural language processing. This deficit stems, at least in part, from the lack of established/public RDB benchmarks as needed for training and evaluation purposes. As a result, related model development thus far often defaults to tabular approaches trained on ubiquitous single-table benchmarks, or on the relational side, graph-based alternatives such as GNNs applied to a completely different set of graph datasets devoid of tabular characteristics. To more precisely target RDBs lying at the nexus of these two complementary regimes, we explore a broad class of baseline models predicated on: (i) converting multi-table datasets into graphs using various strategies equipped with efficient subsampling, while preserving tabular characteristics; and (ii) trainable models with well-matched inductive biases that output predictions based on these input subgraphs. Then, to address the dearth of suitable public benchmarks and reduce siloed comparisons, we assemble a diverse collection of (i) large-scale RDB datasets and (ii) coincident predictive tasks. From a delivery standpoint, we operationalize the above four dimensions (4D) of exploration within a unified, scalable open-source toolbox called 4DBInfer. We conclude by presenting evaluations using 4DBInfer, the results of which highlight the importance of considering each such dimension in the design of RDB predictive models, as well as the limitations of more naive approaches such as simply joining adjacent tables. Our source code is released at https://github.com/awslabs/multi-table-benchmark .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2244248914",
                    "name": "Minjie Wang"
                },
                {
                    "authorId": "2257302315",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "2256992062",
                    "name": "David Wipf"
                },
                {
                    "authorId": "35349851",
                    "name": "Zhenkun Cai"
                },
                {
                    "authorId": "2298888654",
                    "name": "Ning Li"
                },
                {
                    "authorId": "2299152040",
                    "name": "Jianheng Tang"
                },
                {
                    "authorId": "2295790287",
                    "name": "Yanlin Zhang"
                },
                {
                    "authorId": "2275613451",
                    "name": "Zizhao Zhang"
                },
                {
                    "authorId": "2298905319",
                    "name": "Zunyao Mao"
                },
                {
                    "authorId": "2298974245",
                    "name": "Yakun Song"
                },
                {
                    "authorId": "2298929357",
                    "name": "Yanbo Wang"
                },
                {
                    "authorId": "2298940011",
                    "name": "Jiahang Li"
                },
                {
                    "authorId": "2269699052",
                    "name": "Han Zhang"
                },
                {
                    "authorId": "2299189130",
                    "name": "Guang Yang"
                },
                {
                    "authorId": "2268022441",
                    "name": "Xiao Qin"
                },
                {
                    "authorId": "2223137915",
                    "name": "Chuan Lei"
                },
                {
                    "authorId": "2279927888",
                    "name": "Mu-Nan Zhang"
                },
                {
                    "authorId": "2257343831",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2263543517",
                    "name": "Christos Faloutsos"
                },
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                }
            ]
        },
        {
            "paperId": "46210e170045df3c0c50a17bb63e6de480d62f9d",
            "title": "FreshGNN: Reducing Memory Access via Stable Historical Embeddings for Graph Neural Network Training",
            "abstract": "A key performance bottleneck when training graph neural network (GNN) models on large, real-world graphs is loading node features onto a GPU. Due to limited GPU memory, expensive data movement is necessary to facilitate the storage of these features on alternative devices with slower access (e.g. CPU memory). Moreover, the irregularity of graph structures contributes to poor data locality which further exacerbates the problem. Consequently, existing frameworks capable of efficiently training large GNN models usually incur a significant accuracy degradation because of the currently-available shortcuts involved. To address these limitations, we instead propose FreshGNN, a general-purpose GNN mini-batch training framework that leverages a historical cache for storing and reusing GNN node embeddings instead of re-computing them through fetching raw features at every iteration. Critical to its success, the corresponding cache policy is designed, using a combination of gradient-based and staleness criteria, to selectively screen those embeddings which are relatively stable and can be cached, from those that need to be re-computed to reduce estimation errors and subsequent downstream accuracy loss. When paired with complementary system enhancements to support this selective historical cache, FreshGNN is able to accelerate the training speed on large graph datasets such as ogbn-papers100M and MAG240M by 3.4\u00d7 up to 20.5\u00d7 and reduce the memory access by 59%, with less than 1% influence on test accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1512189758",
                    "name": "Kezhao Huang"
                },
                {
                    "authorId": "1557293815",
                    "name": "Haitian Jiang"
                },
                {
                    "authorId": "2108593481",
                    "name": "Minjie Wang"
                },
                {
                    "authorId": "2046958974",
                    "name": "Guangxuan Xiao"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                },
                {
                    "authorId": "2118943843",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "2109583192",
                    "name": "Zengfeng Huang"
                },
                {
                    "authorId": "2467444",
                    "name": "Jidong Zhai"
                },
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                }
            ]
        },
        {
            "paperId": "271f40f4fad4d112e436565e668b79ede690d755",
            "title": "Inductive Relation Prediction Using Analogy Subgraph Embeddings",
            "abstract": "Prevailing methods for relation prediction in heterogeneous graphs including knowledge graphs aim at learning the latent representations (i.e., embeddings) of observed nodes and relations, and are thus limited to the transductive setting where the relation types must be known during training. In this paper, we propose ANalogy SubGraph Embedding Learning (GraphANGEL), a novel relation prediction framework that predicts relations between each node pair by checking whether the subgraphs containing the pair are similar to other subgraphs containing the considered relation. Each graph pattern explicitly represents a specific logical rule, which contributes to an inductive bias that facilitates generalization to unseen relation types and leads to more explainable predictive models. Our model consistently outperforms existing models in terms of heterogeneous graph based recommendation as well as knowledge graph completion. We also empirically demonstrate the capability of our model in generalizing to new relation types while producing explainable heat maps of attention scores across the discovered logics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "16278568",
                    "name": "Jiarui Jin"
                },
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                },
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "2108309275",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                },
                {
                    "authorId": "2156098229",
                    "name": "Yong Yu"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                }
            ]
        },
        {
            "paperId": "6a93b0188940cfaa578e5686f6f60f4e96a6a92b",
            "title": "Self-supervised Amodal Video Object Segmentation",
            "abstract": "Amodal perception requires inferring the full shape of an object that is partially occluded. This task is particularly challenging on two levels: (1) it requires more information than what is contained in the instant retina or imaging sensor, (2) it is difficult to obtain enough well-annotated amodal labels for supervision. To this end, this paper develops a new framework of Self-supervised amodal Video object segmentation (SaVos). Our method efficiently leverages the visual information of video temporal sequences to infer the amodal mask of objects. The key intuition is that the occluded part of an object can be explained away if that part is visible in other frames, possibly deformed as long as the deformation can be reasonably learned. Accordingly, we derive a novel self-supervised learning paradigm that efficiently utilizes the visible object parts as the supervision to guide the training on videos. In addition to learning type prior to complete masks for known types, SaVos also learns the spatiotemporal prior, which is also useful for the amodal task and could generalize to unseen types. The proposed framework achieves the state-of-the-art performance on the synthetic amodal segmentation benchmark FISHBOWL and the real world benchmark KINS-Video-Car. Further, it lends itself well to being transferred to novel distributions using test-time adaptation, outperforming existing models even after the transfer to a new distribution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110070019",
                    "name": "Jian Yao"
                },
                {
                    "authorId": "2159644735",
                    "name": "Yuxin Hong"
                },
                {
                    "authorId": "151472814",
                    "name": "Chiyu Wang"
                },
                {
                    "authorId": "39102205",
                    "name": "Tianjun Xiao"
                },
                {
                    "authorId": "2118328320",
                    "name": "Tong He"
                },
                {
                    "authorId": "2265580622",
                    "name": "Francesco Locatello"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                },
                {
                    "authorId": "35782003",
                    "name": "Yanwei Fu"
                },
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                }
            ]
        },
        {
            "paperId": "6e7e2b075363015e046dcb1fcddae9f5c30c68f5",
            "title": "Learning Enhanced Representations for Tabular Data via Neighborhood Propagation",
            "abstract": "Prediction over tabular data is an essential and fundamental problem in many important downstream tasks. However, existing methods either take a data instance of the table independently as input or do not fully utilize the multi-rows features and labels to directly change and enhance the target data representations. In this paper, we propose to 1) construct a hypergraph from relevant data instance retrieval to model the cross-row and cross-column patterns of those instances, and 2) perform message Propagation to Enhance the target data instance representation for Tabular prediction tasks. Specifically, our specially-designed message propagation step benefits from 1) fusion of label and features during propagation, and 2) locality-aware high-order feature interactions. Experiments on two important tabular data prediction tasks validate the superiority of the proposed PET model against other baselines. Additionally, we demonstrate the effectiveness of the model components and the feature enhancement ability of PET via various ablation studies and visualizations. The code is included in https://github.com/KounianhuaDu/PET.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "2108309275",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2046150680",
                    "name": "Ruiwen Zhou"
                },
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                },
                {
                    "authorId": "2163050574",
                    "name": "Xilong Zhao"
                },
                {
                    "authorId": "16278568",
                    "name": "Jiarui Jin"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                }
            ]
        },
        {
            "paperId": "7276ba18dbb89a1b360f03470ef9c02f53bb62e4",
            "title": "SSEGCN: Syntactic and Semantic Enhanced Graph Convolutional Network for Aspect-based Sentiment Analysis",
            "abstract": "Aspect-based Sentiment Analysis (ABSA) aims to predict the sentiment polarity towards a particular aspect in a sentence. Recently, graph neural networks based on dependency tree convey rich structural information which is proven to be utility for ABSA. However, how to effectively harness the semantic and syntactic structure information from the dependency tree remains a challenging research question. In this paper, we propose a novel Syntactic and Semantic Enhanced Graph Convolutional Network (SSEGCN) model for ABSA task. Specifically, we propose an aspect-aware attention mechanism combined with self-attention to obtain attention score matrices of a sentence, which can not only learn the aspect-related semantic correlations, but also learn the global semantics of the sentence. In order to obtain comprehensive syntactic structure information, we construct syntactic mask matrices of the sentence according to the different syntactic distances between words. Furthermore, to combine syntactic structure and semantic information, we equip the attention score matrices by syntactic mask matrices. Finally, we enhance the node representations with graph convolutional network over attention score matrices for ABSA. Experimental results on benchmark datasets illustrate that our proposed model outperforms state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2109495236",
                    "name": "Zili Zhou"
                },
                {
                    "authorId": "2108837681",
                    "name": "Yanna Wang"
                }
            ]
        },
        {
            "paperId": "a74ef2ac05e167bbef9c5fa4f59f31ee04c00454",
            "title": "Refined Edge Usage of Graph Neural Networks for Edge Prediction",
            "abstract": "Graph Neural Networks (GNNs), originally proposed for node classification, have also motivated many recent works on edge prediction (a.k.a., link prediction). However, existing methods lack elaborate design regarding the distinctions between two tasks that have been frequently overlooked: (i) edges only constitute the topology in the node classification task but can be used as both the topology and the supervisions (i.e., labels) in the edge prediction task; (ii) the node classification makes prediction over each individual node, while the edge prediction is determinated by each pair of nodes. To this end, we propose a novel edge prediction paradigm named Edge-aware Message PassIng neuRal nEtworks (EMPIRE). Concretely, we first introduce an edge splitting technique to specify use of each edge where each edge is solely used as either the topology or the supervision (named as topology edge or supervision edge). We then develop a new message passing mechanism that generates the messages to source nodes (through topology edges) being aware of target nodes (through supervision edges). In order to emphasize the differences between pairs connected by supervision edges and pairs unconnected, we further weight the messages to highlight the relative ones that can reflect the differences. In addition, we design a novel negative node-pair sampling trick that efficiently samples 'hard' negative instances in the supervision instances, and can significantly improve the performance. Experimental results verify that the proposed method can significantly outperform existing state-of-the-art models regarding the edge prediction task on multiple homogeneous and heterogeneous graph datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "16278568",
                    "name": "Jiarui Jin"
                },
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                },
                {
                    "authorId": "2108309275",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "2118943843",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "2156098229",
                    "name": "Yong Yu"
                },
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                }
            ]
        },
        {
            "paperId": "1b65f01c6fdb6c836e82ce716264d5391663228f",
            "title": "Bag of Tricks for Node Classification with Graph Neural Networks",
            "abstract": "Over the past few years, graph neural networks (GNN) and label propagation-based methods have made significant progress in addressing node classification tasks on graphs. However, in addition to their reliance on elaborate architectures and algorithms, there are several key technical details that are frequently overlooked, and yet nonetheless can play a vital role in achieving satisfactory performance. In this paper, we first summarize a series of existing tricks-of-the-trade, and then propose several new ones related to label usage, loss function formulation, and model design that can significantly improve various GNN architectures. We empirically evaluate their impact on final node classification accuracy by conducting ablation studies and demonstrate consistently-improved performance, often to an extent that outweighs the gains from more dramatic changes in the underlying GNN architecture. Notably, many of the top-ranked models on the Open Graph Benchmark (OGB) leaderboard and KDDCUP 2021 Large-Scale Challenge MAG240M-LSC benefit from these techniques we initiated.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                },
                {
                    "authorId": "16278568",
                    "name": "Jiarui Jin"
                },
                {
                    "authorId": "2108309275",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "1811427",
                    "name": "Yong Yu"
                },
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                }
            ]
        },
        {
            "paperId": "2f64df4e61994b127e36f8114da3a070535941f4",
            "title": "Why Propagate Alone? Parallel Use of Labels and Features on Graphs",
            "abstract": "Graph neural networks (GNNs) and label propagation represent two interrelated modeling strategies designed to exploit graph structure in tasks such as node property prediction. The former is typically based on stacked message-passing layers that share neighborhood information to transform node features into predictive embeddings. In contrast, the latter involves spreading label information to unlabeled nodes via a parameter-free diffusion process, but operates independently of the node features. Given then that the material difference is merely whether features or labels are smoothed across the graph, it is natural to consider combinations of the two for improving performance. In this regard, it has recently been proposed to use a randomly-selected portion of the training labels as GNN inputs, concatenated with the original node features for making predictions on the remaining labels. This so-called label trick accommodates the parallel use of features and labels, and is foundational to many of the top-ranking submissions on the Open Graph Benchmark (OGB) leaderboard. And yet despite its wide-spread adoption, thus far there has been little attempt to carefully unpack exactly what statistical properties the label trick introduces into the training pipeline, intended or otherwise. To this end, we prove that under certain simplifying assumptions, the stochastic label trick can be reduced to an interpretable, deterministic training objective composed of two factors. The first is a data-fitting term that naturally resolves potential label leakage issues, while the second serves as a regularization factor conditioned on graph structure that adapts to graph size and connectivity. Later, we leverage this perspective to motivate a broader range of label trick use cases, and provide experiments to verify the efficacy of these extensions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                },
                {
                    "authorId": "16278568",
                    "name": "Jiarui Jin"
                },
                {
                    "authorId": "2108309275",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2144182425",
                    "name": "Yongyi Yang"
                },
                {
                    "authorId": "1391200710",
                    "name": "Jiuhai Chen"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "2119021541",
                    "name": "Yong Yu"
                },
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2109583192",
                    "name": "Zengfeng Huang"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                }
            ]
        },
        {
            "paperId": "9ca0b1c2b14fe2ad61c757baf95fecfbc318d640",
            "title": "Learning Hierarchical Graph Neural Networks for Image Clustering",
            "abstract": "We propose a hierarchical graph neural network (GNN) model that learns how to cluster a set of images into an unknown number of identities using a training set of images annotated with labels belonging to a disjoint set of identities. Our hierarchical GNN uses a novel approach to merge connected components predicted at each level of the hierarchy to form a new graph at the next level. Unlike fully unsupervised hierarchical clustering, the choice of grouping and complexity criteria stems naturally from supervision in the training set. The resulting method, Hi-LANDER, achieves an average of 49% improvement in F-score and 7% increase in Normalized Mutual Information (NMI) relative to current GNN-based clustering algorithms. Additionally, state-of-the-art GNN-based methods rely on separate models to predict linkage probabilities and node densities as intermediate steps of the clustering process. In contrast, our unified framework achieves a three-fold decrease in computational cost. Our training and inference code are released 1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2072728559",
                    "name": "Yifan Xing"
                },
                {
                    "authorId": "48748557",
                    "name": "Tong He"
                },
                {
                    "authorId": "39102205",
                    "name": "Tianjun Xiao"
                },
                {
                    "authorId": "2130354677",
                    "name": "Yongxin Wang"
                },
                {
                    "authorId": "3331521",
                    "name": "Yuanjun Xiong"
                },
                {
                    "authorId": "50875615",
                    "name": "Weihao Xia"
                },
                {
                    "authorId": "2117590329",
                    "name": "David Wipf Paul"
                },
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "1715959",
                    "name": "Stefano Soatto"
                }
            ]
        }
    ]
}