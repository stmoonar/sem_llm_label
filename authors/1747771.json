{
    "authorId": "1747771",
    "papers": [
        {
            "paperId": "07886354d30fadcf6404e6a4fe1fab1ea9d01096",
            "title": "Same Same, But Different: Conditional Multi-Task Learning for Demographic-Specific Toxicity Detection",
            "abstract": "Algorithmic bias often arises as a result of differential subgroup validity, in which predictive relationships vary across groups. For example, in toxic language detection, comments targeting different demographic groups can vary markedly across groups. In such settings, trained models can be dominated by the relationships that best fit the majority group, leading to disparate performance. We propose framing toxicity detection as multi-task learning (MTL), allowing a model to specialize on the relationships that are relevant to each demographic group while also leveraging shared properties across groups. With toxicity detection, each task corresponds to identifying toxicity against a particular demographic group. However, traditional MTL requires labels for all tasks to be present for every data point. To address this, we propose Conditional MTL (CondMTL), wherein only training examples relevant to the given demographic group are considered by the loss function. This lets us learn group specific representations in each branch which are not cross contaminated by irrelevant labels. Results on synthetic and real data show that using CondMTL improves predictive recall over various baselines in general and for the minority demographic group in particular, while having similar overall accuracy.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "144928420",
                    "name": "Soumyajit Gupta"
                },
                {
                    "authorId": "2136623474",
                    "name": "Sooyong Lee"
                },
                {
                    "authorId": "1406443102",
                    "name": "Maria De-Arteaga"
                },
                {
                    "authorId": "1747771",
                    "name": "Matthew Lease"
                }
            ]
        },
        {
            "paperId": "0e2e40e3719159083f5b41a031122fe5334f2789",
            "title": "Learning Complementary Policies for Human-AI Teams",
            "abstract": "Human-AI complementarity is important when neither the algorithm nor the human yields dominant performance across all instances in a given context. Recent work that explored human-AI collaboration has considered decisions that correspond to classification tasks. However, in many important contexts where humans can benefit from AI complementarity, humans undertake course of action. In this paper, we propose a framework for a novel human-AI collaboration for selecting advantageous course of action, which we refer to as Learning Complementary Policy for Human-AI teams (\\textsc{lcp-hai}). Our solution aims to exploit the human-AI complementarity to maximize decision rewards by learning both an algorithmic policy that aims to complement humans by a routing model that defers decisions to either a human or the AI to leverage the resulting complementarity. We then extend our approach to leverage opportunities and mitigate risks that arise in important contexts in practice: 1) when a team is composed of multiple humans with differential and potentially complementary abilities, 2) when the observational data includes consistent deterministic actions, and 3) when the covariate distribution of future decisions differ from that in the historical data. We demonstrate the effectiveness of our proposed methods using data on real human responses and semi-synthetic, and find that our methods offer reliable and advantageous performance across setting, and that it is superior to when either the algorithm or the AI make decisions on their own. We also find that the extensions we propose effectively improve the robustness of the human-AI collaboration performance in the presence of different challenging settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151184349",
                    "name": "Ruijiang Gao"
                },
                {
                    "authorId": "1388405043",
                    "name": "M. Saar-Tsechansky"
                },
                {
                    "authorId": "1406443102",
                    "name": "Maria De-Arteaga"
                },
                {
                    "authorId": "3471102",
                    "name": "Ligong Han"
                },
                {
                    "authorId": "2153200005",
                    "name": "Wei Sun"
                },
                {
                    "authorId": "2109515767",
                    "name": "Min Kyung Lee"
                },
                {
                    "authorId": "1747771",
                    "name": "Matthew Lease"
                }
            ]
        },
        {
            "paperId": "17c72166f791987ad6aa65d9505c40194bfae049",
            "title": "New Metrics to Encourage Innovation and Diversity in Information Retrieval Approaches",
            "abstract": "In evaluation campaigns, participants often explore variations of popular, state-of-the-art baselines as a low-risk strategy to achieve competitive results. While effective, this can lead to local\"hill climbing\"rather than more radical and innovative departure from standard methods. Moreover, if many participants build on similar baselines, the overall diversity of approaches considered may be limited. In this work, we propose a new class of IR evaluation metrics intended to promote greater diversity of approaches in evaluation campaigns. Whereas traditional IR metrics focus on user experience, our two\"innovation\"metrics instead reward exploration of more divergent, higher-risk strategies finding relevant documents missed by other systems. Experiments on four TREC collections show that our metrics do change system rankings by rewarding systems that find such rare, relevant documents. This result is further supported by a controlled, synthetic data experiment, and a qualitative analysis. In addition, we show that our metrics achieve higher evaluation stability and discriminative power than the standard metrics we modify. To support reproducibility, we share our source code.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2202029054",
                    "name": "Mehmet Deniz Turkmen"
                },
                {
                    "authorId": "1747771",
                    "name": "Matthew Lease"
                },
                {
                    "authorId": "33407644",
                    "name": "Mucahid Kutlu"
                }
            ]
        },
        {
            "paperId": "6812ea42fd92397f643c3b3c98fdc53622f90c9b",
            "title": "How Crowd Worker Factors Influence Subjective Annotations: A Study of Tagging Misogynistic Hate Speech in Tweets",
            "abstract": "Crowdsourced annotation is vital to both collecting labelled data to train and test automated content moderation systems and to support human-in-the-loop review of system decisions. However, annotation tasks such as judging hate speech are subjective and thus highly sensitive to biases stemming from annotator beliefs, characteristics and demographics. We conduct two crowdsourcing studies on Mechanical Turk to examine annotator bias in labelling sexist and misogynistic hate speech. Results from 109 annotators show that annotator political inclination, moral integrity, personality traits, and sexist attitudes significantly impact annotation accuracy and the tendency to tag content as hate speech. In addition, semi-structured interviews with nine crowd workers provide further insights regarding the influence of subjectivity on annotations. In exploring how workers interpret a task \u2014 shaped by complex negotiations between platform structures, task instructions, subjective motivations, and external contextual factors \u2014 we see annotations not only impacted by worker factors but also simultaneously shaped by the structures under which they labour.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2125483087",
                    "name": "Danula Hettiachchi"
                },
                {
                    "authorId": "1460085827",
                    "name": "I. Holcombe-James"
                },
                {
                    "authorId": "2237448850",
                    "name": "Stephanie Livingstone"
                },
                {
                    "authorId": "2237469073",
                    "name": "Anjalee de Silva"
                },
                {
                    "authorId": "1747771",
                    "name": "Matthew Lease"
                },
                {
                    "authorId": "144954586",
                    "name": "Flora D. Salim"
                },
                {
                    "authorId": "2202755072",
                    "name": "Mark Sanderson"
                }
            ]
        },
        {
            "paperId": "9b7cadce8fc34c0d440389f1cfa19dee4af2bace",
            "title": "Designing Closed-Loop Models for Task Allocation",
            "abstract": "Automatically assigning tasks to people is challenging because human performance can vary across tasks for many reasons. This challenge is further compounded in real-life settings in which no oracle exists to assess the quality of human decisions and task assignments made. Instead, we find ourselves in a\"closed\"decision-making loop in which the same fallible human decisions we rely on in practice must also be used to guide task allocation. How can imperfect and potentially biased human decisions train an accurate allocation model? Our key insight is to exploit weak prior information on human-task similarity to bootstrap model training. We show that the use of such a weak prior can improve task allocation accuracy, even when human decision-makers are fallible and biased. We present both theoretical analysis and empirical evaluation over synthetic data and a social media toxicity detection task. Results demonstrate the efficacy of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                },
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "1747771",
                    "name": "Matthew Lease"
                }
            ]
        },
        {
            "paperId": "038df01dceb76c5c91e5ac6c2aaff86baa066746",
            "title": "Learning a neural Pareto manifold extractor with constraints",
            "abstract": "Multi-objective optimization (MOO) problems require balancing competing objectives, often under constraints. The Pareto optimal solution set de\ufb01nes all possible optimal trade-offs over such objectives. In this work, we present a novel method for Pareto-front learning : inducing the full Pareto manifold at train-time so users can pick any desired optimal trade-off point at run-time. Our key insight is to exploit Fritz-John Conditions for a novel guided double gradient descent strategy. Evaluation on synthetic benchmark problems allows us to vary MOO problem dif\ufb01culty in controlled fashion and measure accuracy vs. known analytic solutions. We further test scalability and generalization in learning optimal neural model parameterizations for Multi-Task Learning (MTL) on image classi\ufb01cation. Results show consistent improvement in accuracy and ef\ufb01ciency over prior MTL methods as well as techniques from operations research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144928420",
                    "name": "Soumyajit Gupta"
                },
                {
                    "authorId": "151473114",
                    "name": "Gurpreet Singh"
                },
                {
                    "authorId": "16235213",
                    "name": "Raghu Bollapragada"
                },
                {
                    "authorId": "1747771",
                    "name": "Matthew Lease"
                }
            ]
        },
        {
            "paperId": "0cbcf7ad41462c4a7396fe705437fa89f1d70817",
            "title": "Data excellence for AI",
            "abstract": "This forum provides a space to engage with the challenges of designing for intelligent algorithmic experiences. We invite articles that tackle the tensions between research and practice when integrating AI and UX design. We welcome interdisciplinary debate, artful critique, forward-looking research, case studies of AI in practice, and speculative design explorations. --- Juho Kim and Henriette Cramer, Editors",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1745337",
                    "name": "Lora Aroyo"
                },
                {
                    "authorId": "1747771",
                    "name": "Matthew Lease"
                },
                {
                    "authorId": "2990264",
                    "name": "Praveen K. Paritosh"
                },
                {
                    "authorId": "10685155",
                    "name": "M. Schaekermann"
                }
            ]
        },
        {
            "paperId": "254440d043cea61a2c6106c80aed7ddf5918c086",
            "title": "ProtoTEx: Explaining Model Decisions with Prototype Tensors",
            "abstract": "We present ProtoTEx, a novel white-box NLP classification architecture based on prototype networks (Li et al., 2018). ProtoTEx faithfully explains model decisions based on prototype tensors that encode latent clusters of training examples. At inference time, classification decisions are based on the distances between the input text and the prototype tensors, explained via the training examples most similar to the most influential prototypes. We also describe a novel interleaved training algorithm that effectively handles classes characterized by ProtoTEx indicative features. On a propaganda detection task, ProtoTEx accuracy matches BART-large and exceeds BERTlarge with the added benefit of providing faithful explanations. A user study also shows that prototype-based explanations help non-experts to better recognize propaganda in online news.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47295297",
                    "name": "Anubrata Das"
                },
                {
                    "authorId": "2124123754",
                    "name": "Chitrank Gupta"
                },
                {
                    "authorId": "3455255",
                    "name": "Venelin Kovatchev"
                },
                {
                    "authorId": "1747771",
                    "name": "Matthew Lease"
                },
                {
                    "authorId": "2108933878",
                    "name": "J. Li"
                }
            ]
        },
        {
            "paperId": "4ead4b50a31815b234efe43c8f6be75ccdcf4b92",
            "title": "Measuring Annotator Agreement Generally across Complex Structured, Multi-object, and Free-text Annotation Tasks",
            "abstract": "When annotators label data, a key metric for quality assurance is inter-annotator agreement (IAA): the extent to which annotators agree on their labels. Though many IAA measures exist for simple categorical and ordinal labeling tasks, relatively little work has considered more complex labeling tasks, such as structured, multi-object, and free-text annotations. Krippendorff\u2019s \u03b1, best known for use with simpler labeling tasks, does have a distance-based formulation with broader applicability, but little work has studied its efficacy and consistency across complex annotation tasks. We investigate the design and evaluation of IAA measures for complex annotation tasks, with evaluation spanning seven diverse tasks: image bounding boxes, image keypoints, text sequence tagging, ranked lists, free text translations, numeric vectors, and syntax trees. We identify the difficulty of interpretability and the complexity of choosing a distance function as key obstacles in applying Krippendorff\u2019s \u03b1 generally across these tasks. We propose two novel, more interpretable measures, showing they yield more consistent IAA measures across tasks and annotation distance functions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51427469",
                    "name": "Alexander Braylan"
                },
                {
                    "authorId": "144419174",
                    "name": "Omar Alonso"
                },
                {
                    "authorId": "1747771",
                    "name": "Matthew Lease"
                }
            ]
        },
        {
            "paperId": "8992513f975d407d6198744123fb3bfdef5a0d7c",
            "title": "Designing Closed Human-in-the-loop Deferral Pipelines",
            "abstract": "In hybrid human-machine deferral frameworks, a classifier can defer uncertain cases to human decision-makers (who are often themselves fallible). Prior work on simultaneous training of such classifier and deferral models has typically assumed access to an oracle during training to obtain true class labels for training samples, but in practice there often is no such oracle. In contrast, we consider a\"closed\"decision-making pipeline in which the same fallible human decision-makers used in deferral also provide training labels. How can imperfect and biased human expert labels be used to train a fair and accurate deferral framework? Our key insight is that by exploiting weak prior information, we can match experts to input examples to ensure fairness and accuracy of the resulting deferral framework, even when imperfect and biased experts are used in place of ground truth labels. The efficacy of our approach is shown both by theoretical analysis and by evaluation on two tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                },
                {
                    "authorId": "1747771",
                    "name": "Matthew Lease"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                }
            ]
        }
    ]
}