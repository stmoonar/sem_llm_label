{
    "authorId": "1802836",
    "papers": [
        {
            "paperId": "5b0679c7255261b2ce1f8fe037e7b663de123ddc",
            "title": "HGaze Typing: Head-Gesture Assisted Gaze Typing",
            "abstract": "This paper introduces a bi-modal typing interface, HGaze Typing, which combines the simplicity of head gestures with the speed of gaze inputs to provide efficient and comfortable dwell-free text entry. HGaze Typing uses gaze path information to compute candidate words and allows explicit activation of common text entry commands, such as selection, deletion, and revision, by using head gestures (nodding, shaking, and tilting). By adding a head-based input channel, HGaze Typing reduces the size of the screen regions for cancel/deletion buttons and the word candidate list, which are required by most eye-typing interfaces. A user study finds HGaze Typing outperforms a dwell-time-based keyboard in efficacy and user satisfaction. The results demonstrate that the proposed method of integrating gaze and head-movement inputs can serve as an effective interface for text entry and is robust to unintended selections.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802836",
                    "name": "Wenxin Feng"
                },
                {
                    "authorId": "2105470775",
                    "name": "Jiangnan Zou"
                },
                {
                    "authorId": "2905689",
                    "name": "Andrew T. N. Kurauchi"
                },
                {
                    "authorId": "23125434",
                    "name": "C. Morimoto"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                }
            ]
        },
        {
            "paperId": "21584399c300f78b0e93f02e1cfcc938a75e7b55",
            "title": "Designing and Evaluating Head-based Pointing on Smartphones for People with Motor Impairments",
            "abstract": "Head-based pointing is an alternative input method for people with motor impairments to access computing devices. This paper proposes a calibration-free head-tracking input mechanism for mobile devices that makes use of the front-facing camera that is standard on most devices. To evaluate our design, we performed two Fitts\u2019 Law studies. First, a comparison study of our method with an existing head-based pointing solution, Eva Facial Mouse, with subjects without motor impairments. Second, we conducted what we believe is the first Fitts\u2019 Law study using a mobile head tracker with subjects with motor impairments. We extend prior studies with a greater range of index of difficulties (IDs) [1.62, 5.20] bits and achieved promising throughput (average 0.61 bps with motor impairments and 0.90 bps without). We found that users\u2019 throughput was 0.95 bps on average in our most difficult task (IDs: 5.20 bits), which involved selecting a target half the size of the Android recommendation for a touch target after moving nearly the full height of the screen. This suggests the system is capable of fine precision tasks. We summarize our observations and the lessons from our user studies into a set of design guidelines for head-based pointing systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52227780",
                    "name": "Muratcan Cicek"
                },
                {
                    "authorId": "2006772255",
                    "name": "Ankit Dave"
                },
                {
                    "authorId": "1802836",
                    "name": "Wenxin Feng"
                },
                {
                    "authorId": "143965164",
                    "name": "Michael Xuelin Huang"
                },
                {
                    "authorId": "2265623",
                    "name": "J. Haines"
                },
                {
                    "authorId": "2057155807",
                    "name": "Jeffrey Nichols"
                }
            ]
        },
        {
            "paperId": "5d2c2dc0dec918f05ff66d8050716665892416a9",
            "title": "Swipe&Switch: Text Entry Using Gaze Paths and Context Switching",
            "abstract": "Swipe-based methods for text entry by gaze allow users to swipe through the letters of a word by gaze, analogous to how they can swipe with a finger on a touchscreen keyboard. Two challenges for these methods are: (1) gaze paths do not possess clear start and end positions, and (2) it is difficult to design text editing features. We introduce Swipe&Switch, a text-entry interface that uses swiping and switching to improve gaze-based interaction. The interface contains three context regions, and detects the start/end of a gesture and emits text editing commands (e.g., word insertion, deletion) when a user switches focus between these regions. A user study showed that Swipe&Switch provides a better user experience and higher text entry rate over a baseline, EyeSwipe.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2905689",
                    "name": "Andrew T. N. Kurauchi"
                },
                {
                    "authorId": "1802836",
                    "name": "Wenxin Feng"
                },
                {
                    "authorId": "1702222",
                    "name": "Ajjen Joshi"
                },
                {
                    "authorId": "23125434",
                    "name": "C. Morimoto"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                }
            ]
        },
        {
            "paperId": "6a73b79b561108f68936888aad68e6351fd143a8",
            "title": "WATouCH: Enabling Direct Input on Non-touchscreen Using Smartwatch's Photoplethysmogram and IMU Sensor Fusion",
            "abstract": "Interacting with non-touchscreens such as TV or public displays can be difficult and inefficient. We propose WATouCH, a novel method that localizes a smartwatch on a display and allows direct input by turning the smartwatch into a tangible controller. This low-cost solution leverages sensor fusion of the built-in inertial measurement unit (IMU) and photoplethysmogram (PPG) sensor on a smartwatch that is used for heart rate monitoring. Specifically, WATouCH tracks the smartwatch movement using IMU data and corrects its location error caused by drift using the PPG responses to a dynamic visual pattern on the display. We conducted a user study on two tasks -- a point and click and line tracing task -- to evaluate the system usability and user performance. Evaluation results suggested that our sensor fusion mechanism effectively confined IMU-based localization error, achieved encouraging targeting and tracing precision, was well received by the participants, and thus opens up new opportunities for interaction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39628866",
                    "name": "H. Yeo"
                },
                {
                    "authorId": "1802836",
                    "name": "Wenxin Feng"
                },
                {
                    "authorId": "143965164",
                    "name": "Michael Xuelin Huang"
                }
            ]
        },
        {
            "paperId": "0b708594db5cff8bf02c0a2127b744d1a8c653f8",
            "title": "Dwell-free input methods for people with motor impairments",
            "abstract": "Millions of individuals affected by disorders or injuries that cause severe motor impairments have difficulty performing compound manipulations using traditional input devices. This thesis first explores how effective various assistive technologies are for people with motor impairments. The following questions are studied: (1) What activities are performed? (2) What tools are used to support these activities? (3) What are the advantages and limitations of these tools? (4) How do users learn about and choose assistive technologies? (5) Why do users adopt or abandon certain tools? A qualitative study of fifteen people with motor impairments indicates that users have strong needs for efficient text entry and communication tools that are not met by existing technologies.\n\nTo address these needs, this thesis proposes three dwell-free input methods, designed to improve the efficacy of target selection and text entry based on eye-tracking and head-tracking systems. They yield: (1) the Target Reverse Crossing selection mechanism, (2) the EyeSwipe eye-typing interface, and (3) the HGaze Typing interface. With Target Reverse Crossing, a user moves the cursor into a target and reverses over a goal to select it. This mechanism is significantly more efficient than dwell-time selection. Target Reverse Crossing is then adapted in EyeSwipe to delineate the start and end of a word that is eye-typed with a gaze path connecting the intermediate characters (as with traditional gesture typing). When compared with a dwell-based virtual keyboard, EyeSwipe affords higher text entry rates and a more comfortable interaction. Finally, HGaze Typing adds head gestures to gaze-path-based text entry to enable simple and explicit command activations. Results from a user study demonstrate that HGaze Typing has better performance and user satisfaction than a dwell-time method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802836",
                    "name": "Wenxin Feng"
                }
            ]
        },
        {
            "paperId": "18bfc5da4428767928fa9c51f2648d695dc20f8b",
            "title": "Exploration of Assistive Technologies Used by People with Quadriplegia Caused by Degenerative Neurological Diseases",
            "abstract": "ABSTRACT Various assistive devices and interfaces to access the computer have been developed for people with severe motor impairments. This article explores how effective these technologies are for individuals with quadriplegia caused by degenerative neurological diseases. The following questions are studied: (1) What activities are performed? (2) What tools are used? (3) What are the advantages and limitations of the tools? (4) How do users learn about and choose assistive technologies? (5) Why are some technologies abandoned? Results of a qualitative study with 15 participants indicate that study participants have strong needs for efficient text entry and communication that are not met. A lack of information about technology options limits the choices of several of the study participants. The study revealed that automated interface personalization and adaptation to disease progression should be important design goals for future assistive technologies that support users with quadriplegia caused by degenerative neurological diseases.",
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802836",
                    "name": "Wenxin Feng"
                },
                {
                    "authorId": "2128305",
                    "name": "M. Sameki"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                }
            ]
        },
        {
            "paperId": "a656bc76227d5171a96c5de7bbf4c3756cd3023a",
            "title": "EnseWing: Creating an Instrumental Ensemble Playing Experience for Children with Limited Music Training",
            "abstract": "While instrumental ensemble playing can benefit children's music education and collaboration skill development, it requires extensive training on music and instruments, which many school children lack. To help children with limited music training experience instrumental ensemble playing, we created EnseWing, an interactive system that offers such an experience. In this paper, we report the design of the EnseWing experience and a two-month field study. Our results show that EnseWing preserves the music and ensemble skills from traditional instrumental ensemble and provides more collaboration opportunities for children.",
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46232973",
                    "name": "Fei Lyu"
                },
                {
                    "authorId": "143853343",
                    "name": "Feng Tian"
                },
                {
                    "authorId": "1802836",
                    "name": "Wenxin Feng"
                },
                {
                    "authorId": "144937256",
                    "name": "Xiang Cao"
                },
                {
                    "authorId": "1779687",
                    "name": "X. Zhang"
                },
                {
                    "authorId": "2135951",
                    "name": "G. Dai"
                },
                {
                    "authorId": "2110203445",
                    "name": "Hongan Wang"
                }
            ]
        },
        {
            "paperId": "09c14bae4f86bf44c530935fc02eee2243e95e04",
            "title": "EyeSwipe: Dwell-free Text Entry Using Gaze Paths",
            "abstract": "Text entry using gaze-based interaction is a vital communication tool for people with motor impairments. Most solutions require the user to fixate on a key for a given dwell time to select it, thus limiting the typing speed. In this paper we introduce EyeSwipe, a dwell-time-free gaze-typing method. With EyeSwipe, the user gaze-types the first and last characters of a word using the novel selection mechanism \"reverse crossing.\" To gaze-type the characters in the middle of the word, the user only needs to glance at the vicinity of the respective keys. We compared the performance of EyeSwipe with that of a dwell-time-based virtual keyboard. EyeSwipe afforded statistically significantly higher typing rates and more comfortable interaction in experiments with ten participants who reached 11.7 words per minute (wpm) after 30 min typing with EyeSwipe.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2905689",
                    "name": "Andrew T. N. Kurauchi"
                },
                {
                    "authorId": "1802836",
                    "name": "Wenxin Feng"
                },
                {
                    "authorId": "1702222",
                    "name": "Ajjen Joshi"
                },
                {
                    "authorId": "23125434",
                    "name": "C. Morimoto"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                }
            ]
        },
        {
            "paperId": "4e29ea5db936e452f3a4e8e1479c583d6a87e3d4",
            "title": "HMAGIC: head movement and gaze input cascaded pointing",
            "abstract": "Augmentative and alternative communication tools allow people with severe motor disabilities to interact with computers. Two commonly used tools are video-based interfaces and eye trackers. Video-based interfaces map head movements captured by a camera to mouse pointer movements. Alternatively, eye trackers place the mouse pointer at the estimated position of the user's gaze. Eye tracking based interfaces have been shown to even outperform traditional mice in terms of speed, however the accuracy of current eye trackers is not enough for fine mouse pointer placement. In this paper we propose the Head Movement And Gaze Input Cascaded (HMAGIC) pointing technique that combines head movement and gaze-based inputs in a fast and accurate mouse-replacement interface. The interface initially places the pointer at the estimated gaze position and then the user makes fine adjustments with their head movements. We conducted a user experiment to compare HMAGIC with a mouse-replacement interface that uses only head movements to control the pointer. Experimental results indicate that HMAGIC is significantly faster than the head-only interface while still providing accurate mouse pointer positioning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2905689",
                    "name": "Andrew T. N. Kurauchi"
                },
                {
                    "authorId": "1802836",
                    "name": "Wenxin Feng"
                },
                {
                    "authorId": "23125434",
                    "name": "C. Morimoto"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                }
            ]
        },
        {
            "paperId": "9b01ee7a5222ec708b565f046bee64e377dcc610",
            "title": "Target reverse crossing: a selection method for camera-based mouse-replacement systems",
            "abstract": "We propose a selection method, \"target reverse crossing,\" for use with camera-based mouse-replacement for people with motion impairments. We assessed the method by comparing it to the selection mechanism \"dwell-time clicking,\" which is widely used by camera-based mouse-replacement systems. Our results show that target reverse crossing is more efficient than dwell-time clicking, while its one-time success accuracy is lower. We found that target directions have effects on the accuracy of reverse crossing. We also show that increasing the target size improves the performance of reverse crossing significantly, which provides future interface design implications for this selection method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802836",
                    "name": "Wenxin Feng"
                },
                {
                    "authorId": "2108633605",
                    "name": "Ming Chen"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                }
            ]
        }
    ]
}