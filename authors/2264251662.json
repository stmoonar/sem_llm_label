{
    "authorId": "2264251662",
    "papers": [
        {
            "paperId": "31048d9161dad19723b5c0733c7da0ea8b592e8f",
            "title": "Answer, Assemble, Ace: Understanding How Transformers Answer Multiple Choice Questions",
            "abstract": "Multiple-choice question answering (MCQA) is a key competence of performant transformer language models that is tested by mainstream benchmarks. However, recent evidence shows that models can have quite a range of performance, particularly when the task format is diversified slightly (such as by shuffling answer choice order). In this work we ask: how do successful models perform formatted MCQA? We employ vocabulary projection and activation patching methods to localize key hidden states that encode relevant information for predicting the correct answer. We find that prediction of a specific answer symbol is causally attributed to a single middle layer, and specifically its multi-head self-attention mechanism. We show that subsequent layers increase the probability of the predicted answer symbol in vocabulary space, and that this probability increase is associated with a sparse set of attention heads with unique roles. We additionally uncover differences in how different models adjust to alternative symbols. Finally, we demonstrate that a synthetic task can disentangle sources of model error to pinpoint when a model has learned formatted MCQA, and show that an inability to separate answer symbol tokens in vocabulary space is a property of models unable to perform formatted MCQA tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2279337376",
                    "name": "Sarah Wiegreffe"
                },
                {
                    "authorId": "3385516",
                    "name": "Oyvind Tafjord"
                },
                {
                    "authorId": "2083259",
                    "name": "Yonatan Belinkov"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                },
                {
                    "authorId": "2257349398",
                    "name": "Ashish Sabharwal"
                }
            ]
        },
        {
            "paperId": "3462eb6ba2b358ed95afca845e57f22cd0d79a8e",
            "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models",
            "abstract": "Today's most advanced multimodal models remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed models into open ones. As a result, the community is still missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key innovation is a novel, highly detailed image caption dataset collected entirely from human annotators using speech-based descriptions. To enable a wide array of user interactions, we also introduce a diverse dataset mixture for fine-tuning that includes in-the-wild Q&A and innovative 2D pointing data. The success of our approach relies on careful choices for the model architecture details, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets, all of which will be released. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human evaluation. We will be releasing all of our model weights, captioning and fine-tuning data, and source code in the near future. Select model weights, inference code, and demo are available at https://molmo.allenai.org.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1632916259",
                    "name": "Matt Deitke"
                },
                {
                    "authorId": "2323036630",
                    "name": "Christopher Clark"
                },
                {
                    "authorId": "2322783815",
                    "name": "Sangho Lee"
                },
                {
                    "authorId": "2322723039",
                    "name": "Rohun Tripathi"
                },
                {
                    "authorId": "2109409802",
                    "name": "Yue Yang"
                },
                {
                    "authorId": "2303571193",
                    "name": "Jae Sung Park"
                },
                {
                    "authorId": "2259938556",
                    "name": "Mohammadreza Salehi"
                },
                {
                    "authorId": "2314127007",
                    "name": "Niklas Muennighoff"
                },
                {
                    "authorId": "2315302377",
                    "name": "Kyle Lo"
                },
                {
                    "authorId": "2280666145",
                    "name": "Luca Soldaini"
                },
                {
                    "authorId": "2117718562",
                    "name": "Jiasen Lu"
                },
                {
                    "authorId": "2322845849",
                    "name": "Taira Anderson"
                },
                {
                    "authorId": "2312394842",
                    "name": "Erin Bransom"
                },
                {
                    "authorId": "2883417",
                    "name": "Kiana Ehsani"
                },
                {
                    "authorId": "2223118889",
                    "name": "Huong Ngo"
                },
                {
                    "authorId": "2322736233",
                    "name": "YenSung Chen"
                },
                {
                    "authorId": "2322833202",
                    "name": "Ajay Patel"
                },
                {
                    "authorId": "2064210",
                    "name": "Mark Yatskar"
                },
                {
                    "authorId": "2266396865",
                    "name": "Christopher Callison-Burch"
                },
                {
                    "authorId": "2065039588",
                    "name": "Andrew Head"
                },
                {
                    "authorId": "2008211857",
                    "name": "Rose Hendrix"
                },
                {
                    "authorId": "2995635",
                    "name": "F. Bastani"
                },
                {
                    "authorId": "1632920625",
                    "name": "Eli VanderBilt"
                },
                {
                    "authorId": "2267244197",
                    "name": "Nathan Lambert"
                },
                {
                    "authorId": "2212532904",
                    "name": "Yvonne Chou"
                },
                {
                    "authorId": "2322798621",
                    "name": "Arnavi Chheda"
                },
                {
                    "authorId": "2052201732",
                    "name": "Jenna Sparks"
                },
                {
                    "authorId": "46181683",
                    "name": "Sam Skjonsberg"
                },
                {
                    "authorId": "2286676842",
                    "name": "Michael Schmitz"
                },
                {
                    "authorId": "35429963",
                    "name": "Aaron Sarnat"
                },
                {
                    "authorId": "2322794013",
                    "name": "Byron Bischoff"
                },
                {
                    "authorId": "2158819969",
                    "name": "Pete Walsh"
                },
                {
                    "authorId": "145541350",
                    "name": "Christopher Newell"
                },
                {
                    "authorId": "2030980148",
                    "name": "Piper Wolters"
                },
                {
                    "authorId": "2269734955",
                    "name": "Tanmay Gupta"
                },
                {
                    "authorId": "2269736837",
                    "name": "Kuo-Hao Zeng"
                },
                {
                    "authorId": "2322805089",
                    "name": "Jon Borchardt"
                },
                {
                    "authorId": "3458736",
                    "name": "Dirk Groeneveld"
                },
                {
                    "authorId": "2282136556",
                    "name": "Jennifer Dumas"
                },
                {
                    "authorId": "2282136595",
                    "name": "Crystal Nam"
                },
                {
                    "authorId": "2312474",
                    "name": "Sophie Lebrecht"
                },
                {
                    "authorId": "2322805091",
                    "name": "Caitlin Wittlif"
                },
                {
                    "authorId": "3393851",
                    "name": "Carissa Schoenick"
                },
                {
                    "authorId": "2196005933",
                    "name": "Oscar Michel"
                },
                {
                    "authorId": "2262217505",
                    "name": "Ranjay Krishna"
                },
                {
                    "authorId": "20745881",
                    "name": "Luca Weihs"
                },
                {
                    "authorId": "2297646599",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                },
                {
                    "authorId": "2268775608",
                    "name": "Ross Girshick"
                },
                {
                    "authorId": "2257020375",
                    "name": "Ali Farhadi"
                },
                {
                    "authorId": "2684226",
                    "name": "Aniruddha Kembhavi"
                }
            ]
        },
        {
            "paperId": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
            "title": "OLMoE: Open Mixture-of-Experts Language Models",
            "abstract": "We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2314127007",
                    "name": "Niklas Muennighoff"
                },
                {
                    "authorId": "2280666145",
                    "name": "Luca Soldaini"
                },
                {
                    "authorId": "3458736",
                    "name": "Dirk Groeneveld"
                },
                {
                    "authorId": "2315302377",
                    "name": "Kyle Lo"
                },
                {
                    "authorId": "2146964035",
                    "name": "Jacob Daniel Morrison"
                },
                {
                    "authorId": "48872685",
                    "name": "Sewon Min"
                },
                {
                    "authorId": "2257597409",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2158819969",
                    "name": "Pete Walsh"
                },
                {
                    "authorId": "3385516",
                    "name": "Oyvind Tafjord"
                },
                {
                    "authorId": "2267244197",
                    "name": "Nathan Lambert"
                },
                {
                    "authorId": "2261456046",
                    "name": "Yuling Gu"
                },
                {
                    "authorId": "2319386691",
                    "name": "Shane Arora"
                },
                {
                    "authorId": "2166136235",
                    "name": "Akshita Bhagia"
                },
                {
                    "authorId": "2264248042",
                    "name": "Dustin Schwenk"
                },
                {
                    "authorId": "30051202",
                    "name": "David Wadden"
                },
                {
                    "authorId": "2127066887",
                    "name": "Alexander Wettig"
                },
                {
                    "authorId": "2257002316",
                    "name": "Binyuan Hui"
                },
                {
                    "authorId": "2288469507",
                    "name": "Tim Dettmers"
                },
                {
                    "authorId": "2111313627",
                    "name": "Douwe Kiela"
                },
                {
                    "authorId": "2257020375",
                    "name": "Ali Farhadi"
                },
                {
                    "authorId": "2290483619",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2303396379",
                    "name": "Pang Wei Koh"
                },
                {
                    "authorId": "2284268811",
                    "name": "Amanpreet Singh"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                }
            ]
        },
        {
            "paperId": "65b5c132a90b66d6b21f0672abbe9eba5f9c63cb",
            "title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback",
            "abstract": "Learning from preference feedback has emerged as an essential step for improving the generation quality and performance of modern language models (LMs). Despite its widespread use, the way preference-based learning is applied varies wildly, with differing data, learning algorithms, and evaluations used, making disentangling the impact of each aspect difficult. In this work, we identify four core aspects of preference-based learning: preference data, learning algorithm, reward model, and policy training prompts, systematically investigate the impact of these components on downstream model performance, and suggest a recipe for strong learning for preference feedback. Our findings indicate that all aspects are important for performance, with better preference data leading to the largest improvements, followed by the choice of learning algorithm, the use of improved reward models, and finally the use of additional unlabeled prompts for policy training. Notably, PPO outperforms DPO by up to 2.5% in math and 1.2% in general domains. High-quality preference data leads to improvements of up to 8% in instruction following and truthfulness. Despite significant gains of up to 5% in mathematical evaluation when scaling up reward models, we surprisingly observe marginal improvements in other categories. We publicly release the code used for training (https://github.com/hamishivi/EasyLM) and evaluating (https://github.com/allenai/open-instruct) our models, along with the models and datasets themselves (https://huggingface.co/collections/allenai/tulu-v25-suite-66676520fd578080e126f618).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056776606",
                    "name": "Hamish Ivison"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "2144174497",
                    "name": "Jiacheng Liu"
                },
                {
                    "authorId": "7806955",
                    "name": "Zeqiu Wu"
                },
                {
                    "authorId": "22330666",
                    "name": "Valentina Pyatkin"
                },
                {
                    "authorId": "2267244197",
                    "name": "Nathan Lambert"
                },
                {
                    "authorId": "2292425227",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2253903625",
                    "name": "Yejin Choi"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                }
            ]
        },
        {
            "paperId": "71303f1c03a56694d983b1c0230b432714243ba2",
            "title": "SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature",
            "abstract": "We present SciRIFF (Scientific Resource for Instruction-Following and Finetuning), a dataset of 137K instruction-following demonstrations for 54 tasks covering five essential scientific literature understanding capabilities: information extraction, summarization, question answering, claim verification, and classification. SciRIFF demonstrations are notable for their long input contexts, detailed task specifications, and complex structured outputs. While instruction-following resources are available in specific domains such as clinical medicine and chemistry, SciRIFF is the first dataset focused on extracting and synthesizing information from research literature across a wide range of scientific fields. To demonstrate the utility of SciRIFF, we develop a sample-efficient strategy to adapt a general instruction-following model for science by performing additional finetuning on a mix of general-domain and SciRIFF demonstrations. In evaluations on nine held-out scientific tasks, our model -- called SciTulu -- improves over a strong LLM baseline by 28.1% and 6.5% at the 7B and 70B scales respectively, while maintaining general instruction-following performance within 2% of the baseline. We are optimistic that SciRIFF will facilitate the development and evaluation of LLMs to help researchers navigate the ever-growing body of scientific literature. We release our dataset, model checkpoints, and data processing and evaluation code to enable further research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30051202",
                    "name": "David Wadden"
                },
                {
                    "authorId": "2259032293",
                    "name": "Kejian Shi"
                },
                {
                    "authorId": "2146964035",
                    "name": "Jacob Daniel Morrison"
                },
                {
                    "authorId": "23175870",
                    "name": "Aakanksha Naik"
                },
                {
                    "authorId": "14918655",
                    "name": "Shruti Singh"
                },
                {
                    "authorId": "2305813534",
                    "name": "Nitzan Barzilay"
                },
                {
                    "authorId": "2285152499",
                    "name": "Kyle Lo"
                },
                {
                    "authorId": "2266841339",
                    "name": "Tom Hope"
                },
                {
                    "authorId": "2280666145",
                    "name": "Luca Soldaini"
                },
                {
                    "authorId": "2306748517",
                    "name": "Shannon Zejiang Shen"
                },
                {
                    "authorId": "2266840873",
                    "name": "Doug Downey"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                },
                {
                    "authorId": "2527954",
                    "name": "Arman Cohan"
                }
            ]
        },
        {
            "paperId": "8b89036b5136b081867fbec089801b2bd4ab0685",
            "title": "Decoding-Time Language Model Alignment with Multiple Objectives",
            "abstract": "Aligning language models (LMs) to human preferences has emerged as a critical pursuit, enabling these models to better serve diverse user needs. Existing methods primarily focus on optimizing LMs for a single reward function, limiting their adaptability to varied objectives. Here, we propose $\\textbf{multi-objective decoding (MOD)}$, a decoding-time algorithm that outputs the next token from a linear combination of predictions of all base models, for any given weightings over different objectives. We exploit a common form among a family of $f$-divergence regularized alignment approaches (such as PPO, DPO, and their variants) to identify a closed-form solution by Legendre transform, and derive an efficient decoding strategy. Theoretically, we show why existing approaches can be sub-optimal even in natural settings and obtain optimality guarantees for our method. Empirical results demonstrate the effectiveness of the algorithm. For example, compared to a parameter-merging baseline, MOD achieves 12.8% overall reward improvement when equally optimizing towards $3$ objectives. Moreover, we experiment with MOD on combining three fully-finetuned LLMs of different model sizes, each aimed at different objectives such as safety, coding, and general user preference. Unlike traditional methods that require careful curation of a mixture of datasets to achieve comprehensive improvement, we can quickly experiment with preference weightings using MOD to find the best combination of models. Our best combination reduces toxicity on Toxigen to nearly 0% and achieves 7.9--33.3% improvement across other three metrics ($\\textit{i.e.}$, Codex@1, GSM-COT, BBH-COT).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2308470202",
                    "name": "Ruizhe Shi"
                },
                {
                    "authorId": "1556022677",
                    "name": "Yifang Chen"
                },
                {
                    "authorId": "2112209725",
                    "name": "Yushi Hu"
                },
                {
                    "authorId": "2261363520",
                    "name": "Alisa Liu"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                },
                {
                    "authorId": "2261399966",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2273538448",
                    "name": "S. Du"
                }
            ]
        },
        {
            "paperId": "8e9088c102b3714ae4e5cac7ced93a59804bfc7c",
            "title": "RewardBench: Evaluating Reward Models for Language Modeling",
            "abstract": "Reward models (RMs) are at the crux of successfully using RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. Resources for reward model training and understanding are sparse in the nascent open-source community around them. To enhance scientific understanding of reward models, we present RewardBench, a benchmark dataset and code-base for evaluation. The RewardBench dataset is a collection of prompt-chosen-rejected trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We create specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be preferred to another. On the RewardBench leaderboard, we evaluate reward models trained with a variety of methods, such as the direct MLE training of classifiers and the implicit reward modeling of Direct Preference Optimization (DPO). We present many findings on propensity for refusals, reasoning limitations, and instruction following shortcomings of various reward models towards a better understanding of the RLHF process.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267244197",
                    "name": "Nathan Lambert"
                },
                {
                    "authorId": "22330666",
                    "name": "Valentina Pyatkin"
                },
                {
                    "authorId": "2146964035",
                    "name": "Jacob Daniel Morrison"
                },
                {
                    "authorId": "13614871",
                    "name": "Lester James Validad Miranda"
                },
                {
                    "authorId": "51583409",
                    "name": "Bill Yuchen Lin"
                },
                {
                    "authorId": "37619618",
                    "name": "Khyathi Raghavi Chandu"
                },
                {
                    "authorId": "46217681",
                    "name": "Nouha Dziri"
                },
                {
                    "authorId": "2282203839",
                    "name": "Sachin Kumar"
                },
                {
                    "authorId": "2297848637",
                    "name": "Tom Zick"
                },
                {
                    "authorId": "2257385142",
                    "name": "Yejin Choi"
                },
                {
                    "authorId": "2292425227",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                }
            ]
        },
        {
            "paperId": "944b983ee059503c53afef772052d065d662527c",
            "title": "Set the Clock: Temporal Alignment of Pretrained Language Models",
            "abstract": "Language models (LMs) are trained on web text originating from many points in time and, in general, without any explicit temporal grounding. This work investigates the temporal chaos of pretrained LMs and explores various methods to align their internal knowledge to a target time, which we call\"temporal alignment.\"To do this, we first automatically construct a dataset containing 20K time-sensitive questions and their answers for each year from 2000 to 2023. Based on this dataset, we empirically show that pretrained LMs (e.g., LLaMa2), despite having a recent pretraining cutoff (e.g., 2022), mostly answer questions using earlier knowledge (e.g., in 2019). We then develop several methods, from prompting to finetuning, to align LMs to use their most recent knowledge when answering questions, and investigate various factors in this alignment. Our experiments demonstrate that aligning LLaMa2 to the year 2022 can enhance its performance by up to 62% according to that year's answers. This improvement occurs even without explicitly mentioning time information, indicating the possibility of aligning models' internal sense of time after pretraining. Finally, we find that alignment to a historical time is also possible, with up to 2.8$\\times$ the performance of the unaligned LM in 2010 if finetuning models to that year. These findings hint at the sophistication of LMs' internal knowledge organization and the necessity of tuning them properly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280917335",
                    "name": "Bowen Zhao"
                },
                {
                    "authorId": "2210989402",
                    "name": "Zander Brumbaugh"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                },
                {
                    "authorId": "2268796196",
                    "name": "Noah A. Smith"
                }
            ]
        },
        {
            "paperId": "a10512fed043320bb924c644933eff93a9a56eca",
            "title": "Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning",
            "abstract": "Language agents perform complex tasks by using tools to execute each step precisely. However, most existing agents are based on proprietary models or designed to target specific tasks, such as mathematics or multi-hop question answering. We introduce Husky, a holistic, open-source language agent that learns to reason over a unified action space to address a diverse set of complex tasks involving numerical, tabular, and knowledge-based reasoning. Husky iterates between two stages: 1) generating the next action to take towards solving a given task and 2) executing the action using expert models and updating the current solution state. We identify a thorough ontology of actions for addressing complex tasks and curate high-quality data to train expert models for executing these actions. Our experiments show that Husky outperforms prior language agents across 14 evaluation datasets. Moreover, we introduce HuskyQA, a new evaluation set which stress tests language agents for mixed-tool reasoning, with a focus on retrieving missing knowledge and performing numerical reasoning. Despite using 7B models, Husky matches or even exceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of our holistic approach in addressing complex reasoning problems. Our code and models are available at https://github.com/agent-husky/Husky-v1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117060176",
                    "name": "Joongwon Kim"
                },
                {
                    "authorId": "2132497930",
                    "name": "Bhargavi Paranjape"
                },
                {
                    "authorId": "2236429",
                    "name": "Tushar Khot"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                }
            ]
        },
        {
            "paperId": "a1fa960fdfcc08510d348f7c66028f3d91b497f8",
            "title": "The Art of Saying No: Contextual Noncompliance in Language Models",
            "abstract": "Chat-based language models are designed to be helpful, yet they should not comply with every user request. While most existing work primarily focuses on refusal of\"unsafe\"queries, we posit that the scope of noncompliance should be broadened. We introduce a comprehensive taxonomy of contextual noncompliance describing when and how models should not comply with user requests. Our taxonomy spans a wide range of categories including incomplete, unsupported, indeterminate, and humanizing requests (in addition to unsafe requests). To test noncompliance capabilities of language models, we use this taxonomy to develop a new evaluation suite of 1000 noncompliance prompts. We find that most existing models show significantly high compliance rates in certain previously understudied categories with models like GPT-4 incorrectly complying with as many as 30% of requests. To address these gaps, we explore different training strategies using a synthetically-generated training set of requests and expected noncompliant responses. Our experiments demonstrate that while direct finetuning of instruction-tuned models can lead to both over-refusal and a decline in general capabilities, using parameter efficient methods like low rank adapters helps to strike a good balance between appropriate noncompliance and other capabilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223951216",
                    "name": "Faeze Brahman"
                },
                {
                    "authorId": "2308339428",
                    "name": "Sachin Kumar"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2697425",
                    "name": "Pradeep Dasigi"
                },
                {
                    "authorId": "22330666",
                    "name": "Valentina Pyatkin"
                },
                {
                    "authorId": "3023068",
                    "name": "Abhilasha Ravichander"
                },
                {
                    "authorId": "2279337376",
                    "name": "Sarah Wiegreffe"
                },
                {
                    "authorId": "46217681",
                    "name": "Nouha Dziri"
                },
                {
                    "authorId": "2302810573",
                    "name": "K. Chandu"
                },
                {
                    "authorId": "2689239",
                    "name": "Jack Hessel"
                },
                {
                    "authorId": "2258958466",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2292425227",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2259707400",
                    "name": "Yejin Choi"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                }
            ]
        }
    ]
}