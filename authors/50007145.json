{
    "authorId": "50007145",
    "papers": [
        {
            "paperId": "172e75ba47cd4f50ef62f63a4251fdbb11b16525",
            "title": "Knowledge-Grounded Natural Language Recommendation Explanation",
            "abstract": "Explanations accompanying a recommendation can assist users in understanding the decision made by recommendation systems, which in turn increases a user\u2019s confidence and trust in the system. Recently, research has focused on generating natural language explanations in a human-readable format. Thus far, the proposed approaches leverage item reviews written by users, which are often subjective, sparse in language, and unable to account for new items that have not been purchased or reviewed before. Instead, we aim to generate fact-grounded recommendation explanations that are objectively described with item features while implicitly considering a user\u2019s preferences, based on the user\u2019s purchase history. To achieve this, we propose a knowledge graph (KG) approach to natural language explainable recommendation. Our approach draws on user-item features through a novel collaborative filtering-based KG representation to produce fact-grounded, personalized explanations, while jointly learning user-item representations for recommendation scoring. Experimental results show that our approach consistently outperforms previous state-of-the-art models on natural language explainable recommendation metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1441128975",
                    "name": "Anthony Colas"
                },
                {
                    "authorId": "50007145",
                    "name": "J. Araki"
                },
                {
                    "authorId": "3128828",
                    "name": "Zhengyu Zhou"
                },
                {
                    "authorId": "2510385",
                    "name": "Bingqing Wang"
                },
                {
                    "authorId": "143989610",
                    "name": "Z. Feng"
                }
            ]
        },
        {
            "paperId": "2a817dd7fc38ec66fa03b509c89ddc1b9a9fb59f",
            "title": "CoAug: Combining Augmentation of Labels and Labelling Rules",
            "abstract": "Collecting labeled data for Named Entity Recognition (NER) tasks is challenging due to the high cost of manual annotations. Instead, researchers have proposed few-shot self-training and rule-augmentation techniques to minimize the reliance on large datasets. However, inductive biases and restricted logical language lexicon, respectively, can limit the ability of these models to perform well. In this work, we pro-pose CoAug , a co-augmentation framework that allows us to improve few-shot models and rule-augmentation models by bootstrapping predictions from each model. By leveraging rules and neural model predictions to train our models, we complement the benefits of each and achieve the best of both worlds. In our experiments, we show that our best CoAug model can outperform strong weak-supervision-based NER models at least by 6.5 F1 points on the BC5CDR, NCBI-Disease, WikiGold, and CoNLL-2003 datasets. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26339118",
                    "name": "Rakesh R Menon"
                },
                {
                    "authorId": "2510385",
                    "name": "Bingqing Wang"
                },
                {
                    "authorId": "50007145",
                    "name": "J. Araki"
                },
                {
                    "authorId": "3128828",
                    "name": "Zhengyu Zhou"
                },
                {
                    "authorId": "143989610",
                    "name": "Z. Feng"
                },
                {
                    "authorId": "145645852",
                    "name": "Liu Ren"
                }
            ]
        },
        {
            "paperId": "c5a0a92fb521c7d1399cf26e63a6ffb806d1b291",
            "title": "A Textual Dataset for Situated Proactive Response Selection",
            "abstract": "Recent data-driven conversational models are able to return fluent, consistent, and informative responses to many kinds of requests and utterances in task-oriented scenarios.However, these responses are typically limited to just the immediate local topic instead of being wider-ranging and proactively taking the conversation further, for example making suggestions to help customers achieve their goals. This inadequacy reflects a lack of understanding of the interlocutor\u2019s situation and implicit goal. To address the problem, we introduce a task of proactive response selection based on situational information. We present a manually-curated dataset of 1.7k English conversation examples that include situational background information plus for each conversation a set of responses, only some of which are acceptable in the situation. A responsive and informed conversation system should select the appropriate responses and avoid inappropriate ones; doing so demonstrates the ability to adequately understand the initiating request and situation. Our benchmark experiments show that this is not an easy task even for strong neural models, offering opportunities for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145671279",
                    "name": "Naoki Otani"
                },
                {
                    "authorId": "50007145",
                    "name": "J. Araki"
                },
                {
                    "authorId": "2109893608",
                    "name": "Hyeongsik Kim"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                }
            ]
        },
        {
            "paperId": "ef91c31d8aab9fe95fec29149e2fe4568ab2fb32",
            "title": "SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains",
            "abstract": "Prompting pre-trained language models leads to promising results across natural language processing tasks but is less effective when applied in low-resource domains, due to the domain gap between the pre-training data and the downstream task. In this work, we bridge this gap with a novel and lightweight prompting methodology called SwitchPrompt for the adaptation of language models trained on datasets from the general domain to diverse low-resource domains. Using domain-specific keywords with a trainable gated prompt, SwitchPrompt offers domain-oriented prompting, that is, effective guidance on the target domains for general-domain language models. Our few-shot experiments on three text classification benchmarks demonstrate the efficacy of the general-domain pre-trained language models when used with SwitchPrompt. They often even outperform their domain-specific counterparts trained with baseline state-of-the-art prompting methods by up to 10.7% performance increase in accuracy. This result indicates that SwitchPrompt effectively reduces the need for domain-specific language model pre-training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120873790",
                    "name": "Koustava Goswami"
                },
                {
                    "authorId": "47665464",
                    "name": "Lukas Lange"
                },
                {
                    "authorId": "50007145",
                    "name": "J. Araki"
                },
                {
                    "authorId": "145793834",
                    "name": "Heike Adel"
                }
            ]
        },
        {
            "paperId": "87126a964ed14d0d2207747fc732b197e2fc9493",
            "title": "Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer",
            "abstract": "Systems for knowledge-intensive tasks such as open-domain question answering (QA) usually consist of two stages: efficient retrieval of relevant documents from a large corpus and detailed reading of the selected documents. This is usually done through two separate models, a retriever that encodes the query and finds nearest neighbors, and a reader based on Transformers. These two components are usually modeled separately, which necessitates a cumbersome implementation and is awkward to optimize in an end-to-end fashion. In this paper, we revisit this design and eschew the separate architecture and training in favor of a single Transformer that performs retrieval as attention (RAA), and end-to-end training solely based on supervision from the end QA task. We demonstrate for the first time that an end-to-end trained single Transformer can achieve both competitive retrieval and QA performance on in-domain datasets, matching or even slightly outperforming state-of-the-art dense retrievers and readers. Moreover, end-to-end adaptation of our model significantly boosts its performance on out-of-domain datasets in both supervised and unsupervised settings, making our model a simple and adaptable end-to-end solution for knowledge-intensive tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2669515",
                    "name": "Zhengbao Jiang"
                },
                {
                    "authorId": "49715441",
                    "name": "Luyu Gao"
                },
                {
                    "authorId": "50007145",
                    "name": "J. Araki"
                },
                {
                    "authorId": "47929135",
                    "name": "Haibo Ding"
                },
                {
                    "authorId": "1390877035",
                    "name": "Zhiruo Wang"
                },
                {
                    "authorId": "144987107",
                    "name": "Jamie Callan"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "eac5e5ef2bb1d158645ee8ae8f3e167767316b46",
            "title": "Understanding and Improving Zero-shot Multi-hop Reasoning in Generative Question Answering",
            "abstract": "Generative question answering (QA) models generate answers to questions either solely based on the parameters of the model (the closed-book setting) or additionally retrieving relevant evidence (the open-book setting). Generative QA models can answer some relatively complex questions, but the mechanism through which they do so is still poorly understood. We perform several studies aimed at better understanding the multi-hop reasoning capabilities of generative QA models. First, we decompose multi-hop questions into multiple corresponding single-hop questions, and find marked inconsistency in QA models\u2019 answers on these pairs of ostensibly identical question chains. Second, we find that models lack zero-shot multi-hop reasoning ability: when trained only on single-hop questions, models generalize poorly to multi-hop questions. Finally, we demonstrate that it is possible to improve models\u2019 zero-shot multi-hop reasoning capacity through two methods that approximate real multi-hop natural language (NL) questions by training on either concatenation of single-hop questions or logical forms (SPARQL). In sum, these results demonstrate that multi-hop reasoning does not emerge naturally in generative QA models, but can be encouraged by advances in training or modeling techniques. Code is available at https://github.com/jzbjyb/multihop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2669515",
                    "name": "Zhengbao Jiang"
                },
                {
                    "authorId": "50007145",
                    "name": "J. Araki"
                },
                {
                    "authorId": "47929135",
                    "name": "Haibo Ding"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "626576bcda2404c328c8848331e0b34f64394571",
            "title": "Explicitly Capturing Relations between Entity Mentions via Graph Neural Networks for Domain-specific Named Entity Recognition",
            "abstract": "Named entity recognition (NER) is well studied for the general domain, and recent systems have achieved human-level performance for identifying common entity types. However, the NER performance is still moderate for specialized domains that tend to feature complicated contexts and jargonistic entity types. To address these challenges, we propose explicitly connecting entity mentions based on both global coreference relations and local dependency relations for building better entity mention representations. In our experiments, we incorporate entity mention relations by Graph Neural Networks and show that our system noticeably improves the NER performance on two datasets from different domains. We further show that the proposed lightweight system can effectively elevate the NER performance to a higher level even when only a tiny amount of labeled data is available, which is desirable for domain-specific NER.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2901524",
                    "name": "Pei Chen"
                },
                {
                    "authorId": "47929135",
                    "name": "Haibo Ding"
                },
                {
                    "authorId": "50007145",
                    "name": "J. Araki"
                },
                {
                    "authorId": "1455107421",
                    "name": "Ruihong Huang"
                }
            ]
        },
        {
            "paperId": "33422275fbb9958f55419620697faf531482699b",
            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
            "abstract": "Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \u201cHow can we know when language models know, with confidence, the answer to a particular query?\u201d We examine this question from the point of view of calibration, the property of a probabilistic model\u2019s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models\u2014T5, BART, and GPT-2\u2014and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2669515",
                    "name": "Zhengbao Jiang"
                },
                {
                    "authorId": "50007145",
                    "name": "J. Araki"
                },
                {
                    "authorId": "47929135",
                    "name": "Haibo Ding"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "708dcd8456426cd609c89a86344e0007c04c80bf",
            "title": "X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models",
            "abstract": "Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as \"Punta Cana is located in _.\" However, while knowledge is both written and queried in many languages, studies on LMs' factual representation ability have almost invariably been performed on English. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for 23 typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have been released at https://x-factr.github.io.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2669515",
                    "name": "Zhengbao Jiang"
                },
                {
                    "authorId": "49513989",
                    "name": "Antonios Anastasopoulos"
                },
                {
                    "authorId": "50007145",
                    "name": "J. Araki"
                },
                {
                    "authorId": "47929135",
                    "name": "Haibo Ding"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "7618c65685c98fa88526555ae3f62cd5645066ad",
            "title": "Learning Relation Entailment with Structured and Textual Information",
            "abstract": "Relations among words and entities are important for semantic understanding of text, but previous work has largely not considered relations between relations , or meta-relations . In this paper, we speci\ufb01cally examine relation entailment , where the existence of one relation can entail the existence of another relation. Relation entailment allows us to construct relation hierarchies, enabling applications in representation learning, question answering, relation extraction, and summarization. To this end, we formally de\ufb01ne the new task of predicting relation entailment and construct a dataset by expanding the existing Wikidata relation hierarchy without expensive human intervention. We propose several methods that incorporate both structured and textual information to represent relations for this task. Experiments and analysis demonstrate that this task is challenging, and we provide insights into task characteristics that may form a basis for future work. The dataset and code have been released at https://github.com/jzbjyb/RelEnt.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2669515",
                    "name": "Zhengbao Jiang"
                },
                {
                    "authorId": "50007145",
                    "name": "J. Araki"
                },
                {
                    "authorId": "15121583",
                    "name": "Donghan Yu"
                },
                {
                    "authorId": "46752970",
                    "name": "Ruohong Zhang"
                },
                {
                    "authorId": "1485330793",
                    "name": "Wei Xu"
                },
                {
                    "authorId": "35729970",
                    "name": "Yiming Yang"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ]
        }
    ]
}