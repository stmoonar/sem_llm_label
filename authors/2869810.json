{
    "authorId": "2869810",
    "papers": [
        {
            "paperId": "12b20c26a718a2aa2457b3908f107c0e74637a45",
            "title": "Continual Learning on Dynamic Graphs via Parameter Isolation",
            "abstract": "Many real-world graph learning tasks require handling dynamic graphs where new nodes and edges emerge. Dynamic graph learning methods commonly suffer from the catastrophic forgetting problem, where knowledge learned for previous graphs is overwritten by updates for new graphs. To alleviate the problem, continual graph learning methods are proposed. However, existing continual graph learning methods aim to learn new patterns and maintain old ones with the same set of parameters of fixed size, and thus face a fundamental tradeoff between both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN) for continual learning on dynamic graphs that circumvents the tradeoff via parameter isolation and expansion. Our motivation lies in that different parameters contribute to learning different graph patterns. Based on the idea, we expand model parameters to continually learn emerging graph patterns. Meanwhile, to effectively preserve knowledge for unaffected patterns, we find parameters that correspond to them via optimization and freeze them to prevent them from being rewritten. Experiments on eight real-world datasets corroborate the effectiveness of PI-GNN compared to state-of-the-art baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115512664",
                    "name": "Peiyan Zhang"
                },
                {
                    "authorId": "2109341751",
                    "name": "Yuchen Yan"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "3210262",
                    "name": "Senzhang Wang"
                },
                {
                    "authorId": "2110972816",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "2072633489",
                    "name": "Guojie Song"
                },
                {
                    "authorId": "2118021616",
                    "name": "Sunghun Kim"
                }
            ]
        },
        {
            "paperId": "3e2708cca97f36bfc70cf1c907bda8f3755a6999",
            "title": "Longtriever: a Pre-trained Long Text Encoder for Dense Document Retrieval",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9008621",
                    "name": "Junhan Yang"
                },
                {
                    "authorId": "2273539690",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "2273404944",
                    "name": "Guangzhong Sun"
                },
                {
                    "authorId": "2257799839",
                    "name": "Xing Xie"
                }
            ]
        },
        {
            "paperId": "49e0cc79f002ccb85dcb58796220c3fe3487e348",
            "title": "Beyond the Overlapping Users: Cross-Domain Recommendation via Adaptive Anchor Link Learning",
            "abstract": "Cross-Domain Recommendation (CDR) is capable of incorporating auxiliary information from multiple domains to advance recommendation performance. Conventional CDR methods primarily rely on overlapping users, whereby knowledge is conveyed between the source and target identities belonging to the same natural person. However, such a heuristic assumption is not universally applicable due to an individual may exhibit distinct or even conflicting preferences in different domains, leading to potential noises. In this paper, we view the anchor links between users of various domains as the learnable parameters to learn the task-relevant cross-domain correlations. A novel optimal transport based model ALCDR is further proposed to precisely infer the anchor links and deeply aggregate collaborative signals from the perspectives of intra-domain and inter-domain. Our proposal is extensively evaluated over real-world datasets, and experimental results demonstrate its superiority.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109917685",
                    "name": "Yi Zhao"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "113351669",
                    "name": "Jiquan Peng"
                },
                {
                    "authorId": "1885266123",
                    "name": "Xiaohan Fang"
                },
                {
                    "authorId": "1939569",
                    "name": "Feiran Huang"
                },
                {
                    "authorId": "3210262",
                    "name": "Senzhang Wang"
                },
                {
                    "authorId": "2110972816",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "2869419",
                    "name": "Jibing Gong"
                }
            ]
        },
        {
            "paperId": "685e3d87032d3530f639c9d46bab7e75307f357c",
            "title": "TransGNN: Harnessing the Collaborative Power of Transformers and Graph Neural Networks for Recommender Systems",
            "abstract": "Graph Neural Networks (GNNs) have emerged as promising solutions for collaborative filtering (CF) through the modeling of user-item interaction graphs. The nucleus of existing GNN-based recommender systems involves recursive message passing along user-item interaction edges to refine encoded embeddings. Despite their demonstrated effectiveness, current GNN-based methods encounter challenges of limited receptive fields and the presence of noisy\"interest-irrelevant\"connections. In contrast, Transformer-based methods excel in aggregating information adaptively and globally. Nevertheless, their application to large-scale interaction graphs is hindered by inherent complexities and challenges in capturing intricate, entangled structural information. In this paper, we propose TransGNN, a novel model that integrates Transformer and GNN layers in an alternating fashion to mutually enhance their capabilities. Specifically, TransGNN leverages Transformer layers to broaden the receptive field and disentangle information aggregation from edges, which aggregates information from more relevant nodes, thereby enhancing the message passing of GNNs. Additionally, to capture graph structure information effectively, positional encoding is meticulously designed and integrated into GNN layers to encode such structural knowledge into node attributes, thus enhancing the Transformer's performance on graphs. Efficiency considerations are also alleviated by proposing the sampling of the most relevant nodes for the Transformer, along with two efficient sample update strategies to reduce complexity. Furthermore, theoretical analysis demonstrates that TransGNN offers increased expressiveness compared to GNNs, with only a marginal increase in linear complexity. Extensive experiments on five public datasets validate the effectiveness and efficiency of TransGNN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115512664",
                    "name": "Peiyan Zhang"
                },
                {
                    "authorId": "2109341751",
                    "name": "Yuchen Yan"
                },
                {
                    "authorId": "2302364906",
                    "name": "Xi Zhang"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "3210262",
                    "name": "Senzhang Wang"
                },
                {
                    "authorId": "2261394511",
                    "name": "Feiran Huang"
                },
                {
                    "authorId": "2118021616",
                    "name": "Sunghun Kim"
                }
            ]
        },
        {
            "paperId": "89faee594a314118dcc51dc114c8e99eeae98c51",
            "title": "PASS: Personalized Advertiser-aware Sponsored Search",
            "abstract": "The nucleus of online sponsored search systems lies in measuring the relevance between the search intents of users and the advertising purposes of advertisers. Existing conventional doublet-based (query-keyword) relevance models solely rely on short queries and keywords to uncover such intents, which ignore the diverse and personalized preferences of participants (i.e., users and advertisers), resulting in undesirable advertising performance. In this paper, we investigate the novel problem of Personalized A dvertiser-aware Sponsored Search (PASS). Our motivation lies in incorporating the portraits of users and advertisers into relevance models to facilitate the modeling of intrinsic search intents and advertising purposes, leading to a quadruple-based (i.e., user-query-keyword-advertiser) task. Various types of historical behaviors are explored in the format of hypergraphs to provide abundant signals on identifying the preferences of participants. A novel heterogeneous textual hypergraph transformer is further proposed to deeply fuse the textual semantics and the high-order hypergraph topology. Our proposal is extensively evaluated over real industry datasets, and experimental results demonstrate its superiority.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2162043833",
                    "name": "Zhoujin Tian"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "2188068349",
                    "name": "Zhiqiang Zuo"
                },
                {
                    "authorId": "2152090164",
                    "name": "Zengxuan Wen"
                },
                {
                    "authorId": "49755259",
                    "name": "Lichao Sun"
                },
                {
                    "authorId": null,
                    "name": "Xinyue Hu"
                },
                {
                    "authorId": "2108540565",
                    "name": "Wen Zhang"
                },
                {
                    "authorId": "2146285313",
                    "name": "Haizhen Huang"
                },
                {
                    "authorId": "3210262",
                    "name": "Senzhang Wang"
                },
                {
                    "authorId": "2066621592",
                    "name": "Weiwei Deng"
                },
                {
                    "authorId": "2110972816",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "2181348993",
                    "name": "Qi Zhang"
                }
            ]
        },
        {
            "paperId": "a53e247c282f231208dc9f5a39c96f2ca9ac5651",
            "title": "xGCN: An Extreme Graph Convolutional Network for Large-scale Social Link Prediction",
            "abstract": "Graph neural networks (GNNs) have seen widespread usage across multiple real-world applications, yet in transductive learning, they still face challenges in accuracy, efficiency, and scalability, due to the extensive number of trainable parameters in the embedding table and the paradigm of stacking neighborhood aggregations. This paper presents a novel model called xGCN for large-scale network embedding, which is a practical solution for link predictions. xGCN addresses these issues by encoding graph-structure data in an extreme convolutional manner, and has the potential to push the performance of network embedding-based link predictions to a new record. Specifically, instead of assigning each node with a directly learnable embedding vector, xGCN regards node embeddings as static features. It uses a propagation operation to smooth node embeddings and relies on a Refinement neural Network (RefNet) to transform the coarse embeddings derived from the unsupervised propagation into new ones that optimize a training objective. The output of RefNet, which are well-refined embeddings, will replace the original node embeddings. This process is repeated iteratively until the model converges to a satisfying status. Experiments on three social network datasets with link prediction tasks show that xGCN not only achieves the best accuracy compared with a series of competitive baselines but also is highly efficient and scalable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2182111071",
                    "name": "Xiran Song"
                },
                {
                    "authorId": "2813328",
                    "name": "Jianxun Lian"
                },
                {
                    "authorId": "145948697",
                    "name": "H. Huang"
                },
                {
                    "authorId": "1820930645",
                    "name": "Zihan Luo"
                },
                {
                    "authorId": "101829183",
                    "name": "Wei Zhou"
                },
                {
                    "authorId": "2215474182",
                    "name": "Xue Lin"
                },
                {
                    "authorId": "21862228",
                    "name": "Mingqi Wu"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "2110972816",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "2109791075",
                    "name": "Hai Jin"
                }
            ]
        },
        {
            "paperId": "aa1e0de459abc6eb17bfcc6f8c50bee59b107ca0",
            "title": "Multi-Grained Topological Pre-Training of Language Models in Sponsored Search",
            "abstract": "Relevance models measure the semantic closeness between queries and the candidate ads, widely recognized as the nucleus of sponsored search systems. Conventional relevance models solely rely on the textual data within the queries and ads, whose performance is hindered by the scarce semantic information in these short texts. Recently, user behavior graphs have been incorporated to provide complementary information beyond pure textual semantics.Despite the promising performance, behavior-enhanced models suffer from exhausting resource costs due to the extra computations introduced by explicit topological aggregations. In this paper, we propose a novel Multi-Grained Topological Pre-Training paradigm, MGTLM, to teach language models to understand multi-grained topological information in behavior graphs, which contributes to eliminating explicit graph aggregations and avoiding information loss. Extensive experimental results over online and offline settings demonstrate the superiority of our proposal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2162043833",
                    "name": "Zhoujin Tian"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "2188068349",
                    "name": "Zhiqiang Zuo"
                },
                {
                    "authorId": "2152090164",
                    "name": "Zengxuan Wen"
                },
                {
                    "authorId": "2188127948",
                    "name": "Xinyue Hu"
                },
                {
                    "authorId": "2149248164",
                    "name": "Xiao Han"
                },
                {
                    "authorId": "2146285313",
                    "name": "Haizhen Huang"
                },
                {
                    "authorId": "3210262",
                    "name": "Senzhang Wang"
                },
                {
                    "authorId": "2066621592",
                    "name": "Weiwei Deng"
                },
                {
                    "authorId": "2110972816",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "2181348993",
                    "name": "Qi Zhang"
                }
            ]
        },
        {
            "paperId": "cb65248507e1b95f833e9e86c955137393a9c19a",
            "title": "Generative Sentiment Transfer via Adaptive Masking",
            "abstract": "Sentiment transfer aims at revising the input text to satisfy a given sentiment polarity while retaining the original semantic content. The nucleus of sentiment transfer lies in precisely separating the sentiment information from the content information. Existing explicit approaches generally identify and mask sentiment tokens simply based on prior linguistic knowledge and manually-defined rules, leading to low generality and undesirable transfer performance. In this paper, we view the positions to be masked as the learnable parameters, and further propose a novel AM-ST model to learn adaptive task-relevant masks based on the attention mechanism. Moreover, a sentiment-aware masked language model is further proposed to fill in the blanks in the masked positions by incorporating both context and sentiment polarity to capture the multi-grained semantics comprehensively. AM-ST is thoroughly evaluated on two popular datasets, and the experimental results demonstrate the superiority of our proposal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149183002",
                    "name": "Ying Xie"
                },
                {
                    "authorId": "2111063150",
                    "name": "Jie Xu"
                },
                {
                    "authorId": "3461864",
                    "name": "Liqiang Qiao"
                },
                {
                    "authorId": "2118113473",
                    "name": "Yun Liu"
                },
                {
                    "authorId": "143857288",
                    "name": "Fei Huang"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                }
            ]
        },
        {
            "paperId": "d874af0891e0ed816d3db3379a700a2a42f1be08",
            "title": "ConvFormer: Revisiting Transformer for Sequential User Modeling",
            "abstract": "Sequential user modeling, a critical task in personalized recommender systems, focuses on predicting the next item a user would prefer, requiring a deep understanding of user behavior sequences. Despite the remarkable success of Transformer-based models across various domains, their full potential in comprehending user behavior remains untapped. In this paper, we re-examine Transformer-like architectures aiming to advance state-of-the-art performance. We start by revisiting the core building blocks of Transformer-based methods, analyzing the effectiveness of the item-to-item mechanism within the context of sequential user modeling. After conducting a thorough experimental analysis, we identify three essential criteria for devising efficient sequential user models, which we hope will serve as practical guidelines to inspire and shape future designs. Following this, we introduce ConvFormer, a simple but powerful modification to the Transformer architecture that meets these criteria, yielding state-of-the-art results. Additionally, we present an acceleration technique to minimize the complexity associated with processing extremely long sequences. Experiments on four public datasets showcase ConvFormer's superiority and confirm the validity of our proposed criteria.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256769065",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2813328",
                    "name": "Jianxun Lian"
                },
                {
                    "authorId": "31395194",
                    "name": "M. Wu"
                },
                {
                    "authorId": "2157147028",
                    "name": "Haoxuan Li"
                },
                {
                    "authorId": "2047532575",
                    "name": "Jiajun Fan"
                },
                {
                    "authorId": "66717145",
                    "name": "Wanyue Xu"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "2110972816",
                    "name": "Xing Xie"
                }
            ]
        },
        {
            "paperId": "da05a0387d68d83814c6597d9dfdb30b421fd5ba",
            "title": "Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models",
            "abstract": "Machine learning has demonstrated remarkable performance over finite datasets, yet whether the scores over the fixed benchmarks can sufficiently indicate the model's performance in the real world is still in discussion. In reality, an ideal robust model will probably behave similarly to the oracle (e.g., the human users), thus a good evaluation protocol is probably to evaluate the models' behaviors in comparison to the oracle. In this paper, we introduce a new robustness measurement that directly measures the image classification model's performance compared with a surrogate oracle (i.e., a foundation model). Besides, we design a simple method that can accomplish the evaluation beyond the scope of the benchmarks. Our method extends the image datasets with new samples that are sufficiently perturbed to be distinct from the ones in the original sets, but are still bounded within the same image-label structure the original test image represents, constrained by a foundation model pretrained with a large amount of samples. As a result, our new method will offer us a new way to evaluate the models' robustness performance, free of limitations of fixed benchmarks or constrained perturbations, although scoped by the power of the oracle. In addition to the evaluation results, we also leverage our generated data to understand the behaviors of the model and our new evaluation strategies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115512664",
                    "name": "Peiyan Zhang"
                },
                {
                    "authorId": "2143856677",
                    "name": "Hao Liu"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "2110972816",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "2118021616",
                    "name": "Sunghun Kim"
                },
                {
                    "authorId": "49528192",
                    "name": "Haohan Wang"
                }
            ]
        }
    ]
}