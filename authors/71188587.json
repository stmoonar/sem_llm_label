{
    "authorId": "71188587",
    "papers": [
        {
            "paperId": "09c3a340f5d6bb8b9b9f6a4a77326c0d692d4c66",
            "title": "AdaPT: A Set of Guidelines for Hyperbolic Multimodal Multilingual NLP",
            "abstract": "The Euclidean space is the familiar space for training neural models and performing arithmetic operations. However, many data types inherently possess complex geometries, and model training methods involve operating over their latent representations, which cannot be effectively captured in the Euclidean space. The hyperbolic space provides a more generalized representative geometry to model the hierarchical complexities of the tree-like structure of natural language. We propose A DA PT a set of guidelines for initialization, parametrization, and training of neural networks, which adapts to the dataset and can be used with different manifolds. A DA PT can be generalized over any existing neural network training methodology and leads to more stable training without a substantial increase in training time. We apply A DA PT guidelines over two state-of-the-art deep learning approaches and empirically demonstrate its effectiveness through experiments on three tasks over 12 languages across speech and text. Through extensive qualitative analysis, we put forward the applicability of A DA PT as a set of guidelines optimally utilizing the manifold geometry, which can be extended to various downstream tasks across languages and modalities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "1824294087",
                    "name": "Shrey Pandit"
                },
                {
                    "authorId": "2069609589",
                    "name": "Vishwa Shah"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2313536726",
                    "name": "Shafiq Joty"
                }
            ]
        },
        {
            "paperId": "334d7fba900eab258cd4fbb5152539e83678b9c4",
            "title": "ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild",
            "abstract": "Given the ubiquity of charts as a data analysis, visualization, and decision-making tool across industries and sciences, there has been a growing interest in developing pre-trained foundation models as well as general purpose instruction-tuned models for chart understanding and reasoning. However, existing methods suffer crucial drawbacks across two critical axes affecting the performance of chart representation models: they are trained on data generated from underlying data tables of the charts, ignoring the visual trends and patterns in chart images, and use weakly aligned vision-language backbone models for domain-specific training, limiting their generalizability when encountering charts in the wild. We address these important drawbacks and introduce ChartGemma, a novel chart understanding and reasoning model developed over PaliGemma. Rather than relying on underlying data tables, ChartGemma is trained on instruction-tuning data generated directly from chart images, thus capturing both high-level trends and low-level visual information from a diverse set of charts. Our simple approach achieves state-of-the-art results across $5$ benchmarks spanning chart summarization, question answering, and fact-checking, and our elaborate qualitative studies on real-world charts show that ChartGemma generates more realistic and factually correct summaries compared to its contemporaries. We release the code, model checkpoints, dataset, and demos at https://github.com/vis-nlp/ChartGemma.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2310230414",
                    "name": "Aayush Bajaj"
                },
                {
                    "authorId": "2310228208",
                    "name": "Aaryaman Kartha"
                },
                {
                    "authorId": "2274022429",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ]
        },
        {
            "paperId": "4b5fefaccd9153da9895f69ee3ec7ce6c0b747d0",
            "title": "Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications",
            "abstract": "Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We introduce RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. RS transforms a classifier into a smoothed classifier to obtain robust representations, whereas MI forces a model to exploit the surrounding context of a masked token in an input sequence. RSMI improves adversarial robustness by 2 to 3 times over existing state-of-the-art methods on benchmark datasets. We also perform in-depth qualitative analysis to validate the effectiveness of the different stages of RSMI and probe the impact of its components through extensive ablations. By empirically proving the stability of RSMI, we put it forward as a practical method to robustly train large-scale NLP models. Our code and datasets are available at https://github.com/Han8931/rsmi_nlp",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1382771958",
                    "name": "Han Cheol Moon"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                },
                {
                    "authorId": "2091437375",
                    "name": "Ruochen Zhao"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2162736149",
                    "name": "Xu Chi"
                }
            ]
        },
        {
            "paperId": "7dcb732c92e9c5b53aff482e543db4909dfa62dc",
            "title": "Learning Through Interpolative Augmentation of Dynamic Curvature Spaces",
            "abstract": "Mixup is an efficient data augmentation technique, which improves generalization by interpolating random examples. While numerous approaches have been developed for Mixup in the Euclidean and in the hyperbolic space, they do not fully use the intrinsic properties of the examples, i.e., they manually set the geometry (Euclidean or hyperbolic) based on the overall dataset, which may be sub-optimal since each example may require a different geometry. We propose DynaMix, a framework that automatically selects an example-specific geometry and performs Mixup between the different geometries to improve training dynamics and generalization. Through extensive experiments in image and text modalities we show that DynaMix outperforms state-of-the-art methods over six downstream applications. We find that DynaMix is more useful in low-resource and semi-supervised settings likely because it displays a probabilistic view of the geometry.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2186676914",
                    "name": "Parth Chhabra"
                },
                {
                    "authorId": "2157860264",
                    "name": "A. Neerkaje"
                },
                {
                    "authorId": "1923351",
                    "name": "Shivam Agarwal"
                },
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2275117",
                    "name": "S. Chava"
                }
            ]
        },
        {
            "paperId": "015271efe207165753b7d8c1346b91fe8f6064fd",
            "title": "THINK: Temporal Hypergraph Hyperbolic Network",
            "abstract": "Network-based time series forecasting is a challenging task as it involves complex geometric properties, higher-order relations, and scale-free characteristics. Previous work has modeled network-based series as oversimplified graphs or has ignored the power law dynamics of real-world temporal and dynamic networks, which could yield suboptimal results. With the aim to address these issues, here we propose THINK, a novel framework based on hypergraph learning that captures the hyperbolic properties of time-evolving dynamic hypergraphs. We design an elegant hyperbolic distance-aware hypergraph attention mechanism to better capture informative internal structural features on the Poincar\u00e9 ball. Through quantitative and conceptual analysis on seven tasks across temporal, and time-evolving dynamic hypergraphs, we demonstrate THINK\u2019s practicality in comparison to a variety of benchmarks spanning finance, health, and energy networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1923351",
                    "name": "Shivam Agarwal"
                },
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "145325584",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "12524628",
                    "name": "Tyler Derr"
                }
            ]
        },
        {
            "paperId": "1cec3e33185a7c7e4f60cb85278320321facbc7a",
            "title": "PISA: PoIncar\u00e9 Saliency-Aware Interpolative Augmentation",
            "abstract": "Saliency-aware portion-wise mixup has proven to be an effective data augmentation technique for different modalities and tasks. However, it involves calculating the saliency over gradient vectors in the Euclidean space, representations that often possess complicated geometries and inherent hierarchical structure. We propose PISA, saliency-aware interpolative regularization operating in the hyperbolic space, to better capture the complex geometries of representations. To this end, we also formulate a saliency-aware mixup for speech signals. PISA outperforms existing state-of-the-art interpolative augmentation methods on 7 benchmark and low-resource datasets from the domains of speech signal processing and computer vision. PISA results in more stable training than existing data augmentation methods while being robust to adversarial attacks. It can be generalized across modalities, models and downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2069609589",
                    "name": "Vishwa Shah"
                },
                {
                    "authorId": "144144799",
                    "name": "Puneet Mathur"
                },
                {
                    "authorId": "144582538",
                    "name": "Vasu Sharma"
                },
                {
                    "authorId": "1699159",
                    "name": "Dinesh Manocha"
                }
            ]
        },
        {
            "paperId": "2009939f7f8adff27164debdb3c6f0455076b266",
            "title": "Tweet Based Reach Aware Temporal Attention Network for NFT Valuation",
            "abstract": "Non-Fungible Tokens (NFTs) are a relatively unexplored class of assets. Designing strategies to forecast NFT trends is an intricate task due to its extremely volatile nature. The market is largely driven by public sentiment and \"hype\", which in turn has a high correlation with conversations taking place on social media platforms like Twitter. Prior work done for modelling stock market data does not take into account the extent of impact certain highly influential tweets and their authors can have on the market. Building on these limitations and the nature of the NFT market, we propose a novel reach-aware temporal learning approach to make predictions for forecasting future trends in the NFT market. We perform experiments on a new dataset consisting of over 1.3 million tweets and 180 thousand NFT transactions spanning over 15 NFT collections curated by us. Our model (TA-NFT) outperforms other state-of-the-art methods by an average of 36%. Through extensive quantitative and ablative analysis, we demonstrate the ability of our approach as a practical method for predicting NFT trends.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2161000447",
                    "name": "Ritesh Soun"
                },
                {
                    "authorId": "2157860264",
                    "name": "A. Neerkaje"
                },
                {
                    "authorId": "144582538",
                    "name": "Vasu Sharma"
                },
                {
                    "authorId": "2047009798",
                    "name": "Dipanwita Guhathakurta"
                },
                {
                    "authorId": "2275117",
                    "name": "S. Chava"
                }
            ]
        },
        {
            "paperId": "610fb4e05db08943a6b313426df37a59cc25c270",
            "title": "CIAug: Equipping Interpolative Augmentation with Curriculum Learning",
            "abstract": "Interpolative data augmentation has proven to be effective for NLP tasks. Despite its merits, the sample selection process in mixup is random, which might make it difficult for the model to generalize better and converge faster. We propose CIAug, a novel curriculum-based learning method that builds upon mixup. It leverages the relative position of samples in hyperbolic embedding space as a complexity measure to gradually mix up increasingly difficult and diverse samples along training. CIAug achieves state-of-the-art results over existing interpolative augmentation methods on 10 benchmark datasets across 4 languages in text classification and named-entity recognition tasks. It also converges and achieves benchmark F1 scores 3 times faster. We empirically analyze the various components of CIAug, and evaluate its robustness against adversarial attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "2161000447",
                    "name": "Ritesh Soun"
                },
                {
                    "authorId": "1824294087",
                    "name": "Shrey Pandit"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2175482552",
                    "name": "Sarvagya Malaviya"
                },
                {
                    "authorId": "1826312",
                    "name": "Yuval Pinter"
                }
            ]
        },
        {
            "paperId": "ca2f63950685a97e5ab6b8e6b2db78a8995e94a2",
            "title": "Chart-to-Text: A Large-Scale Benchmark for Chart Summarization",
            "abstract": "Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2158815576",
                    "name": "Shankar Kanthara"
                },
                {
                    "authorId": "2158811816",
                    "name": "Rixie Tiffany Ko Leong"
                },
                {
                    "authorId": "144366156",
                    "name": "Xiang Lin"
                },
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ]
        },
        {
            "paperId": "cf4275634ccf8102d080842db300e7585b9f7f42",
            "title": "DMix: Adaptive Distance-aware Interpolative Mixup",
            "abstract": "Interpolation-based regularisation methods such as Mixup, which generate virtual training samples, have proven to be effective for various tasks and modalities.We extend Mixup and propose DMix, an adaptive distance-aware interpolative Mixup that selects samples based on their diversity in the embedding space. DMix leverages the hyperbolic space as a similarity measure among input samples for a richer encoded representation.DMix achieves state-of-the-art results on sentence classification over existing data augmentation methods on 8 benchmark datasets across English, Arabic, Turkish, and Hindi languages while achieving benchmark F1 scores in 3 times less number of iterations.We probe the effectiveness of DMix in conjunction with various similarity measures and qualitatively analyze the different components.DMix being generalizable, can be applied to various tasks, models and modalities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "1824294087",
                    "name": "Shrey Pandit"
                },
                {
                    "authorId": "2161000447",
                    "name": "Ritesh Soun"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2143919864",
                    "name": "Diyi Yang"
                },
                {
                    "authorId": "2125481734",
                    "name": "Lucie Flek"
                }
            ]
        }
    ]
}