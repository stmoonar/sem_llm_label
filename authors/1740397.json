{
    "authorId": "1740397",
    "papers": [
        {
            "paperId": "5c6192f5366dec1eb2717ceee4343e75cd92d31c",
            "title": "Can GPT-4 do L2 analytic assessment?",
            "abstract": "Automated essay scoring (AES) to evaluate second language (L2) proficiency has been a firmly established technology used in educational contexts for decades. Although holistic scoring has seen advancements in AES that match or even exceed human performance, analytic scoring still encounters issues as it inherits flaws and shortcomings from the human scoring process. The recent introduction of large language models presents new opportunities for automating the evaluation of specific aspects of L2 writing proficiency. In this paper, we perform a series of experiments using GPT-4 in a zero-shot fashion on a publicly available dataset annotated with holistic scores based on the Common European Framework of Reference and aim to extract detailed information about their underlying analytic components. We observe significant correlations between the automatically predicted analytic scores and multiple features associated with the individual proficiency components.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1490643385",
                    "name": "Stefano Bann\u00f2"
                },
                {
                    "authorId": "3284223",
                    "name": "Hari Krishna Vydana"
                },
                {
                    "authorId": "145962472",
                    "name": "K. Knill"
                },
                {
                    "authorId": "1740397",
                    "name": "M. Gales"
                }
            ]
        },
        {
            "paperId": "a33c3006cb9e95d04ec94dfee32f51c4b0e1fef2",
            "title": "Learn and Don't Forget: Adding a New Language to ASR Foundation Models",
            "abstract": "Foundation ASR models often support many languages, e.g. 100 languages in Whisper. However, there has been limited work on integrating an additional, typically low-resource, language, while maintaining performance on the original language set. Fine-tuning, while simple, may degrade the accuracy of the original set. We compare three approaches that exploit adaptation parameters: soft language code tuning, train only the language code; soft prompt tuning, train prepended tokens; and LoRA where a small set of additional parameters are optimised. Elastic Weight Consolidation (EWC) offers an alternative compromise with the potential to maintain performance in specific target languages. Results show that direct fine-tuning yields the best performance for the new language but degrades existing language capabilities. EWC can address this issue for specific languages. If only adaptation parameters are used, the language capabilities are maintained but at the cost of performance in the new language.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265756514",
                    "name": "Mengjie Qian"
                },
                {
                    "authorId": "2309225840",
                    "name": "Siyuan Tang"
                },
                {
                    "authorId": "2266352883",
                    "name": "Rao Ma"
                },
                {
                    "authorId": "145962472",
                    "name": "K. Knill"
                },
                {
                    "authorId": "1740397",
                    "name": "M. Gales"
                }
            ]
        },
        {
            "paperId": "a6d7a27a05d1e50523c7135afb2b450fb610e2af",
            "title": "Grammatical Error Feedback: An Implicit Evaluation Approach",
            "abstract": "Grammatical feedback is crucial for consolidating second language (L2) learning. Most research in computer-assisted language learning has focused on feedback through grammatical error correction (GEC) systems, rather than examining more holistic feedback that may be more useful for learners. This holistic feedback will be referred to as grammatical error feedback (GEF). In this paper, we present a novel implicit evaluation approach to GEF that eliminates the need for manual feedback annotations. Our method adopts a grammatical lineup approach where the task is to pair feedback and essay representations from a set of possible alternatives. This matching process can be performed by appropriately prompting a large language model (LLM). An important aspect of this process, explored here, is the form of the lineup, i.e., the selection of foils. This paper exploits this framework to examine the quality and need for GEC to generate feedback, as well as the system used to generate feedback, using essays from the Cambridge Learner Corpus.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1490643385",
                    "name": "Stefano Bann\u00f2"
                },
                {
                    "authorId": "145962472",
                    "name": "K. Knill"
                },
                {
                    "authorId": "1740397",
                    "name": "M. Gales"
                }
            ]
        },
        {
            "paperId": "c8b397c3388079268cf1b7c7568488a06057c68f",
            "title": "Efficient Sample-Specific Encoder Perturbations",
            "abstract": "Encoder-decoder foundation models have displayed state-of-the-art performance on a range of autoregressive sequence tasks. This paper proposes a simple and lightweight modification to such systems to control the behaviour according to a specific attribute of interest. This paper proposes a novel inference-efficient approach to modifying the behaviour of an encoder-decoder system according to a specific attribute of interest. Specifically, we show that a small proxy network can be used to find a sample-by-sample perturbation of the encoder output of a frozen foundation model to trigger the decoder to generate improved decodings. This work explores a specific realization of this framework focused on improving the COMET performance of Flan-T5 on Machine Translation and the WER of Whisper foundation models on Speech Recognition. Results display consistent improvements in performance evaluated through COMET and WER respectively. Furthermore, experiments also show that the proxies are robust to the exact nature of the data used to train them and can extend to other domains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1400414048",
                    "name": "Yassir Fathullah"
                },
                {
                    "authorId": "1740397",
                    "name": "M. Gales"
                }
            ]
        },
        {
            "paperId": "048564c2ebbf90507302bd3f2ce451b3070c7fa5",
            "title": "Speak & Improve: L2 English Speaking Practice Tool",
            "abstract": "A problem for building and studying approaches to automatically assess and give feedback to L2 English learners on their speaking ability is a lack of suitable data sets and platforms to run experiments on. This paper describes the Speak & Improve (S&I) speaking practice tool, which has been designed to meet the needs of researchers while also offering learners the opportunity to practise their English speaking and improve their confidence. S&I has tasks that allow the learner to demonstrate and improve their proficiency across the English speaking construct. Most of the tasks are free speaking, that is, the learner is not constrained in what they have to say. Over 400,000 learners worldwide have tried the alpha version of S&I. This paper presents the next version which provides more choice and flexibility to the users and more feedback on performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2306692550",
                    "name": "Diane Nicholls"
                },
                {
                    "authorId": "145962472",
                    "name": "K. Knill"
                },
                {
                    "authorId": "1740397",
                    "name": "M. Gales"
                },
                {
                    "authorId": "1804969",
                    "name": "A. Ragni"
                },
                {
                    "authorId": "2306740957",
                    "name": "Paul Ricketts"
                }
            ]
        },
        {
            "paperId": "067aaf0d1cde4ee21063be137559f2fe50125570",
            "title": "Multi-Head State Space Model for Speech Recognition",
            "abstract": "State space models (SSMs) have recently shown promising results on small-scale sequence and language modelling tasks, rivalling and outperforming many attention-based approaches. In this paper, we propose a multi-head state space (MH-SSM) architecture equipped with special gating mechanisms, where parallel heads are taught to learn local and global temporal dynamics on sequence data. As a drop-in replacement for multi-head attention in transformer encoders, this new model significantly outperforms the transformer transducer on the LibriSpeech speech recognition corpus. Furthermore, we augment the transformer block with MH-SSMs layers, referred to as the Stateformer, achieving state-of-the-art performance on the LibriSpeech task, with word error rates of 1.76\\%/4.37\\% on the development and 1.91\\%/4.36\\% on the test sets without using an external language model.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1400414048",
                    "name": "Yassir Fathullah"
                },
                {
                    "authorId": "1699859",
                    "name": "Chunyang Wu"
                },
                {
                    "authorId": "3316583",
                    "name": "Yuan Shangguan"
                },
                {
                    "authorId": "49222521",
                    "name": "J. Jia"
                },
                {
                    "authorId": "22253126",
                    "name": "Wenhan Xiong"
                },
                {
                    "authorId": "3222225",
                    "name": "Jay Mahadeokar"
                },
                {
                    "authorId": "2145159028",
                    "name": "Chunxi Liu"
                },
                {
                    "authorId": "152345059",
                    "name": "Yangyang Shi"
                },
                {
                    "authorId": "1729960",
                    "name": "Ozlem Kalinli"
                },
                {
                    "authorId": "1727524",
                    "name": "M. Seltzer"
                },
                {
                    "authorId": "1740397",
                    "name": "M. Gales"
                }
            ]
        },
        {
            "paperId": "2b8b54f1db94e0ac587b73804d8aa3366be8f13d",
            "title": "Annotation of L2 English Speech for Developing and Evaluating End-to-End Spoken Grammatical Error Correction",
            "abstract": "A challenge for automated spoken language assessment and feedback is the lack of high quality manually annotated L2 learner corpora, even for a common language like English. At the same time the popularity of end-to-end systems, which integrate speech recognition (ASR) with downstream tasks, has increased. This paper describes the annotation of a corpus that supports end-to-end system evaluation for Spoken Grammatical Error Correction (SGEC). There raises a number of challenges. This is further complicated as the annotation is preferably able to handle evaluation and development of individual modules, such as ASR, disfluency detection and GEC, combinations of these modules, as well as the final end-to-end system. A detailed description of the process used to annotate data from the Linguaskill Speaking test, a multi-level test for candidates from CEFR levels below A1 to C1 and above, is given. An example of how the corpus has been used to evaluate an advanced SGEC system is presented.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145962472",
                    "name": "K. Knill"
                },
                {
                    "authorId": "46951456",
                    "name": "D. Nicholls"
                },
                {
                    "authorId": "1740397",
                    "name": "M. Gales"
                },
                {
                    "authorId": "2232508654",
                    "name": "Pawel Stroinski"
                },
                {
                    "authorId": "2232510013",
                    "name": "Alex Watkinson"
                }
            ]
        },
        {
            "paperId": "32fe63f88d82213371c7b5caa950bc72bfb47c38",
            "title": "Sentiment Perception Adversarial Attacks on Neural Machine Translation Systems",
            "abstract": "With the advent of deep learning methods, Neural Machine Translation (NMT) systems have become increasingly powerful. However, deep learning based systems are susceptible to adversarial attacks, where imperceptible changes to the input can cause undesirable changes at the output of the system. To date there has been little work investigating adversarial attacks on sequence-to-sequence systems, such as NMT models. Previous work in NMT has examined attacks with the aim of introducing target phrases in the output sequence. In this work, adversarial attacks for NMT systems are explored from an output perception perspective. Thus the aim of an attack is to change the perception of the output sequence, without altering the perception of the input sequence. For example, an adversary may distort the sentiment of translated reviews to have an exaggerated positive sentiment. In practice it is challenging to run extensive human perception experiments, so a proxy deep-learning classifier applied to the NMT output is used to measure perception changes. Experiments demonstrate that the sentiment perception of NMT systems' output sequences can be changed significantly with small imperceptible changes to input sequences.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2007545675",
                    "name": "Vyas Raina"
                },
                {
                    "authorId": "1740397",
                    "name": "M. Gales"
                }
            ]
        },
        {
            "paperId": "555c97eceb2979b1f4da0b8e466202212f6bc30d",
            "title": "Unsupervised Multi-Hashing for Image Retrieval in Non-stationary Environments",
            "abstract": "Hashing methods help retrieve swiftly in large-scale dataset, which is important for real-world image retrieval. New data is produced continually in the real world which may cause concept drift and inaccurate retrieval results. To address this issue, hashing methods in non-stationary environments are proposed. However, most hashing methods in non-stationary data environments are supervised. In practice, it is hard to get exact labels of data especially in non-stationary data environments. Therefore, we propose the unsupervised multi-hashing (UMH) method for unsupervised image retrieval in non-stationary environments. Thus, in the UMH, a set of hash functions is trained and added to the kept list of hash functions sets when a new data chunk occurs. Then, multiple sets of hash functions are kept with different weights to guarantee that similarity information in old and new data are both adapted. Experiments on two real-world image datasets show that the UMH yields better retrieval performance in non-stationary environments than other comparative methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143685831",
                    "name": "Yi Yang"
                },
                {
                    "authorId": "8194253",
                    "name": "Qihua Li"
                },
                {
                    "authorId": "145682039",
                    "name": "Xing Tian"
                },
                {
                    "authorId": "2061154698",
                    "name": "Wing W. Y. Ng"
                },
                {
                    "authorId": "2196822204",
                    "name": "Hui Wang"
                },
                {
                    "authorId": "145801638",
                    "name": "J. Kittler"
                },
                {
                    "authorId": "1740397",
                    "name": "M. Gales"
                },
                {
                    "authorId": "2220065485",
                    "name": "Rob Cooper"
                }
            ]
        },
        {
            "paperId": "6137cff058ee8d34b45839ddd228b042c9c2a072",
            "title": "Adapting an Unadaptable ASR System",
            "abstract": "As speech recognition model sizes and training data requirements grow, it is increasingly common for systems to only be available via APIs from online service providers rather than having direct access to models themselves. In this scenario it is challenging to adapt systems to a specific target domain. To address this problem we consider the recently released OpenAI Whisper ASR as an example of a large-scale ASR system to assess adaptation methods. An error correction based approach is adopted, as this does not require access to the model, but can be trained from either 1-best or N-best outputs that are normally available via the ASR API. LibriSpeech is used as the primary target domain for adaptation. The generalization ability of the system in two distinct dimensions are then evaluated. First, whether the form of correction model is portable to other speech recognition domains, and secondly whether it can be used for ASR models having a different architecture.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "50397234",
                    "name": "Rao Ma"
                },
                {
                    "authorId": "10769548",
                    "name": "Mengjie Qian"
                },
                {
                    "authorId": "1740397",
                    "name": "M. Gales"
                },
                {
                    "authorId": "145962472",
                    "name": "K. Knill"
                }
            ]
        }
    ]
}