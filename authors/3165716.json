{
    "authorId": "3165716",
    "papers": [
        {
            "paperId": "62ac45c894b38a9ec6ca089d1bea292281089d04",
            "title": "Authorship Obfuscation in Multilingual Machine-Generated Text Detection",
            "abstract": "High-quality text generation capability of recent Large Language Models (LLMs) causes concerns about their misuse (e.g., in massive generation/spread of disinformation). Machine-generated text (MGT) detection is important to cope with such threats. However, it is susceptible to authorship obfuscation (AO) methods, such as paraphrasing, which can cause MGTs to evade detection. So far, this was evaluated only in monolingual settings. Thus, the susceptibility of recently proposed multilingual detectors is still unknown. We fill this gap by comprehensively benchmarking the performance of 10 well-known AO methods, attacking 37 MGT detection methods against MGTs in 11 languages (i.e., 10 $\\times$ 37 $\\times$ 11 = 4,070 combinations). We also evaluate the effect of data augmentation on adversarial robustness using obfuscated texts. The results indicate that all tested AO methods can cause evasion of automated detection in all tested languages, where homoglyph attacks are especially successful. However, some of the AO methods severely damaged the text, making it no longer readable or easily recognizable by humans (e.g., changed language, weird characters).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260109257",
                    "name": "Dominik Macko"
                },
                {
                    "authorId": "144535025",
                    "name": "R\u00f3bert M\u00f3ro"
                },
                {
                    "authorId": "150035131",
                    "name": "Adaku Uchendu"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "2260072705",
                    "name": "Jason Samuel Lucas"
                },
                {
                    "authorId": "66848311",
                    "name": "Michiharu Yamashita"
                },
                {
                    "authorId": "66674465",
                    "name": "Nafis Irtiza Tripto"
                },
                {
                    "authorId": "2279666194",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "3165716",
                    "name": "Jakub Simko"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                }
            ]
        },
        {
            "paperId": "abd552a721666deb24938ba8f77731790a847ea0",
            "title": "Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation",
            "abstract": "While fine-tuning of pre-trained language models generally helps to overcome the lack of labelled training samples, it also displays model performance instability. This instability mainly originates from randomness in initialisation or data shuffling. To address this, researchers either modify the training process or augment the available samples, which typically results in increased computational costs. We propose a new mitigation strategy, called Delayed Ensemble with Noisy Interpolation (DENI), that leverages the strengths of ensembling, noise regularisation and model interpolation, while retaining computational efficiency. We compare DENI with 9 representative mitigation strategies across 3 models, 4 tuning strategies and 7 text classification datasets. We show that: 1) DENI outperforms the best performing mitigation strategy (Ensemble), while using only a fraction of its cost; 2) the mitigation strategies are beneficial for parameter-efficient fine-tuning (PEFT) methods, outperforming full fine-tuning in specific cases; and 3) combining DENI with data augmentation often leads to even more effective instability mitigation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2051747952",
                    "name": "Branislav Pecher"
                },
                {
                    "authorId": "1865742502",
                    "name": "J\u00e1n Cegin"
                },
                {
                    "authorId": "2266340559",
                    "name": "R\u00f3bert Belanec"
                },
                {
                    "authorId": "3165716",
                    "name": "Jakub Simko"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                }
            ]
        },
        {
            "paperId": "c6db2f2e73f75c69639e566f6a65a14a19c48bba",
            "title": "Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation",
            "abstract": "The latest generative large language models (LLMs) have found their application in data augmentation tasks, where small numbers of text samples are LLM-paraphrased and then used to fine-tune downstream models. However, more research is needed to assess how different prompts, seed data selection strategies, filtering methods, or model settings affect the quality of paraphrased data (and downstream models). In this study, we investigate three text diversity incentive methods well established in crowdsourcing: taboo words, hints by previous outlier solutions, and chaining on previous outlier solutions. Using these incentive methods as part of instructions to LLMs augmenting text datasets, we measure their effects on generated texts lexical diversity and downstream model performance. We compare the effects over 5 different LLMs, 6 datasets and 2 downstream models. We show that diversity is most increased by taboo words, but downstream model performance is highest with hints.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1865742502",
                    "name": "J\u00e1n Cegin"
                },
                {
                    "authorId": "2051747952",
                    "name": "Branislav Pecher"
                },
                {
                    "authorId": "3165716",
                    "name": "Jakub Simko"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                },
                {
                    "authorId": "2069676493",
                    "name": "Peter Brusilovsky"
                }
            ]
        },
        {
            "paperId": "4cd30b50bcc0b919ad9c77a9bfbcd9428db27e83",
            "title": "Multilingual Previously Fact-Checked Claim Retrieval",
            "abstract": "Fact-checkers are often hampered by the sheer amount of online content that needs to be fact-checked. NLP can help them by retrieving already existing fact-checks relevant to the content being investigated. This paper introduces a new multilingual dataset -- MultiClaim -- for previously fact-checked claim retrieval. We collected 28k posts in 27 languages from social media, 206k fact-checks in 39 languages written by professional fact-checkers, as well as 31k connections between these two groups. This is the most extensive and the most linguistically diverse dataset of this kind to date. We evaluated how different unsupervised methods fare on this dataset and its various dimensions. We show that evaluating such a diverse dataset has its complexities and proper care needs to be taken before interpreting the results. We also evaluated a supervised fine-tuning approach, improving upon the unsupervised method significantly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2217264741",
                    "name": "Mat'uvs Pikuliak"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "144535025",
                    "name": "R\u00f3bert M\u00f3ro"
                },
                {
                    "authorId": "2215213164",
                    "name": "Timo Hromadka"
                },
                {
                    "authorId": "2215214171",
                    "name": "Timotej Smolen"
                },
                {
                    "authorId": "52562063",
                    "name": "Martin Meli\u0161ek"
                },
                {
                    "authorId": "2217274494",
                    "name": "Ivan Vykopal"
                },
                {
                    "authorId": "3165716",
                    "name": "Jakub Simko"
                },
                {
                    "authorId": "2114533885",
                    "name": "Juraj Podrou\u017eek"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                }
            ]
        },
        {
            "paperId": "c48e33add72c9c7d2cf06f151e893460d96f8f05",
            "title": "Is it indeed bigger better? The comprehensive study of claim detection LMs applied for disinformation tackling",
            "abstract": "This study compares the performance of (1) fine-tuned models and (2) extremely large language models on the task of check-worthy claim detection. For the purpose of the comparison we composed a multilingual and multi-topical dataset comprising texts of various sources and styles. Building on this, we performed a benchmark analysis to determine the most general multilingual and multi-topical claim detector. We chose three state-of-the-art models in the check-worthy claim detection task and fine-tuned them. Furthermore, we selected three state-of-the-art extremely large language models without any fine-tuning. We made modifications to the models to adapt them for multilingual settings and through extensive experimentation and evaluation. We assessed the performance of all the models in terms of accuracy, recall, and F1-score in in-domain and cross-domain scenarios. Our results demonstrate that despite the technological progress in the area of natural language processing, the models fine-tuned for the task of check-worthy claim detection still outperform the zero-shot approaches in a cross-domain settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266239634",
                    "name": "Martin Hyben"
                },
                {
                    "authorId": "2266239628",
                    "name": "Sebastian Kula"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "144535025",
                    "name": "R\u00f3bert M\u00f3ro"
                },
                {
                    "authorId": "3165716",
                    "name": "Jakub Simko"
                }
            ]
        },
        {
            "paperId": "ef2241b1e36dfa160ae3ca51f88cec77ae358a71",
            "title": "ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness",
            "abstract": "The emergence of generative large language models (LLMs) raises the question: what will be its impact on crowdsourcing? Traditionally, crowdsourcing has been used for acquiring solutions to a wide variety of human-intelligence tasks, including ones involving text generation, modification or evaluation. For some of these tasks, models like ChatGPT can potentially substitute human workers. In this study, we investigate whether this is the case for the task of paraphrase generation for intent classification. We apply data collection methodology of an existing crowdsourcing study (similar scale, prompts and seed data) using ChatGPT and Falcon-40B. We show that ChatGPT-created paraphrases are more diverse and lead to at least as robust models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1865742502",
                    "name": "J\u00e1n Cegin"
                },
                {
                    "authorId": "3165716",
                    "name": "Jakub Simko"
                },
                {
                    "authorId": "1804693",
                    "name": "Peter Brusilovsky"
                }
            ]
        },
        {
            "paperId": "29dfc9f02f34528be20ac3865961281b55ab7699",
            "title": "Monant Medical Misinformation Dataset: Mapping Articles to Fact-Checked Claims",
            "abstract": "False information has a significant negative influence on individuals as well as on the whole society. Especially in the current COVID-19 era, we witness an unprecedented growth of medical misinformation. To help tackle this problem with machine learning approaches, we are publishing a feature-rich dataset of approx. 317k medical news articles/blogs and 3.5k fact-checked claims. It also contains 573 manually and more than 51k automatically labelled mappings between claims and articles. Mappings consist of claim presence, i.e., whether a claim is contained in a given article, and article stance towards the claim. We provide several baselines for these two tasks and evaluate them on the manually labelled part of the dataset. The dataset enables a number of additional tasks related to medical misinformation, such as misinformation characterisation studies or studies of misinformation diffusion between sources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "2051747952",
                    "name": "Branislav Pecher"
                },
                {
                    "authorId": "1954148",
                    "name": "M. Tomlein"
                },
                {
                    "authorId": "144535025",
                    "name": "R\u00f3bert M\u00f3ro"
                },
                {
                    "authorId": "51238270",
                    "name": "Elena Stefancova"
                },
                {
                    "authorId": "3165716",
                    "name": "Jakub Simko"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                }
            ]
        },
        {
            "paperId": "4d212f2d5ad969740130535683b6b72d7fc1f28a",
            "title": "Autonomation, not Automation: Activities and Needs of Fact-checkers as a Basis for Designing Human-Centered AI Systems",
            "abstract": "To mitigate the negative effects of false information more effectively, the development of Artificial Intelligence (AI) systems assisting fact-checkers is needed. Nevertheless, the lack of focus on the needs of these stakeholders results in their limited acceptance and skepticism toward automating the whole fact-checking process. In this study, we conducted semi-structured in-depth interviews with Central European fact-checkers. Their activities and problems were analyzed using iterative content analysis. The most significant problems were validated with a survey of European fact-checkers, in which we collected 24 responses from 20 countries, i.e., 62\\% of active European signatories of the International Fact-Checking Network (IFCN). Our contributions include an in-depth examination of the variability of fact-checking work in non-English speaking regions, which still remained largely uncovered. By aligning them with the knowledge from prior studies, we created conceptual models that help understand the fact-checking processes. Thanks to the interdisciplinary collaboration, we extend the fact-checking process in AI research by three additional stages. In addition, we mapped our findings on the fact-checkers' activities and needs to the relevant tasks for AI research. The new opportunities identified for AI researchers and developers have implications for the focus of AI research in this domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "146090083",
                    "name": "Andrea Hrckova"
                },
                {
                    "authorId": "144535025",
                    "name": "R\u00f3bert M\u00f3ro"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "3165716",
                    "name": "Jakub Simko"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                }
            ]
        },
        {
            "paperId": "716a3af92aa10065ee7f2cfbd50625d6b761f431",
            "title": "Auditing YouTube\u2019s Recommendation Algorithm for Misinformation Filter Bubbles",
            "abstract": "In this article, we present results of an auditing study performed over YouTube aimed at investigating how fast a user can get into a misinformation filter bubble, but also what it takes to \u201cburst the bubble,\u201d i.e., revert the bubble enclosure. We employ a sock puppet audit methodology, in which pre-programmed agents (acting as YouTube users) delve into misinformation filter bubbles by watching misinformation-promoting content. Then they try to burst the bubbles and reach more balanced recommendations by watching misinformation-debunking content. We record search results, home page results, and recommendations for the watched videos. Overall, we recorded 17,405 unique videos, out of which we manually annotated 2,914 for the presence of misinformation. The labeled data was used to train a machine learning model classifying videos into three classes (promoting, debunking, neutral) with the accuracy of 0.82. We use the trained model to classify the remaining videos that would not be feasible to annotate manually. Using both the manually and automatically annotated data, we observe the misinformation bubble dynamics for a range of audited topics. Our key finding is that even though filter bubbles do not appear in some situations, when they do, it is possible to burst them by watching misinformation-debunking content (albeit it manifests differently from topic to topic). We also observe a sudden decrease of misinformation filter bubble effect when misinformation-debunking videos are watched after misinformation-promoting videos, suggesting a strong contextuality of recommendations. Finally, when comparing our results with a previous similar study, we do not observe significant improvements in the overall quantity of recommended misinformation content.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "144535025",
                    "name": "R\u00f3bert M\u00f3ro"
                },
                {
                    "authorId": "1954148",
                    "name": "M. Tomlein"
                },
                {
                    "authorId": "2051747952",
                    "name": "Branislav Pecher"
                },
                {
                    "authorId": "3165716",
                    "name": "Jakub Simko"
                },
                {
                    "authorId": "51238270",
                    "name": "Elena Stefancova"
                },
                {
                    "authorId": "2446470",
                    "name": "Michal Kompan"
                },
                {
                    "authorId": "146090083",
                    "name": "Andrea Hrckova"
                },
                {
                    "authorId": "2114533885",
                    "name": "Juraj Podrou\u017eek"
                },
                {
                    "authorId": "2175977506",
                    "name": "Adrian Gavornik"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                }
            ]
        },
        {
            "paperId": "7f9016990ff32313852326a0fa0fda3a34408806",
            "title": "A Game for Crowdsourcing Adversarial Examples for False Information Detection",
            "abstract": "False information detection models are susceptible to adversarial attacks. Such susceptibility is a critical weakness of detection models. Automated creation of adversarial samples can ultimately help to augment training sets and create more robust detection models. However, automatically generated adversarial samples often do not preserve the information contained in the original text, leading to information loss. There is a need for adversarial sample generators that can preserve the original information. To explore the properties such generators should have and to inform their future design, we conducted a study to collect adversarial samples from human agents using a Game with a purpose (GWAP). Player\u2019s goal is to modify a given tweet until a detection model is tricked thus creating an adversarial sample. We qualitatively analysed the collected adversarial samples and identified desired properties/strategies that an adversarial information-preserving generator should exhibit. These strategies are validated on detection models based on a transformer and LSTM models to confirm their applicability on different models. Based on these findings, we propose a novel generator approach that will exhibit the desired properties in order to generate high-quality information-preserving adversarial samples.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1865742502",
                    "name": "J\u00e1n Cegin"
                },
                {
                    "authorId": "3165716",
                    "name": "Jakub Simko"
                },
                {
                    "authorId": "2069676493",
                    "name": "Peter Brusilovsky"
                }
            ]
        }
    ]
}