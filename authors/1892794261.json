{
    "authorId": "1892794261",
    "papers": [
        {
            "paperId": "1fe3a802efdc4f1a3e5c8187547f38a3ec65750b",
            "title": "Unsupervised Dense Retrieval Training with Web Anchors",
            "abstract": "In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as \"homepage\" or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1892794261",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "2111312954",
                    "name": "X. Liu"
                },
                {
                    "authorId": "2139787803",
                    "name": "Chenyan Xiong"
                }
            ]
        },
        {
            "paperId": "2f78c98f8510d234fb7dedfb318dbd7c8c457465",
            "title": "Learning Task Skills and Goals Simultaneously from Physical Interaction",
            "abstract": "In real-world human-robot systems, it is essential for a robot to comprehend human objectives and respond accordingly while performing an extended series of motor actions. Although human objective alignment has recently emerged as a promising paradigm in the realm of physical human-robot interaction, its application is typically confined to generating simple motions due to inherent theoretical limitations. In this work, our goal is to develop a general formulation to learn manipulation functional modules and long-term task goals simultaneously from physical human-robot interaction. We show the feasibility of our framework in enabling robots to align their behaviors with the long-term task objectives inferred from human interactions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2035571489",
                    "name": "Haonan Chen"
                },
                {
                    "authorId": "2074638007",
                    "name": "Ye-Ji Mun"
                },
                {
                    "authorId": "2151324428",
                    "name": "Zhe Huang"
                },
                {
                    "authorId": "2238953897",
                    "name": "Yilong Niu"
                },
                {
                    "authorId": "1892794261",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "2238952560",
                    "name": "D. L. McPherson"
                },
                {
                    "authorId": "1404112858",
                    "name": "K. Driggs-Campbell"
                }
            ]
        },
        {
            "paperId": "38aaf8a29df6deeff0bf64cc835d242a25b10337",
            "title": "Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers",
            "abstract": "This paper explores the effectiveness of model-generated signals in improving zero-shot generalization of text-to-text Transformers such as T5. We study various designs to pretrain T5 using an auxiliary model to construct more challenging token replacements for the main model to denoise. Key aspects under study include the decoding target, the location of the RTD head, and the masking pattern. Based on these studies, we develop a new model, METRO-T0, which is pretrained using the redesigned ELECTRA-Style pretraining strategies and then prompt-finetuned on a mixture of NLP tasks. METRO-T0 outperforms all similar-sized baselines on prompted NLP benchmarks, such as _T0 Eval_ and MMLU, and rivals the state-of-the-art T0-11B model with only **8%** of its parameters. Our analysis on model\u2019s neural activation and parameter sensitivity reveals that the effectiveness of METRO-T0 stems from more balanced contribution of parameters and better utilization of their capacity. The code and model checkpoints are available at [https://github.com/gonglinyuan/metro_t0](https://github.com/gonglinyuan/metro_t0).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32816503",
                    "name": "Linyuan Gong"
                },
                {
                    "authorId": "2139787803",
                    "name": "Chenyan Xiong"
                },
                {
                    "authorId": "46522098",
                    "name": "Xiaodong Liu"
                },
                {
                    "authorId": "34765717",
                    "name": "Payal Bajaj"
                },
                {
                    "authorId": "1892794261",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "2172244319",
                    "name": "Alvin Cheung"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "50706785",
                    "name": "Xia Song"
                }
            ]
        },
        {
            "paperId": "f09102e00d8569f1ac34637b9e3ebb49f3b2ac7f",
            "title": "Towards Safe Multi-Level Human-Robot Interaction in Industrial Tasks",
            "abstract": "Multiple levels of safety measures are required by multiple interaction modes which collaborative robots need to perform industrial tasks with human co-workers. We develop three independent modules to account for safety in different types of human-robot interaction: vision-based safety monitoring pauses robot when human is present in a shared space; contact-based safety monitoring pauses robot when unexpected contact happens between human and robot; hierarchical intention tracking keeps robot in a safe distance from human when human and robot work independently, and switches robot to compliant mode when human intends to guide robot. We discuss the prospect of future research in development and integration of multi-level safety modules. We focus on how to provide safety guarantees for collaborative robot solutions with human behavior modeling.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2151324428",
                    "name": "Zhe Huang"
                },
                {
                    "authorId": "2074638007",
                    "name": "Ye-Ji Mun"
                },
                {
                    "authorId": "2035571489",
                    "name": "Haonan Chen"
                },
                {
                    "authorId": "1892794261",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "2008768",
                    "name": "Yilong Niu"
                },
                {
                    "authorId": "47057383",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "2159207116",
                    "name": "Ninghan Zhong"
                },
                {
                    "authorId": "39093367",
                    "name": "Ha-Il You"
                },
                {
                    "authorId": "48500893",
                    "name": "D. McPherson"
                },
                {
                    "authorId": "1404112858",
                    "name": "K. Driggs-Campbell"
                }
            ]
        },
        {
            "paperId": "019cb62ec0e5192d9950a21ead39afc4c70e8045",
            "title": "P4E: Few-Shot Event Detection as Prompt-Guided Identification and Localization",
            "abstract": "We propose P4E, an identify-and-localize event detection framework that integrates the best of few-shot prompting and structured prediction. Our framework decomposes event detection into an identification task and a localization task. For the identification task, which we formulate as multi-label classification, we leverage cloze-based prompting to align our objective with the pre-training task of language models, allowing our model to quickly adapt to new event types. We then employ an event type-agnostic sequence labeling model to localize the event trigger conditioned on the identification output. This heterogeneous model design allows P4E to quickly learn new event types without sacrificing the ability to make structured predictions. Our experiments demonstrate the effectiveness of our proposed design, and P4E shows superior performance for few-shot event detection on benchmark datasets FewEvent and MAVEN and comparable performance to SOTA for fully-supervised event detection on ACE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109154767",
                    "name": "Sha Li"
                },
                {
                    "authorId": "3355833",
                    "name": "Liyuan Liu"
                },
                {
                    "authorId": "1892794261",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "2181650518",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "16d4b709cc56bca5888ef1960bafba9beafb5904",
            "title": "PILED: An Identify-and-Localize Framework for Few-Shot Event Detection",
            "abstract": "Practical applications of event extraction systems have long been hindered by their need for heavy human annotation. In order to scale up to new domains and event types, models must learn to cope with limited supervision, as in few-shot learning settings. To this end, the major challenge is to let the model master the semantics of event types, without requiring abundant event mention annotations. In our study, we employ cloze prompts to elicit event-related knowledge from pretrained language models and further use event de\ufb01nitions and keywords to pinpoint the trigger word. By formulating the event detection task as an identify-then-localize procedure, we minimize the number of type-speci\ufb01c parameters, enabling our model to quickly adapt to event detection tasks for new types. Experiments on three event detection benchmark datasets (ACE, FewEvent, MAVEN) show that our proposed method performs favorably under fully supervised settings and surpasses existing few-shot methods by 21% F1 on the FewEvent dataset and 20% on the MAVEN dataset when only 5 examples are provided for each event type.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109154767",
                    "name": "Sha Li"
                },
                {
                    "authorId": "2109392217",
                    "name": "Liyuan Liu"
                },
                {
                    "authorId": "1892794261",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "2113323573",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "17da8aea52ea9c5678b9148f06448d035e26c1dd",
            "title": "Seamless Interaction Design with Coexistence and Cooperation Modes for Robust Human-Robot Collaboration",
            "abstract": "A robot needs multiple interaction modes to robustly collaborate with a human in complicated industrial tasks. We develop a Coexistence-and-Cooperation (CoCo) human-robot collaboration system. Coexistence mode enables the robot to work with the human on different sub-tasks independently in a shared space. Cooperation mode enables the robot to follow human guidance and recover failures. A human intention tracking algorithm takes in both human and robot motion measurements as input and provides a switch on the interaction modes. We demonstrate the effectiveness of CoCo system in a use case analogous to a real world multi-step assembly task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2151324428",
                    "name": "Zhe Huang"
                },
                {
                    "authorId": "2074638007",
                    "name": "Ye-Ji Mun"
                },
                {
                    "authorId": "47057383",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "1892794261",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "2159207116",
                    "name": "Ninghan Zhong"
                },
                {
                    "authorId": "2157045350",
                    "name": "Weihang Liang"
                },
                {
                    "authorId": "2052582704",
                    "name": "Junyi Geng"
                },
                {
                    "authorId": "120572161",
                    "name": "Tan Chen"
                },
                {
                    "authorId": "1404112858",
                    "name": "K. Driggs-Campbell"
                }
            ]
        },
        {
            "paperId": "1d954bb3f172c65f0a21c46231fdab6241a63c88",
            "title": "Hierarchical Intention Tracking for Robust Human-Robot Collaboration in Industrial Assembly Tasks",
            "abstract": "Collaborative robots require effective human intention estimation to safely and smoothly work with humans in less structured tasks such as industrial assembly, where human intention continuously changes. We propose the concept of intention tracking and introduce a collaborative robot system that concurrently tracks intentions at hierarchical levels. The high-level intention is tracked to estimate human's interaction pattern and enable robot to (1) avoid collision with human to minimize interruption and (2) assist human to correct failure. The low-level intention estimate provides robot with task-related information. We implement the system on a UR5e robot and demonstrate robust, seamless and ergonomic human-robot collaboration in an ablative pilot study of an assembly use case.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2151324428",
                    "name": "Zhe Huang"
                },
                {
                    "authorId": "2074638007",
                    "name": "Ye-Ji Mun"
                },
                {
                    "authorId": "47057383",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "1892794261",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "2159207116",
                    "name": "Ninghan Zhong"
                },
                {
                    "authorId": "2157045350",
                    "name": "Weihang Liang"
                },
                {
                    "authorId": "2052582704",
                    "name": "Junyi Geng"
                },
                {
                    "authorId": "120572161",
                    "name": "Tan Chen"
                },
                {
                    "authorId": "1404112858",
                    "name": "K. Driggs-Campbell"
                }
            ]
        },
        {
            "paperId": "3b228d551a403c69cbd04883740158264e13514b",
            "title": "KoMen: Domain Knowledge Guided Interaction Recommendation for Emerging Scenarios",
            "abstract": "User-User interaction recommendation, or interaction recommendation, is an indispensable service in social platforms, where the system automatically predicts with whom a user wants to interact. In real-world social platforms, we observe that user interactions may occur in diverse scenarios, and new scenarios constantly emerge, such as new games or sales promotions. There are two challenges in these emerging scenarios: (1) The behavior of users on the emerging scenarios could be different from existing ones due to the diversity among scenarios; (2) Emerging scenarios may only have scarce user behavioral data for model learning. Towards these two challenges, we present KoMen, a Domain Knowledge Guided Meta-learning framework for Interaction Recommendation. KoMen first learns a set of global model parameters shared among all scenarios and then quickly adapts the parameters for an emerging scenario based on its similarities with the existing ones. There are two highlights of KoMen: (1) KoMen customizes global model parameters by incorporating domain knowledge of the scenarios (e.g., a taxonomy that organizes scenarios by their purposes and functions), which captures scenario inter-dependencies with very limited training. (2) KoMen learns the scenario-specific parameters through a mixture-of-expert architecture, which reduces model variance resulting from data scarcity while still achieving the expressiveness to handle diverse scenarios. Extensive experiments demonstrate that KoMen achieves state-of-the-art performance on a public benchmark dataset and a large-scale real industry dataset. Remarkably, KoMen improves over the best baseline w.r.t. weighted ROC-AUC by 2.14% and 2.03% on the two datasets, respectively. Our code is available at: https://github.com/Veronicium/koMen.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1892794261",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "2118452539",
                    "name": "Zhen Wang"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                },
                {
                    "authorId": "2110479359",
                    "name": "Yaliang Li"
                },
                {
                    "authorId": "1696332",
                    "name": "Bolin Ding"
                },
                {
                    "authorId": "2642895",
                    "name": "Hongbo Deng"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "433e052ed37dd7fd43bf1bfd9e07419deb690814",
            "title": "Open-Vocabulary Argument Role Prediction for Event Extraction",
            "abstract": "The argument role in event extraction refers to the relation between an event and an argument participating in it. Despite the great progress in event extraction, existing studies still depend on roles pre-defined by domain experts. These studies expose obvious weakness when extending to emerging event types or new domains without available roles. Therefore, more attention and effort needs to be devoted to automatically customizing argument roles. In this paper, we define this essential but under-explored task: open-vocabulary argument role prediction. The goal of this task is to infer a set of argument roles for a given event type. We propose a novel unsupervised framework, RolePred for this task. Specifically, we formulate the role prediction problem as an in-filling task and construct prompts for a pre-trained language model to generate candidate roles. By extracting and analyzing the candidate arguments, the event-specific roles are further merged and selected. To standardize the research of this task, we collect a new event extraction dataset from WikiPpedia including 142 customized argument roles with rich semantics. On this dataset, RolePred outperforms the existing methods by a large margin. Source code and dataset are available on our GitHub repository: https://github.com/yzjiao/RolePred",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1381900594",
                    "name": "Yizhu Jiao"
                },
                {
                    "authorId": "2109154767",
                    "name": "Sha Li"
                },
                {
                    "authorId": "1892794261",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "1606040932",
                    "name": "Ming Zhong"
                },
                {
                    "authorId": "2181650518",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        }
    ]
}