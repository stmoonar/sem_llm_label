{
    "authorId": "34886987",
    "papers": [
        {
            "paperId": "5371a5fe3cebaa0b8174818a5935de59963c6769",
            "title": "Leveraging Small Language Models for Text2SPARQL Tasks to Improve the Resilience of AI Assistance",
            "abstract": "In this work we will show that language models with less than one billion parameters can be used to translate natural language to SPARQL queries after fine-tuning. Using three different datasets ranging from academic to real world, we identify prerequisites that the training data must fulfill in order for the training to be successful. The goal is to empower users of semantic web technology to use AI assistance with affordable commodity hardware, making them more resilient against external factors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2236686278",
                    "name": "Felix Brei"
                },
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "34886987",
                    "name": "Lars-Peter Meyer"
                }
            ]
        },
        {
            "paperId": "f0fc4c8ab5b5417ea98482fe2ad92f80800965fb",
            "title": "Assessing SPARQL capabilities of Large Language Models",
            "abstract": "The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs) offers significant synergistic potential for knowledge-driven applications. One possible integration is the interpretation and generation of formal languages, such as those used in the Semantic Web, with SPARQL being a core technology for accessing KGs. In this paper, we focus on measuring out-of-the box capabilities of LLMs to work with SPARQL and more specifically with SPARQL SELECT queries applying a quantitative approach. We implemented various benchmarking tasks in the LLM-KG-Bench framework for automated execution and evaluation with several LLMs. The tasks assess capabilities along the dimensions of syntax, semantic read, semantic create, and the role of knowledge graph prompt inclusion. With this new benchmarking tasks, we evaluated a selection of GPT, Gemini, and Claude models. Our findings indicate that working with SPARQL SELECT queries is still challenging for LLMs and heavily depends on the specific LLM as well as the complexity of the task. While fixing basic syntax errors seems to pose no problems for the best of the current LLMs evaluated, creating semantically correct SPARQL SELECT queries is difficult in several cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34886987",
                    "name": "Lars-Peter Meyer"
                },
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "2236686278",
                    "name": "Felix Brei"
                },
                {
                    "authorId": "2320464515",
                    "name": "Natanael Arndt"
                }
            ]
        },
        {
            "paperId": "10e0eb48710d7a98f284e41ec27f9a3c09f9b841",
            "title": "Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?",
            "abstract": "Large Language Models (LLMs) are advancing at a rapid pace, with significant improvements at natural language processing and coding tasks. Yet, their ability to work with formal languages representing data, specifically within the realm of knowledge graph engineering, remains under-investigated. To evaluate the proficiency of various LLMs, we created a set of five tasks that probe their ability to parse, understand, analyze, and create knowledge graphs serialized in Turtle syntax. These tasks, each embodying distinct degrees of complexity and being able to scale with the size of the problem, have been integrated into our automated evaluation system, the LLM-KG-Bench. The evaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4, Claude 1.3, and Claude 2.0, as well as two freely accessible offline models, GPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth understanding of the strengths and shortcomings of LLMs in relation to their application within RDF knowledge graph engineering workflows utilizing Turtle representation. While our findings show that the latest commercial models outperform their forerunners in terms of proficiency with the Turtle language, they also reveal an apparent weakness. These models fall short when it comes to adhering strictly to the output formatting constraints, a crucial requirement in this context.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "34886987",
                    "name": "Lars-Peter Meyer"
                },
                {
                    "authorId": "2258592",
                    "name": "Natanael Arndt"
                },
                {
                    "authorId": "2236686278",
                    "name": "Felix Brei"
                },
                {
                    "authorId": "2108312",
                    "name": "Kirill Bulert"
                }
            ]
        },
        {
            "paperId": "673f45a49083220e76a5f957cf58d1b15aa8455e",
            "title": "Scaling RML and SPARQL-based Knowledge Graph Construction with Apache Spark",
            "abstract": "Approaches for the construction of knowledge graphs from heterogeneous data sources range from ad-hoc scripts to dedicated mapping languages. Two common foundations are thereby RML and SPARQL. So far, both approaches are treated as different: On the one hand there are tools specifically for processing RML whereas on the other hand there are tools that extend SPARQL in order to incorporate additional data sources. In this work, we first show how this gap can be bridged by translating RML to a sequence of SPARQL CONSTRUCT queries and introduce the necessary SPARQL extensions. In a subsequent step, we employ techniques to optimize SPARQL query workloads as well as individual query execution times in order to obtain an optimized sequence of queries with respect to the order and uniqueness of the generated triples. Finally, we present a corresponding SPARQL query execution engine based on the Apache Spark Big Data framework. In our evaluation on benchmarks we show that our approach is capable of achieving RML mapping execution performance that surpasses the current state of the art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2390605",
                    "name": "Claus Stadler"
                },
                {
                    "authorId": "2625507",
                    "name": "Lorenz B\u00fchmann"
                },
                {
                    "authorId": "34886987",
                    "name": "Lars-Peter Meyer"
                },
                {
                    "authorId": "2110607609",
                    "name": "Michael Martin"
                }
            ]
        },
        {
            "paperId": "d0e3af5f20a451c04770929979d7a8406a1a2466",
            "title": "Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering",
            "abstract": "As the field of Large Language Models (LLMs) evolves at an accelerated pace, the critical need to assess and monitor their performance emerges. We introduce a benchmarking framework focused on knowledge graph engineering (KGE) accompanied by three challenges addressing syntax and error correction, facts extraction and dataset generation. We show that while being a useful tool, LLMs are yet unfit to assist in knowledge graph generation with zero-shot prompting. Consequently, our LLM-KG-Bench framework provides automatic evaluation and storage of LLM responses as well as statistical data and visualization tools to support tracking of prompt engineering and model performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34886987",
                    "name": "Lars-Peter Meyer"
                },
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "48555227",
                    "name": "K. Junghanns"
                },
                {
                    "authorId": "2236686278",
                    "name": "Felix Brei"
                },
                {
                    "authorId": "2108312",
                    "name": "Kirill Bulert"
                },
                {
                    "authorId": "1411787759",
                    "name": "Sabine Grunder-Fahrer"
                },
                {
                    "authorId": "2216105617",
                    "name": "Michael Martin"
                }
            ]
        },
        {
            "paperId": "1711fca4f09e6bffd3da6dd111780d95d43fd665",
            "title": "Ontoflow: A User-Friendly Ontology Development Workflow",
            "abstract": "For many years, the development of widely applicable and high-quality ontologies has been an ongoing research topic. Among the various challenges, the lack of integrated development environments for non-technical domain experts has been one of the most pressing research issues. But while the participation of domain experts is vital for the applicability of ontologies, there are hardly any software tools available that facilitate their active engagement. We present a solution that addresses this research gap by automating the ontology development process with the help of a workflow engine. We define a pipeline that facilitates ontology implementation, serialization, documentation and testing within the scope of a seamless automatic routine that can be easily set up by the ontology engineer and triggered by a non-technical domain expert. Thus, the processing pipeline takes care of most of the operations that usually have to be carried out by an ontology or software engineer. We demonstrate the applicability of the approach by developing an ontology with OntoFlow and validating its functioning with a large-scale ontology dataset from Linked Open Vocabularies (LOV).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2212030380",
                    "name": "Gordian Dziwis"
                },
                {
                    "authorId": "2486262",
                    "name": "L. Wenige"
                },
                {
                    "authorId": "34886987",
                    "name": "Lars-Peter Meyer"
                },
                {
                    "authorId": "2110607609",
                    "name": "Michael Martin"
                }
            ]
        },
        {
            "paperId": "fd2e7cbcb51dcc15764f860a5d61957ace537b06",
            "title": "Better a Microbenchmark on a Cluster than a User at the Office: Flink Cluster Benchmarking",
            "abstract": "When operating an Apache Flink cluster, performance problems may occur on all components of its setup. Reproducing those problems in different software or hardware components and on different nodes requires systematic experiments. We present an Apache Flink cluster benchmark set for server operators which is able to measure the performance of an Apache Flink cluster. This enables spotlighting irregularities in software or hardware behaviour.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2787910",
                    "name": "D. Reichelt"
                },
                {
                    "authorId": "34886987",
                    "name": "Lars-Peter Meyer"
                },
                {
                    "authorId": "1486403194",
                    "name": "Stefan K\u00fchne"
                }
            ]
        }
    ]
}