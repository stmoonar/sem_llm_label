{
    "authorId": "47783376",
    "papers": [
        {
            "paperId": "09a06bfe34431b0e802f0798e406022bbd37e118",
            "title": "Multi-Output RNN-T Joint Networks for Multi-Task Learning of ASR and Auxiliary Tasks",
            "abstract": "We propose a multi-output joint network architecture for RNN-T transducer, for multi-task modeling of ASR and auxiliary tasks that rely on ASR outputs. Each output of the joint network predicts tar-get labels with disjoint vocabularies for each task, while sharing the same audio features by the encoder and language model features by the prediction network. Each task is trained with an RNN-T loss that marginalizes over all possible paths, and we allow multiple tasks to share the blank logit so that they are synchronized. We demonstrate our method on two auxiliary tasks, namely capitalization and pause prediction, and discuss different considerations for modeling and inference procedures. For capitalization, we successfully distill capitalization labels from a standalone text normalization model, and achieve competitive Uppercase Error Rate (UER) while offering streaming capability and improved inference efficiency. In addition, our model has similar capitalization accuracy compared to a mixed-case ASR model, but obtains improved WERs if integrated with external language models. For pause prediction, we achieve the same performance as the previous two-step approach while providing a simpler training recipe without affecting ASR accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117928344",
                    "name": "Weiran Wang"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "51267247",
                    "name": "Shaojin Ding"
                },
                {
                    "authorId": "46702482",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "40604450",
                    "name": "Shuo-yiin Chang"
                },
                {
                    "authorId": "1743961",
                    "name": "David Rybach"
                },
                {
                    "authorId": "1784851",
                    "name": "Tara N. Sainath"
                },
                {
                    "authorId": "2145999837",
                    "name": "Yanzhang He"
                },
                {
                    "authorId": "143685627",
                    "name": "Ian McGraw"
                },
                {
                    "authorId": "2109681515",
                    "name": "Shankar Kumar"
                }
            ]
        },
        {
            "paperId": "31f87d4d974dfa8b3de92b054c1e3089f07a357a",
            "title": "Sharing Low Rank Conformer Weights for Tiny Always-On Ambient Speech Recognition Models",
            "abstract": "Continued improvements in machine learning techniques offer exciting new opportunities through the use of larger models and larger training datasets. However, there is a growing need to offer these new capabilities on-board low-powered devices such as smart-phones, wearables and other embedded environments where only low memory is available. Towards this, we consider methods to reduce the model size of Conformer-based speech recognition models which typically require models with greater than 100M parameters down to just 5M parameters while minimizing impact on model quality. Such a model allows us to achieve always-on ambient speech recognition on edge devices with low-memory neural processors. We propose model weight reuse at different levels within our model architecture: (i) repeating full conformer block layers, (ii) sharing specific conformer modules across layers, (iii) sharing sub-components per conformer module, and (iv) sharing decomposed sub-component weights after low-rank decomposition. By sharing weights at different levels of our model, we can retain the full model in-memory while increasing the number of virtual trans-formations applied to the input. Through a series of ablation studies and evaluations, we find that with weight sharing and a low-rank architecture, we can achieve a WER of 2.84 and 2.94 for Librispeech dev-clean and test-clean respectively with a 5M parameter model.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1596820289",
                    "name": "Steven M. Hernandez"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "51267247",
                    "name": "Shaojin Ding"
                },
                {
                    "authorId": "47043446",
                    "name": "A. Bruguier"
                },
                {
                    "authorId": "2557391",
                    "name": "Rohit Prabhavalkar"
                },
                {
                    "authorId": "1784851",
                    "name": "Tara N. Sainath"
                },
                {
                    "authorId": "2145999837",
                    "name": "Yanzhang He"
                },
                {
                    "authorId": "143685627",
                    "name": "Ian McGraw"
                }
            ]
        },
        {
            "paperId": "685248df424b5b5748c53c93cfab93f7e35e2cff",
            "title": "Multimodal Representation Learning of Cardiovascular Magnetic Resonance Imaging",
            "abstract": "Self-supervised learning is crucial for clinical imaging applications, given the lack of explicit labels in healthcare. However, conventional approaches that rely on precise vision-language alignment are not always feasible in complex clinical imaging modalities, such as cardiac magnetic resonance (CMR). CMR provides a comprehensive visualization of cardiac anatomy, physiology, and microstructure, making it challenging to interpret. Additionally, CMR reports require synthesizing information from sequences of images and different views, resulting in potentially weak alignment between the study and diagnosis report pair. To overcome these challenges, we propose \\textbf{CMRformer}, a multimodal learning framework to jointly learn sequences of CMR images and associated cardiologist's reports. Moreover, one of the major obstacles to improving CMR study is the lack of large, publicly available datasets. To bridge this gap, we collected a large \\textbf{CMR dataset}, which consists of 13,787 studies from clinical cases. By utilizing our proposed CMRformer and our collected dataset, we achieved remarkable performance in real-world clinical tasks, such as CMR image retrieval and diagnosis report retrieval. Furthermore, the learned representations are evaluated to be practically helpful for downstream applications, such as disease classification. Our work could potentially expedite progress in the CMR study and lead to more accurate and effective diagnosis and treatment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "2119407102",
                    "name": "Peide Huang"
                },
                {
                    "authorId": "1389703562",
                    "name": "Makiya Nakashima"
                },
                {
                    "authorId": "2158008485",
                    "name": "Jae-Hyeok Lee"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "2214612170",
                    "name": "W. Tang"
                },
                {
                    "authorId": "2118465661",
                    "name": "Po-Heng Chen"
                },
                {
                    "authorId": "2204079759",
                    "name": "C. Nguyen"
                },
                {
                    "authorId": "50435752",
                    "name": "Byung-Hak Kim"
                },
                {
                    "authorId": "2179058130",
                    "name": "Debbie Kwon"
                },
                {
                    "authorId": "2180027829",
                    "name": "Douglas Weber"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "103185169",
                    "name": "David Chen"
                }
            ]
        },
        {
            "paperId": "82da02137bae421a3f7a89c3bf2ab662037f4dfa",
            "title": "Embodied Executable Policy Learning with Language-based Scene Summarization",
            "abstract": "Large Language models (LLMs) have shown remarkable success in assisting robot learning tasks, i.e., complex household planning.However, the performance of pretrained LLMs heavily relies on domain-specific templated text data, which may be infeasible in real-world robot learning tasks with image-based observations. Moreover, existing LLMs with text inputs lack the capability to evolve with non-expert interactions with environments.In this work, we introduce a novel learning paradigm that generates robots\u2019 executable actions in the form of text, derived solely from visual observations. Our proposed paradigm stands apart from previous works, which utilized either language instructions or a combination of language and visual data as inputs. We demonstrate that our proposed method can employ two fine-tuning strategies, including imitation learning and reinforcement learning approaches, to adapt to the target test tasks effectively.We conduct extensive experiments involving various model selections, environments, and tasks across 7 house layouts in the VirtualHome environment. Our experimental results demonstrate that our method surpasses existing baselines, confirming the effectiveness of this novel learning paradigm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "87c9ac281bb606fb348acfb70348ce744a761d68",
            "title": "MultiSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos",
            "abstract": "Multimodal summarization with multimodal output (MSMO) has emerged as a promising research direction. Nonetheless, numerous limitations exist within existing public MSMO datasets, including insufficient upkeep, data inaccessibility, limited size, and the absence of proper categorization, which pose significant challenges to effective research. To address these challenges and provide a comprehensive dataset for this new direction, we have meticulously curated the MultiSum dataset. Our new dataset features (1) Human-validated summaries for both video and textual content, providing superior human instruction and labels for multimodal learning. (2) Comprehensively and meticulously arranged categorization, spanning 17 principal categories and 170 subcategories to encapsulate a diverse array of real-world scenarios. (3) Benchmark tests performed on the proposed dataset to assess varied tasks and methods, including video temporal segmentation , video summarization , text summarization , and multimodal summarization . To champion accessibility and collaboration, we release the MultiSum dataset and the data collection tool as fully open-source resources, fostering trans-parency and accelerating future developments. Our project website can be found at https://multisum-dataset.github.io/ .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "2219660996",
                    "name": "Aditesh Kumar"
                },
                {
                    "authorId": "2220631291",
                    "name": "Karthik Mittal"
                },
                {
                    "authorId": "2219563428",
                    "name": "Claire Jin"
                },
                {
                    "authorId": "2149231840",
                    "name": "Zhengyuan Yang"
                },
                {
                    "authorId": "50703697",
                    "name": "Linjie Li"
                },
                {
                    "authorId": "2124948371",
                    "name": "Jianfeng Wang"
                },
                {
                    "authorId": "71788673",
                    "name": "Bo Li"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "29957038",
                    "name": "Lijuan Wang"
                }
            ]
        },
        {
            "paperId": "a22b88c046bd84d29ae592ba1e247e7891476920",
            "title": "Learning Shared Safety Constraints from Multi-task Demonstrations",
            "abstract": "Regardless of the particular task we want them to perform in an environment, there are often shared safety constraints we want our agents to respect. For example, regardless of whether it is making a sandwich or clearing the table, a kitchen robot should not break a plate. Manually specifying such a constraint can be both time-consuming and error-prone. We show how to learn constraints from expert demonstrations of safe task completion by extending inverse reinforcement learning (IRL) techniques to the space of constraints. Intuitively, we learn constraints that forbid highly rewarding behavior that the expert could have taken but chose not to. Unfortunately, the constraint learning problem is rather ill-posed and typically leads to overly conservative constraints that forbid all behavior that the expert did not take. We counter this by leveraging diverse demonstrations that naturally occur in multi-task settings to learn a tighter set of constraints. We validate our method with simulation experiments on high-dimensional continuous control tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237958989",
                    "name": "Konwoo Kim"
                },
                {
                    "authorId": "2073365488",
                    "name": "Gokul Swamy"
                },
                {
                    "authorId": "76923918",
                    "name": "Zuxin Liu"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "2487768",
                    "name": "Sanjiban Choudhury"
                },
                {
                    "authorId": "2238047775",
                    "name": "Zhiwei Steven Wu"
                }
            ]
        },
        {
            "paperId": "bcd12a70426d37b1856d5178694d8f804297e6f7",
            "title": "Pixel-wise Smoothing for Certified Robustness against Camera Motion Perturbations",
            "abstract": "Deep learning-based visual perception models lack robustness when faced with camera motion perturbations in practice. The current certification process for assessing robustness is costly and time-consuming due to the extensive number of image projections required for Monte Carlo sampling in the 3D camera motion space. To address these challenges, we present a novel, efficient, and practical framework for certifying the robustness of 3D-2D projective transformations against camera motion perturbations. Our approach leverages a smoothing distribution over the 2D pixel space instead of in the 3D physical space, eliminating the need for costly camera motion sampling and significantly enhancing the efficiency of robustness certifications. With the pixel-wise smoothed classifier, we are able to fully upper bound the projection errors using a technique of uniform partitioning in camera motion space. Additionally, we extend our certification framework to a more general scenario where only a single-frame point cloud is required in the projection oracle. Through extensive experimentation, we validate the trade-off between effectiveness and efficiency enabled by our proposed method. Remarkably, our approach achieves approximately 80% certified accuracy while utilizing only 30% of the projected image frames. The code is available at https://github.com/HanjiangHu/pixel-wise-smoothing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "113164891",
                    "name": "Hanjiang Hu"
                },
                {
                    "authorId": "76923918",
                    "name": "Zuxin Liu"
                },
                {
                    "authorId": "2008285677",
                    "name": "Linyi Li"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "e5d31a57c28225eec753e2174fe8c3fbbeca20fa",
            "title": "Converting ECG Signals to Images for Efficient Image-text Retrieval via Encoding",
            "abstract": "Automated interpretation of electrocardiograms (ECG) has garnered significant attention with the advancements in machine learning methodologies. Despite the growing interest in automated ECG interpretation using machine learning, most current studies focus solely on classification or regression tasks and overlook a crucial aspect of clinical cardio-disease diagnosis: the diagnostic report generated by experienced human clinicians. In this paper, we introduce a novel approach to ECG interpretation, leveraging recent breakthroughs in Large Language Models (LLMs) and Vision-Transformer (ViT) models. Rather than treating ECG diagnosis as a classification or regression task, we propose an alternative method of automatically identifying the most similar clinical cases based on the input ECG data. Also, since interpreting ECG as images are more affordable and accessible, we process ECG as encoded images and adopt a vision-language learning paradigm to jointly learn vision-language alignment between encoded ECG images and ECG diagnosis reports. Encoding ECG into images can result in an efficient ECG retrieval system, which will be highly practical and useful in clinical applications. More importantly, our findings could serve as a crucial resource for providing diagnostic services in regions where only paper-printed ECG images are accessible due to past underdevelopment.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "2131159786",
                    "name": "Shiqi Liu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "2157140664",
                    "name": "Jingqi Zhang"
                },
                {
                    "authorId": "1722611",
                    "name": "Chaojing Duan"
                },
                {
                    "authorId": "2151860122",
                    "name": "Michael Rosenberg"
                },
                {
                    "authorId": "1418480916",
                    "name": "E. Liu"
                },
                {
                    "authorId": "2180027829",
                    "name": "Douglas Weber"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "e976f9e6e83a22144dc6026342a64d2ffde8408b",
            "title": "Your Room is not Private: Gradient Inversion Attack on Reinforcement Learning",
            "abstract": "The prominence of embodied Artificial Intelligence (AI), which empowers robots to navigate, perceive, and engage within virtual environments, has attracted significant attention, owing to the remarkable advancements in computer vision and large language models. Privacy emerges as a pivotal concern within the realm of embodied AI, as the robot accesses substantial personal information. However, the issue of privacy leakage in embodied AI tasks, particularly in relation to reinforcement learning algorithms, has not received adequate consideration in research. This paper aims to address this gap by proposing an attack on the value-based algorithm and the gradient-based algorithm, utilizing gradient inversion to reconstruct states, actions, and supervision signals. The choice of using gradients for the attack is motivated by the fact that commonly employed federated learning techniques solely utilize gradients computed based on private user data to optimize models, without storing or transmitting the data to public servers. Nevertheless, these gradients contain sufficient information to potentially expose private data. To validate our approach, we conduct experiments on the AI2THOR simulator and evaluate our algorithm on active perception, a prevalent task in embodied AI. The experimental results demonstrate the effectiveness of our method in successfully reconstructing all information from the data across 120 room layouts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Miao Li"
                },
                {
                    "authorId": "152425748",
                    "name": "Wenhao Ding"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "354c122c93ed56c8a17877542358a921ce5aee01",
            "title": "Improve Speech Enhancement using Perception-High-Related Time-Frequency Loss",
            "abstract": "Commonly used speech enhancement (SE) training losses like mean absolute error (MAE) loss and short-time Fourier transformation (STFT) loss suffer from the problem of mismatch with the speech quality, which leads to suboptimal training results. To tackle this problem, we propose a new loss named perception-high-related time-frequency (PHRTF) loss. The proposed loss modifies STFT loss by adding a trainable module named perceptual spectrum mask predictor (PSMP). This module can predict the perceptual spectrum mask (PSM) from the magnitude spectrum of enhanced and clean speech. Further, PHRTF loss multiplies the amplitude error spectrum (AES) with PSM to emphasize perception-relevant loss components to correlate highly with the speech quality. We conduct experiments on the VoiceBank-DEMAND dataset, and the results show that PHRTF loss has a significantly higher correlation with the speech quality than other losses. Meanwhile, PHRTF loss outperforms other losses and improves PESQ by 0.32 over MAE loss and 0.19 over STFT loss on the training of Wave-U-Net. We also apply PHRTF loss to a more advanced SE model, and the training result outperforms other competitive baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "2156121216",
                    "name": "Zhan Zhang"
                },
                {
                    "authorId": "2107272845",
                    "name": "B. Yu"
                },
                {
                    "authorId": "2115623532",
                    "name": "Yuehai Wang"
                }
            ]
        }
    ]
}