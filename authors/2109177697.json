{
    "authorId": "2109177697",
    "papers": [
        {
            "paperId": "1803a7681ec3fb46e76d160e449059551649e633",
            "title": "WOT-Class: Weakly Supervised Open-world Text Classification",
            "abstract": "State-of-the-art weakly supervised text classification methods, while significantly reduced the required human supervision, still requires the supervision to cover all the classes of interest. This is never easy to meet in practice when human explore new, large corpora without complete pictures. In this paper, we work on a novel yet important problem of weakly supervised open-world text classification, where supervision is only needed for a few examples from a few known classes and the machine should handle both known and unknown classes in test time. General open-world classification has been studied mostly using image classification; however, existing methods typically assume the availability of sufficient known-class supervision and strong unknown-class prior knowledge (e.g., the number and/or data distribution). We propose a novel framework \u00f8ur that lifts those strong assumptions. Specifically, it follows an iterative process of (a) clustering text to new classes, (b) mining and ranking indicative words for each class, and (c) merging redundant classes by using the overlapped indicative words as a bridge. Extensive experiments on 7 popular text classification datasets demonstrate that \u00f8ur outperforms strong baselines consistently with a large margin, attaining 23.33% greater average absolute macro-F1 over existing approaches across all datasets. Such competent accuracy illuminates the practical potential of further reducing human effort for text classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155412834",
                    "name": "Tianle Wang"
                },
                {
                    "authorId": "2240689",
                    "name": "Zihan Wang"
                },
                {
                    "authorId": "2109177697",
                    "name": "Weitang Liu"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "2197f636216106eff9313676d76d420a68965dad",
            "title": "Gradient-based Wang-Landau Algorithm: A Novel Sampler for Output Distribution of Neural Networks over the Input Space",
            "abstract": "The output distribution of a neural network (NN) over the entire input space captures the complete input-output mapping relationship, offering insights toward a more comprehensive NN understanding. Exhaustive enumeration or traditional Monte Carlo methods for the entire input space can exhibit impractical sampling time, especially for high-dimensional inputs. To make such difficult sampling computationally feasible, in this paper, we propose a novel Gradient-based Wang-Landau (GWL) sampler. We first draw the connection between the output distribution of a NN and the density of states (DOS) of a physical system. Then, we renovate the classic sampler for the DOS problem, the Wang-Landau algorithm, by replacing its random proposals with gradient-based Monte Carlo proposals. This way, our GWL sampler investigates the under-explored subsets of the input space much more efficiently. Extensive experiments have verified the accuracy of the output distribution generated by GWL and also showcased several interesting findings - for example, in a binary image classification task, both CNN and ResNet mapped the majority of human unrecognizable images to very negative logit values.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109177697",
                    "name": "Weitang Liu"
                },
                {
                    "authorId": "2208885173",
                    "name": "Ying-Wai Li"
                },
                {
                    "authorId": "89197172",
                    "name": "Yi-Zhuang You"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "1af23b78166b6adbccfedff04abcc0dd23ab1bd5",
            "title": "Can multi-label classification networks know what they don't know?",
            "abstract": "Estimating out-of-distribution (OOD) uncertainty is a central challenge for safely deploying machine learning models in the open-world environment. Improved methods for OOD detection in multi-class classification have emerged, while OOD detection methods for multi-label classification remain underexplored and use rudimentary techniques. We propose JointEnergy, a simple and effective method, which estimates the OOD indicator scores by aggregating energy scores from multiple labels. We show that JointEnergy can be mathematically interpreted from a joint likelihood perspective. Our results show consistent improvement over previous methods that are based on the maximum-valued scores, which fail to capture joint information from multiple labels. We demonstrate the effectiveness of our method on three common multi-label classification benchmarks, including MS-COCO, PASCAL-VOC, and NUS-WIDE. We show that JointEnergy can reduce the FPR95 by up to 10.05% compared to the previous best baseline, establishing state-of-the-art performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145335027",
                    "name": "Haoran Wang"
                },
                {
                    "authorId": "2109177697",
                    "name": "Weitang Liu"
                },
                {
                    "authorId": "146025808",
                    "name": "Alex E. Bocchieri"
                },
                {
                    "authorId": "1527103472",
                    "name": "Yixuan Li"
                }
            ]
        },
        {
            "paperId": "8fb74ced065de3c5266f363ca43d06ab7dfd9e55",
            "title": "Energy-based Out-of-distribution Detection for Multi-label Classification",
            "abstract": "Out-of-distribution (OOD) detection is essential to prevent anomalous inputs from causing a model to fail during deployment. Improved methods for OOD detection in multi-class classification have emerged, while OOD detection methods for multi-label classification remain underexplored and use rudimentary techniques. We propose SumEnergy, a simple and effective method, which estimates the OOD indicator scores by aggregating energy scores from multiple labels. We show that SumEnergy can be mathematically interpreted from a joint likelihood perspective. Our results show consistent improvement over previous methods that are based on the maximum-valued scores, which fail to capture joint information from multiple labels. We demonstrate the effectiveness of our method on three common multi-label classification benchmarks, including MS-COCO, PASCAL-VOC, and NUS-WIDE. We show that SumEnergy reduces the FPR95 by up to 10.05% compared to the previous best baseline, establishing state-of-the-art performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145335027",
                    "name": "Haoran Wang"
                },
                {
                    "authorId": "2109177697",
                    "name": "Weitang Liu"
                },
                {
                    "authorId": "146025808",
                    "name": "Alex E. Bocchieri"
                },
                {
                    "authorId": "1527103472",
                    "name": "Yixuan Li"
                }
            ]
        },
        {
            "paperId": "1a8f40410441a1839a3686799328a687e505fd0d",
            "title": "Object Localization and Motion Transfer learning with Capsules",
            "abstract": "Inspired by CapsNet's routing-by-agreement mechanism, with its ability to learn object properties, and by center-of-mass calculations from physics, we propose a CapsNet architecture with object coordinate atoms and an LSTM network for evaluation. The first is based on CapsNet but uses a new routing algorithm to find the objects' approximate positions in the image coordinate system, and the second is a parameterized affine transformation network that can predict future positions from past positions by learning the translation transformation from 2D object coordinates generated from the first network. We demonstrate the learned translation transformation is transferable to another dataset without the need to train the transformation network again. Only the CapsNet needs training on the new dataset. As a result, our work shows that object recognition and motion prediction can be separated, and that motion prediction can be transferred to another dataset with different object types.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109177697",
                    "name": "Weitang Liu"
                },
                {
                    "authorId": "47223178",
                    "name": "Emad Barsoum"
                },
                {
                    "authorId": "1758404",
                    "name": "John Douglas Owens"
                }
            ]
        },
        {
            "paperId": "f74ccbc8988b7f0b847c480d4e8bea3082f4f931",
            "title": "Sample-Efficient Deep RL with Generative Adversarial Tree Search",
            "abstract": "We propose Generative Adversarial Tree Search (GATS), a sample-efficient Deep Reinforcement Learning (DRL) algorithm. While Monte Carlo Tree Search (MCTS) is known to be effective for search and planning in RL but, it is often sampleinefficient and therefore expensive to apply in practice. In this work, we train Generative Adversarial Networks (GANs) to model an environment\u2019s dynamics and train a predictor to learn the reward function. While typical DRL algorithms estimate the Q function or optimize directly over a parameterized policy, we exploit the collected data through interaction with the environment to train both a reward predictor conditional GAN that simulated state transitions. During planning, we deploy finite depth MCTS, using the trained generative model for the tree search and estimated Q value for the leaves, in order to find the best policy. We theoretically show that GATS improves the bias-variance trade-off in DRL. On the Atari game Pong, GATS significantly reduces the bias in Q estimates and leads to a drastic reduction of sample complexity of DQN by a factor of 200%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3371922",
                    "name": "K. Azizzadenesheli"
                },
                {
                    "authorId": "145951921",
                    "name": "Brandon Yang"
                },
                {
                    "authorId": "2109177697",
                    "name": "Weitang Liu"
                },
                {
                    "authorId": "2563117",
                    "name": "E. Brunskill"
                },
                {
                    "authorId": "32219137",
                    "name": "Zachary Chase Lipton"
                },
                {
                    "authorId": "2047844",
                    "name": "Anima Anandkumar"
                }
            ]
        },
        {
            "paperId": "296a7a6d0684d1d139305fb81dc20fa9a44d1c45",
            "title": "Gunrock",
            "abstract": "For large-scale graph analytics on the GPU, the irregularity of data access and control flow, and the complexity of programming GPUs, have presented two significant challenges to developing a programmable high-performance graph library. \u201cGunrock,\u201d our graph-processing system designed specifically for the GPU, uses a high-level, bulk-synchronous, data-centric abstraction focused on operations on a vertex or edge frontier. Gunrock achieves a balance between performance and expressiveness by coupling high-performance GPU computing primitives and optimization strategies with a high-level programming model that allows programmers to quickly develop new graph primitives with small code size and minimal GPU programming knowledge. We characterize the performance of various optimization strategies and evaluate Gunrock\u2019s overall performance on different GPU architectures on a wide range of graph primitives that span from traversal-based algorithms and ranking algorithms, to triangle counting and bipartite-graph-based algorithms. The results show that on a single GPU, Gunrock has on average at least an order of magnitude speedup over Boost and PowerGraph, comparable performance to the fastest GPU hardwired primitives and CPU shared-memory graph libraries, such as Ligra and Galois, and better performance than any other GPU high-level graph library.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1772360",
                    "name": "Yangzihao Wang"
                },
                {
                    "authorId": "2111415414",
                    "name": "Yuechao Pan"
                },
                {
                    "authorId": "1779628",
                    "name": "Andrew A. Davidson"
                },
                {
                    "authorId": "1929145",
                    "name": "Yuduo Wu"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                },
                {
                    "authorId": null,
                    "name": "Leyuan Wang"
                },
                {
                    "authorId": "2249761562",
                    "name": "Muhammad Osama"
                },
                {
                    "authorId": "47177305",
                    "name": "C. Yuan"
                },
                {
                    "authorId": "2109177697",
                    "name": "Weitang Liu"
                },
                {
                    "authorId": "8631609",
                    "name": "Andy T. Riffel"
                },
                {
                    "authorId": "1758404",
                    "name": "John Douglas Owens"
                }
            ]
        }
    ]
}