{
    "authorId": "2004747401",
    "papers": [
        {
            "paperId": "838b6218d4840635631eaac113dd556d8da734a9",
            "title": "MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn NLU",
            "abstract": "Although Large Language Models(LLMs) can generate coherent and contextually relevant text, they often struggle to recognise the intent behind the human user's query. Natural Language Understanding (NLU) models, however, interpret the purpose and key information of user's input to enable responsive interactions. Existing NLU models generally map individual utterances to a dual-level semantic frame, involving sentence-level intent and word-level slot labels. However, real-life conversations primarily consist of multi-turn conversations, involving the interpretation of complex and extended dialogues. Researchers encounter challenges addressing all facets of multi-turn dialogue conversations using a unified single NLU model. This paper introduces a novel approach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge distillation for multi-turn NLU. To achieve this, we construct distinct teachers for varying levels of conversation knowledge, namely, sentence-level intent detection, word-level slot filling, and conversation-level domain classification. These teachers are then fine-tuned to acquire specific knowledge of their designated levels. A multi-teacher loss is proposed to facilitate the combination of these multi-level teachers, guiding a student model in multi-turn dialogue tasks. The experimental results demonstrate the efficacy of our model in improving the overall multi-turn conversation understanding, showcasing the potential for advancements in NLU models through the incorporation of multi-level dialogue knowledge distillation techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2316172109",
                    "name": "Yan Li"
                },
                {
                    "authorId": "2004747401",
                    "name": "So-Eon Kim"
                },
                {
                    "authorId": "2192676433",
                    "name": "Seong-Bae Park"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                }
            ]
        },
        {
            "paperId": "3f28f2b879c018c5ed26dff3d02d1e06748d33e5",
            "title": "Query Similarity of Various Linguistic Levels for Hybridized Conversational Agents",
            "abstract": "The performance of retrieval-based conversational agents is affected by the discrepancy between a user query and a retrieved query similar to the user query. There have been a number of previous studies to cope with this discrepancy, and a skeleton-based response generation is one of the successful approaches. However, it shows some ineffectiveness in that it considers only the lexical similarity in finding a similar query from a database of query-response pairs. Therefore, this paper proposes a CNN-based model which uses the combination of the neural representation of two queries and manually-designed lexico-syntactic features to determine the similarity between the queries. According to the experimental results on a manually-constructed dataset, the proposed model outperforms legacy search engine in finding similar queries from the database, which proves the plausibility of the proposed model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2004747401",
                    "name": "So-Eon Kim"
                },
                {
                    "authorId": "2159807650",
                    "name": "Choong-Seon Hong"
                },
                {
                    "authorId": "2149246983",
                    "name": "Seong-Bae Park"
                }
            ]
        },
        {
            "paperId": "acb5d156a5b36705c92770dd5b27e180420afd35",
            "title": "A Topical Category-Aware Neural Text Summarizer",
            "abstract": "The advent of the sequence-to-sequence model and the attention mechanism has increased the comprehension and readability of automatically generated summaries. However, most previous studies on text summarization have focused on generating or extracting sentences only from an original text, even though every text has a latent topic category. That is, even if a topic category helps improve the summarization quality, there have been no efforts to utilize such information in text summarization. Therefore, this paper proposes a novel topical category-aware neural text summarizer which is differentiated from legacy neural summarizers in that it reflects the topic category of an original text into generating a summary. The proposed summarizer adopts the class activation map (CAM) as topical influence of the words in the original text. Since the CAM excerpts the words relevant to a specific category from the text, it allows the attention mechanism to be influenced by the topic category. As a result, the proposed neural summarizer reflects the topical information of a text as well as the content information into a summary by combining the attention mechanism and CAM. The experiments on The New York Times Annotated Corpus show that the proposed model outperforms the legacy attention-based sequence-to-sequence model, which proves that it is effective at reflecting a topic category into automatic summarization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2004747401",
                    "name": "So-Eon Kim"
                },
                {
                    "authorId": "122608628",
                    "name": "N. Kaibalina"
                },
                {
                    "authorId": "1786338",
                    "name": "Seong-Bae Park"
                }
            ]
        }
    ]
}