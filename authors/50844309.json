{
    "authorId": "50844309",
    "papers": [
        {
            "paperId": "1361a13ef166c142dd5ca9548f02720d834fd011",
            "title": "Approximate Distance-based Anomaly Detection at Massive Scale",
            "abstract": "Anomaly detection is valuable in many domains. Distance-based algorithms are one of the most well-known techniques for anomaly detection. These algorithms have been shown to produce good accuracy in a wide range of applications. However, distance-based algorithms are limited in terms of scalability. This makes it difficult for these algorithms to handle massive data sets. To overcome this limitation, in this paper we propose a parallel formulation of the popular KNN distance-based algorithm for unsupervised anomaly detection. Our formulation focused on improving scalability by analyzing in parallel different parts of the data. This approach is valuable whenever the analyzed data does not fit comfortably into memory, or when it is distributed across several locations. To reduce synchronization latency, the proposed algorithm computes approximate nearest neighborhoods. We show, both theoretically and empirically, how this approach greatly improves scalability without significantly penalizing detection accuracy. To validate our approach, we perform extensive experiments on both synthetic and real-world data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50844309",
                    "name": "Sivam Pasupathipillai"
                },
                {
                    "authorId": "2539248",
                    "name": "Emanuele Della Valle"
                }
            ]
        },
        {
            "paperId": "0c981434226dd25c582d1a59d987ae9b4d1f2063",
            "title": "Cost-Aware Streaming Data Analysis: Distributed vs Single-Thread",
            "abstract": "Distributed systems have become the preferred solution for dealing with Big Data analysis tasks. These systems are able to achieve superior performance by managing a large pool of resources as a single entity. However, in many contexts, performance is not the only metric to consider. When comparing two performance equivalent solutions, their cost becomes an important factor. Distributed systems are usually more expensive to deploy than traditional single-threaded applications. In this work, we build on these considerations by presenting an empirical study that compares the cost of two performance equivalent solutions for a real streaming data analysis task for the Telecommunication industry. The first solution is built on popular distributed processing engines (Apache Spark), while the second solution is a single-threaded application built on an home-brew stream processing framework (Natron). We show that, in the case of continuous analysis, the benefits of distributed processing are outvalued by the distributed data ingestion costs. This is also the case for periodic analysis. However, if data ingestion costs are fixed and small, we show that the most cost-effective solution depends on the dataset size.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50535911",
                    "name": "Marco Balduini"
                },
                {
                    "authorId": "50844309",
                    "name": "Sivam Pasupathipillai"
                },
                {
                    "authorId": "2539248",
                    "name": "Emanuele Della Valle"
                }
            ]
        }
    ]
}