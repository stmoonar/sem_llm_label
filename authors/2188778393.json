{
    "authorId": "2188778393",
    "papers": [
        {
            "paperId": "c743b4e65db9bbafc551314034a0c53fbd1f4c0c",
            "title": "mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection",
            "abstract": "This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1405284505",
                    "name": "Markus Reiter-Haas"
                },
                {
                    "authorId": "2188778393",
                    "name": "Alexander Ertl"
                },
                {
                    "authorId": "2212325380",
                    "name": "Kevin Innerhofer"
                },
                {
                    "authorId": "3124784",
                    "name": "E. Lex"
                }
            ]
        },
        {
            "paperId": "c9504dfeabeffc836bcaba20a04afce0bb731394",
            "title": "Gradient-based Weight Density Balancing for Robust Dynamic Sparse Training",
            "abstract": "Training a sparse neural network from scratch requires optimizing connections at the same time as the weights themselves. Typically, the weights are redistributed after a predefined number of weight updates, removing a fraction of the parameters of each layer and inserting them at different locations in the same layers. The density of each layer is determined using heuristics, often purely based on the size of the parameter tensor. While the connections per layer are optimized multiple times during training, the density of each layer remains constant. This leaves great unrealized potential, especially in scenarios with a high sparsity of 90% and more. We propose Global Gradient-based Redistribution, a technique which distributes weights across all layers - adding more weights to the layers that need them most. Our evaluation shows that our approach is less prone to unbalanced weight distribution at initialization than previous work and that it is able to find better performing sparse subnetworks at very high sparsity levels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52116137",
                    "name": "Mathias Parger"
                },
                {
                    "authorId": "2188778393",
                    "name": "Alexander Ertl"
                },
                {
                    "authorId": "2188778607",
                    "name": "Paul Eibensteiner"
                },
                {
                    "authorId": "2110565449",
                    "name": "J. H. Mueller"
                },
                {
                    "authorId": "2061234702",
                    "name": "Martin Winter"
                },
                {
                    "authorId": "1681353",
                    "name": "M. Steinberger"
                }
            ]
        }
    ]
}